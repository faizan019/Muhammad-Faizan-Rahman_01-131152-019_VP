{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import regex as re\n",
    "import warnings\n",
    "import arrow\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "import arrow\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "from string import Template\n",
    "\n",
    "\n",
    "# supress warnings to debugging\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.auth\n",
    "import pandas_gbq\n",
    "import arrow\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from string import Template\n",
    "from datetime import datetime as dt, timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "\n",
    "def hvac_function(project, dataset_src, dataset_dest, table_src, table_dest, tz_info, config, client_id, device_id):\n",
    "    def unique_logs(project, client_id, device_id, client, dataset_dest, table_dest, templogs):\n",
    "        sql = client.query(f\"\"\"\n",
    "            select client_id,device_id,rule_name, aliases,criticality,test,qa,count(*) as count\n",
    "            from `{project}.{dataset_dest}.{table_dest}` where extract(date from time_local) = CURRENT_DATE()\n",
    "            and test='fail' and criticality = 'high'\n",
    "            and client_id = '{client_id}' and device_id = '{device_id}'\n",
    "            group by client_id,device_id,rule_name,aliases,criticality,test,qa\n",
    "        \"\"\")\n",
    "\n",
    "        previous_logs = sql.to_dataframe()\n",
    "        merged_templogs = pd.merge(templogs, previous_logs,\n",
    "                                   on=['client_id', 'device_id', 'rule_name', 'aliases', 'criticality', 'test', 'qa'],\n",
    "                                   how='left')\n",
    "\n",
    "        merged_templogs['count'] = merged_templogs['count'].fillna(0)\n",
    "        merged_templogs = merged_templogs[merged_templogs['count'] <= 3]\n",
    "        if merged_templogs.shape[0] >= 1:\n",
    "            return merged_templogs[['client_id',\n",
    "                                    'device_id',\n",
    "                                    'rule_name',\n",
    "                                    'aliases',\n",
    "                                    'criticality',\n",
    "                                    'time_local',\n",
    "                                    'test',\n",
    "                                    'date_update',\n",
    "                                    'qa']]\n",
    "        else:\n",
    "            return pd.DataFrame(columns=['client_id',\n",
    "                                         'device_id',\n",
    "                                         'rule_name',\n",
    "                                         'aliases',\n",
    "                                         'criticality',\n",
    "                                         'time_local',\n",
    "                                         'test',\n",
    "                                         'date_update',\n",
    "                                         'qa'])\n",
    "\n",
    "    def create_logs(rule, data_rd, logs, templogs):\n",
    "        obj = geFramework(rule, data_rd)\n",
    "\n",
    "        if obj is not None:\n",
    "            dff = obj.return_log()\n",
    "            logs = pd.concat([logs, dff], axis=0)\n",
    "\n",
    "            dff = dff.reset_index(drop=True)\n",
    "            if rule['mute_alert'] is None:\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "            else:\n",
    "                aliases_list = rule['mute_alert'].split(',')\n",
    "\n",
    "                for j in aliases_list:\n",
    "                    dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "                    logs.loc[logs.aliases.str.contains(j.strip()),\"mute_alert\"] = \"muted\"\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "            if rule['mute_logs'] is None:\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "                clear_logs = logs\n",
    "            elif rule['mute_logs'].lower() == 'all':\n",
    "                dff = dff[~dff.rule_name.str.contains(rule['rule_name'])]\n",
    "\n",
    "                clear_logs = logs[~logs.rule_name.str.contains(rule['rule_name'])]\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "            else:\n",
    "                aliases_list = rule['mute_logs'].split(',')\n",
    "\n",
    "                for j in aliases_list:\n",
    "                    dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "                    clear_logs = logs[~logs.aliases.str.contains(j.strip())]\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "        return clear_logs, templogs\n",
    "\n",
    "    output = config\n",
    "\n",
    "    rules = pd.DataFrame(output['rules'])\n",
    "    rules.index = rules.index.astype(int)\n",
    "    rules.sort_index(inplace=True)\n",
    "    rules = rules[['rule_name',\n",
    "                   'alias_a',\n",
    "                   'alias_b',\n",
    "                   'alias_c',\n",
    "                   'alias_d',\n",
    "                   'equation_1',\n",
    "                   'equation_2',\n",
    "                   'duration_hours',\n",
    "                   'criticality',\n",
    "                   'start_date',\n",
    "                   'end_date',\n",
    "                   'mute_alert',\n",
    "                   'mute_logs',\n",
    "                   'qa']].copy()\n",
    "\n",
    "    rules = rules.where(pd.notnull(rules), None)\n",
    "    settings = output['settings']['mute_rules']\n",
    "    logs_link = output['logs_link']\n",
    "    duration_hours = rules['duration_hours'].to_list()\n",
    "\n",
    "    logs = pd.DataFrame(columns=[\"client_id\",\n",
    "                                 \"device_id\",\n",
    "                                 \"rule_name\",\n",
    "                                 \"aliases\",\n",
    "                                 \"criticality\",\n",
    "                                 \"time_local\",\n",
    "                                 \"test\",\n",
    "                                 \"date_update\",\n",
    "                                 \"qa\",\n",
    "                                 \"mute_alert\",\n",
    "                                 \"data_start_time\"])\n",
    "    templogs = pd.DataFrame(columns=[\"client_id\",\n",
    "                                     \"device_id\",\n",
    "                                     \"rule_name\",\n",
    "                                     \"aliases\",\n",
    "                                     \"criticality\",\n",
    "                                     \"time_local\",\n",
    "                                     \"test\",\n",
    "                                     \"date_update\",\n",
    "                                     \"qa\",\n",
    "                                     \"mute_alert\",\n",
    "                                     \"data_start_time\"])\n",
    "\n",
    "    # credentials, project_id = google.auth.default()\n",
    "\n",
    "    #client = bigquery.Client(project=project)\n",
    "    end_time = arrow.get('2023-01-31 15:00:00')\n",
    "    start = (end_time + datetime.timedelta(hours=-max(duration_hours) - 5)).date()\n",
    "    end = (end_time + datetime.timedelta(hours=max(duration_hours))).date()\n",
    "\n",
    "#     end_time = arrow.utcnow().replace(second=0, microsecond=0).to(tz_info).shift(minutes=-15)\n",
    "#     end_time = arrow.get('2023-01-31 15:00:00')\n",
    "    start_time = end_time + datetime.timedelta(hours=-max(duration_hours))\n",
    "\n",
    "    str_end_time = str(end_time.datetime).split('+')[0]\n",
    "    str_start_time = str(start_time.datetime).split('+')[0]\n",
    "    print(f'Data between {str_start_time} and {str_end_time}')\n",
    "\n",
    "    if settings is False:\n",
    "        data_rd = pd.read_csv('D:\\\\Thermosphr\\\\QA Vectoize task\\\\gera-arcaden-germany.csv')\n",
    "        \n",
    "        date = arrow.utcnow().date()\n",
    "        date = date.replace(year=1900)\n",
    "        if data_rd.shape[0] > 0:\n",
    "\n",
    "            print('ready to run function')\n",
    "\n",
    "            # recursively apply rules over data\n",
    "\n",
    "            for i in range(rules.shape[0]):\n",
    "                if not rules.iloc[i]['mute_logs']:\n",
    "                    start_date = rules.iloc[i]['start_date']\n",
    "                    end_date = rules.iloc[i]['end_date']\n",
    "\n",
    "                    if start_date == None and end_date == None:\n",
    "\n",
    "                        logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "\n",
    "                    elif start_date != None and end_date != None:\n",
    "\n",
    "                        start_date = dt.strptime(start_date, '%m-%d').date()\n",
    "                        end_date = dt.strptime(end_date, '%m-%d').date()\n",
    "\n",
    "                        if start_date < end_date:\n",
    "                            if (start_date <= date and end_date >= date):\n",
    "                                logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "                        elif start_date > end_date:\n",
    "                            if (start_date >= date and end_date >= date):\n",
    "                                logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "                        else:\n",
    "                            print('Logs not created for this rule  :  ', rules.iloc[i]['rule_name'])\n",
    "                else:\n",
    "                    print(f\"{rules.iloc[i]['rule_name']} is muted\")\n",
    "\n",
    "            current_time = datetime.datetime.utcnow().replace(\n",
    "                second=0, microsecond=0)\n",
    "            # Finding out duplicate aliases if any\n",
    "#             df1 = dataframe[[\"alias\", \"propertyId\"]].groupby([\"alias\", \"propertyId\"]).count().reset_index()\n",
    "#             df2 = df1[['alias']].groupby(['alias'])['alias'].count().reset_index(name='count')\n",
    "#             df3 = df2.loc[df2['count'] > 1].sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "            templogs['client_id'] = client_id\n",
    "            templogs['device_id'] = device_id\n",
    "            templogs = templogs[~templogs.duplicated()].reset_index(drop=True)\n",
    "            templogs = templogs[templogs.time_local == str_end_time]\n",
    "         \n",
    "\n",
    "            logs['client_id'] = client_id\n",
    "            logs['device_id'] = device_id\n",
    "            logs['data_start_time'] = pd.to_datetime(str_start_time, format='%Y-%m-%d %H:%M:%S')\n",
    "            logs['date_update'] = pd.to_datetime(str(current_time), format='%Y-%m-%d %H:%M:%S')\n",
    "            #logs = logs[logs.time_local == str_end_time]\n",
    "            logs = logs[~logs.duplicated()].reset_index(drop=True)\n",
    "            logs[['time_local', 'date_update', 'data_start_time']] = logs[['time_local', 'date_update', 'data_start_time']].astype(str)\n",
    "            #table = client.get_table(f\"{project}.{dataset_dest}.{table_dest}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('No Data for given time period:')\n",
    "            print(start_time)\n",
    "            print(end_time)\n",
    "    else:\n",
    "        print(\"All rules are muted\")\n",
    "        \n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class geFramework():\n",
    "    def __init__(self, rule, batch):\n",
    "\n",
    "        # get subset of data, min-time and max-time for the given start and end time\n",
    "        #batch = self.__batch_subset(batch, rule['duration_hours'])\n",
    "        if 'missing_values' not in rule['rule_name']:\n",
    "            batch.fillna(method='ffill', inplace=True)\n",
    "        # indices of the rule dataframe\n",
    "        alias_cols = rule.index\n",
    "        subs = 'alias'\n",
    "        # get columns of rule book that alias in their names\n",
    "        self.__res = [\n",
    "            alias_col for alias_col in alias_cols if subs in alias_col]\n",
    "        self.__rule = rule\n",
    "\n",
    "        # create empty dataframe for the logs\n",
    "        self.__df_logs = pd.DataFrame(columns=[\n",
    "            'rule_name', 'aliases', 'criticality', 'time_local', 'test', 'date_update','qa'])\n",
    "        aliases = []\n",
    "        # get a list of aliases for the given targets in the rule book and append that lists\n",
    "        # in the aliases list\n",
    "\n",
    "        # in case alias_a is not null\n",
    "        if rule[self.__res[0]] is not None:\n",
    "\n",
    "            for i in self.__res:\n",
    "                if rule[i] is not None and '-' not in rule[i]:\n",
    "                    # get all aliases for given keywords in config columns\n",
    "                    aliases.append(self.__alias_substrings(\n",
    "                        batch, [rule[i]], col_name='alias'))\n",
    "\n",
    "                # In case minus sign is found to exclude that given alias\n",
    "                elif rule[i] is not None and '-' in rule[i]:\n",
    "                    keyword = rule[i].replace('-', '').strip()\n",
    "                    # get all aliases for given keyword in config columns\n",
    "                    excluded_aliases = self.__alias_substrings(\n",
    "                        batch, [keyword], col_name='alias')\n",
    "                    # get all columns(aliases) of the dataframe\n",
    "                    unique_aliases = batch.columns.to_list()\n",
    "                    # drop the given keyword's aliases and time_local from all aliases in dataframe\n",
    "                    required_aliases = list(\n",
    "                        set(unique_aliases) - set(excluded_aliases) - set(['time_local']))\n",
    "                    # append required aliases in the aliases list\n",
    "                    aliases.append(required_aliases)\n",
    "\n",
    "            # in case there is only one group of aliases in aliases list\n",
    "            if len(aliases) == 1:\n",
    "                # iterate over all group of  alaises\n",
    "                for alias in aliases[0]:\n",
    "\n",
    "                    # get a dataframe for each alias\n",
    "                    df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                              resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                    # in case dataframe is not empty\n",
    "                    if df.shape[0] > 0:\n",
    "                        # apply the equation that is given in the rule book (equation_1)\n",
    "                        df = self.apply_equation_1(\n",
    "                            self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                        df = self.apply_equation_2(\n",
    "                            self, df, rule, equation=rule['equation_2'])\n",
    "                        # create logs\n",
    "                        df1 = self.__logs_data(df, rule, [alias])\n",
    "                        self.__df_logs = pd.concat(\n",
    "                            [self.__df_logs, df1], axis=0)\n",
    "\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {alias}')\n",
    "            # if in case there are multiple aliases groups in aliases list\n",
    "            else:\n",
    "\n",
    "                # create a list of different pairs of aliases\n",
    "                alias_pair = self.__alias_pairs(self, batch, rule, aliases)\n",
    "\n",
    "                # iterate for all pairs\n",
    "                for pair in alias_pair:\n",
    "\n",
    "                    # in case length of a pair is not equal to 0\n",
    "                    if len(pair) != 0:\n",
    "                        # get a dataframe for each pair\n",
    "                        df = self.alias_pairs_dataframe(self, data_rd=batch, aliases=pair, kpi=rule['rule_name'],\n",
    "                                                        resample_grain=None, daily_resample=None, agg=None,\n",
    "                                                        non_negative=None)\n",
    "                        # in case dataframe is not empty\n",
    "                        if df.shape[0] > 0:\n",
    "                            # apply the equation that is given in the rule book (equation_1)\n",
    "                            df = self.apply_equation_1(\n",
    "                                self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                            df = self.apply_equation_2(\n",
    "                                self, df, rule, equation=rule['equation_2'])\n",
    "                            #                     # create logs\n",
    "                            df1 = self.__logs_data(df, rule, pair)\n",
    "                            self.__df_logs = pd.concat(\n",
    "                                [self.__df_logs, df1], axis=0)\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {pair}')\n",
    "        # if in case alias_a is null\n",
    "        else:\n",
    "            # get all columns (aliases) of the dataframe except time_local or time\n",
    "            aliases.append(self.__alias_substrings(\n",
    "                batch, None, col_name='alias'))\n",
    "            # iterate for all aliases\n",
    "            for alias in aliases[0]:\n",
    "                # get a dataframe for each pair\n",
    "                df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                          resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                # in case dataframe is not empty\n",
    "                if df.shape[0] > 0:\n",
    "                    # apply the equation that is given in the rule book (equation_1)\n",
    "                    df = self.apply_equation_1(\n",
    "                        self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                    df = self.apply_equation_2(\n",
    "                        self, df, rule, equation=rule['equation_2'])\n",
    "                    #                     # create logs\n",
    "                    df1 = self.__logs_data(df, rule, [alias])\n",
    "                    self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "                # in case dataframe is empty\n",
    "                else:\n",
    "                    print(f'Data is not available for {alias}')\n",
    "\n",
    "    @staticmethod\n",
    "    def __logs_data(df=None, rule=None, alias_list=None, result=None):\n",
    "        \"\"\" Gets resultant dataframe and rules for each kpi and creates logs\n",
    "            in the form of a dataframe.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        Resultant dataframe for each kpi\n",
    "        rule:dataframe\n",
    "        Its the config file\n",
    "        tgt_alias:string\n",
    "        Name of the target alias\n",
    "        ref_alias: string\n",
    "        Name of the reference alias\n",
    "        \"\"\"\n",
    "        # create an empty dataframe\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "        # concate all aliases as one string\n",
    "        if len(alias_list) == 1:\n",
    "            logs_alias = alias_list[0]\n",
    "        else:\n",
    "            logs_alias = \",\".join(alias_list)\n",
    "\n",
    "        # get current utc time (function running time) for date_update\n",
    "        # current_time = datetime.datetime.utcnow().replace(\n",
    "        #     second=0, microsecond=0)\n",
    "\n",
    "        df1 = df[['test', 'time_local']].copy()\n",
    "        df1['aliases'] = logs_alias\n",
    "        df1['rule_name'] = rule['rule_name']\n",
    "        df1['criticality'] = rule['criticality']\n",
    "        df1['qa'] = rule['qa']\n",
    "\n",
    "        return df1\n",
    "\n",
    "    @staticmethod\n",
    "    def count(df, max_delta):\n",
    "\n",
    "        try:\n",
    "            count_true = df['value'].value_counts()[True]\n",
    "        except:\n",
    "            count_true = 0\n",
    "\n",
    "        return count_true\n",
    "\n",
    "    @staticmethod\n",
    "    def streak(df):\n",
    "        \"\"\"This function calculates delta for only true values, in case\n",
    "        this calculated delta is greater or equal to the max_delta then\n",
    "        test will fail otherwise pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dataframe\n",
    "        Dataframe after applying equation\n",
    "        rule: dataframe\n",
    "        Each row of the config file as a dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        df: dataframe\n",
    "        delta: float\n",
    "        \"\"\"\n",
    "\n",
    "        # set True value to 1 and False to 0\n",
    "        df.loc[df[\"value\"] == True, \"value\"] = 1\n",
    "        df.loc[df[\"value\"] == False, \"value\"] = 0\n",
    "        # Check that original values are not equal to shifted values\n",
    "        df['expected_streak'] = df.value.ne(df.value.shift())\n",
    "        # get timelocal for the start of streak\n",
    "        df['streak_time'] = df.loc[df.expected_streak == True]['time_local']\n",
    "        # convert streak start time to datetime\n",
    "        df['streak_time'] = pd.to_datetime(df['streak_time'])\n",
    "        # converte time_local to datetime\n",
    "        df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "        # fill null values from previous values by using forward filling\n",
    "        df = df.fillna(method='ffill')\n",
    "        # calculate delta in minutes\n",
    "        df['delta'] = (df['time_local'] - df['streak_time']\n",
    "                       ).dt.total_seconds() / 60\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_equation_1(self, df=None, equation=None, temp=None):\n",
    "        \"\"\" This function basically takes a dataframe and an equation\n",
    "        and then tries to apply this equation and results in the form of boolean.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: Dataframe\n",
    "        A Dataframe for an aliases or a pair of aliases\n",
    "        equation: String\n",
    "        This string has a set of instruction\n",
    "        Returns\n",
    "        -------\n",
    "        df: Dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        def func(x):\n",
    "            y = eval(equation)\n",
    "            return y\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "            # in case equation is given and not missing_values or constant values\n",
    "            if equation is not None:\n",
    "                # iterate for given given alias columns in the config file\n",
    "                for col in self.__res:\n",
    "                    # add dataframe with alias column\n",
    "                    if col in equation:\n",
    "                        col_name = f\"x['{col}']\"\n",
    "                        equation = equation.replace(col, col_name)\n",
    "\n",
    "                # time and date base conditions\n",
    "                if 'day' in equation:\n",
    "                    df['day'] = df.time_local.dt.dayofweek\n",
    "                    equation = equation.replace('day', 'x[\"day\"]')\n",
    "                if 'date' in equation:\n",
    "                    df['date'] = df.time_local.dt.date\n",
    "                    equation = equation.replace('date', 'str(x[\"date\"])')\n",
    "                if 'time' in equation:\n",
    "                    df['time'] = df.time_local.apply(lambda x: x.strftime('%H:%M'))\n",
    "                    equation = equation.replace('time', 'str(x[\"time\"])')\n",
    "                if 'hour' in equation:\n",
    "                    df['hour'] = df.time_local.dt.hour\n",
    "                    equation = equation.replace('hour', 'x[\"hour\"]')\n",
    "                if 'minute' in equation:\n",
    "                    df['minute'] = df.time_local.dt.minute\n",
    "                    equation = equation.replace('minute', 'x[\"minute\"]')\n",
    "                if 'month' in equation:\n",
    "                    df['month'] = df.time_local.dt.month\n",
    "                    equation = equation.replace('month', 'x[\"month\"]')\n",
    "\n",
    "                # apply equation\n",
    "                df['value'] = df.apply(func, axis=1)\n",
    "            # in case equation is None\n",
    "            else:\n",
    "                df['value'] = df['alias_a']\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_equation_2(self, df=None, rule=None, equation=None, kpi=None):\n",
    "        if df.shape[0] > 0:\n",
    "            if equation is not None:\n",
    "                if 'streak' in equation:\n",
    "                    df = self.streak(df)\n",
    "                    duration = float(re.findall(\"[0-9.]+\", equation)[0])\n",
    "                    frac, whole = math.modf(duration)\n",
    "                    duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                    equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "                    df['test'] = eval(equation)\n",
    "                    \n",
    "\n",
    "                else:\n",
    "                    aggregation = re.findall('\\w+', equation)[0]\n",
    "                    operator = re.findall(\"[^a-zA-Z0-9_.,\\s\\(\\)\\[\\]]+\", equation)[0]\n",
    "                    window = rule['duration_hours']\n",
    "                    duration = str(window) + 'h'\n",
    "                    threshold = float(re.findall(\"[0-9]+\", equation)[0])\n",
    "                    df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "                    df.set_index('time_local', inplace=True)\n",
    "                    df['test'] = eval(f\"df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\")\n",
    "                    df.reset_index(inplace=True)\n",
    "            else:\n",
    "                df = self.streak(df)\n",
    "                duration = float(rule['duration_hours'])\n",
    "                frac, whole = math.modf(duration)\n",
    "                duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "                df['test'] = eval(equation)\n",
    "\n",
    "            #             display(df)\n",
    "            df = df.loc[df.test == True]\n",
    "            df.test = 'fail'\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_substrings(df, in_list, col_name='alias'):\n",
    "        \"\"\" This function returns a list of aliases for a given reference in the\n",
    "            the config file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        It's the source dataframe\n",
    "        in_list: list\n",
    "        It's the list of given aliases in the config file\n",
    "        col_name: string\n",
    "        It's the column name in the source dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_aliases: list\n",
    "        This list contains the aliases that are filtered out\n",
    "        \"\"\"\n",
    "\n",
    "        groups = in_list\n",
    "\n",
    "        if in_list is not None:\n",
    "            filtered_aliases = []\n",
    "            substrings = []\n",
    "\n",
    "            # Creating list of substrings\n",
    "            for group_id in groups:\n",
    "                # print(group_id)\n",
    "                substrings.append(group_id.split('_'))\n",
    "\n",
    "            unique_aliases = df.columns\n",
    "            # print(unique_aliases)\n",
    "            # Removing None from the list of unique aliases\n",
    "            unique_aliases = list(filter(None, unique_aliases))\n",
    "            # Filter the aliases that are to be used for the KPI\n",
    "            for alias in unique_aliases:\n",
    "                for sub in substrings:\n",
    "                    if all(a.strip() in alias for a in sub):\n",
    "                        # print(alias)\n",
    "                        filtered_aliases.append(alias)\n",
    "        else:\n",
    "\n",
    "            remove_cols = ['time_local']\n",
    "            filtered_aliases = list(set(df.columns.to_list()) - set(remove_cols))\n",
    "\n",
    "        return filtered_aliases\n",
    "\n",
    "    @staticmethod\n",
    "    def __batch_subset(batch, duration):\n",
    "        \"\"\"\n",
    "        Gets a dataframe and a specific duration, where it takes a subset\n",
    "        of dataframe that falls in the specific hours of duration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: dataframe\n",
    "        It's the main dataframe whose subset is to be taken based on time column.\n",
    "        duration:\n",
    "        A time period for which a subset of the dataframe\n",
    "        will be taken.\n",
    "        Returns\n",
    "        -------\n",
    "        batch_subset: dataframe\n",
    "        A subset of the main dataframe for which timing is in between\n",
    "        the specific duration of hours.\n",
    "        batch_min_time: time stamp\n",
    "        It's the start time of the subset dataframe\n",
    "        batch_max_time: time stamp\n",
    "        It's the end time of the subset dataframe.\n",
    "        \"\"\"\n",
    "        # set min and max time variable as global\n",
    "        global batch_min_time\n",
    "        global batch_max_time\n",
    "\n",
    "        # get maximum timestamp of the batch\n",
    "        batch_max_time = str(arrow.get(batch['time_local'].max()).datetime).split('+')[0]\n",
    "        # convert string formated timestamp to datetime object\n",
    "        batch_max_time = datetime.datetime.strptime(batch_max_time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # get minimum timestamp by considering duration hours\n",
    "        batch_min_time = batch_max_time - datetime.timedelta(hours=int(duration),\n",
    "                                                             minutes=int(round(duration % 1, 4) * 60))\n",
    "        # get subset of data for the minimum and maximum timestamps\n",
    "        batch_subset = batch[(batch.time_local >= str(batch_min_time)) & (batch.time_local <= str(batch_max_time))]\n",
    "\n",
    "        return batch_subset\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_pairs_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                              daily_resample=None, agg=None, non_negative=None):\n",
    "        # create an empty dataframe\n",
    "        df_master = pd.DataFrame()\n",
    "        # get indices of aliases's elements\n",
    "        res = [index for index, val in enumerate(aliases)]\n",
    "        # create copy of the aliases\n",
    "        alias_cols = aliases.copy()\n",
    "        # append time_local into the copied alaises\n",
    "        alias_cols.append('time_local')\n",
    "        # get config file aliases columns having same index as res list's elements\n",
    "        cols = [val for index, val in enumerate(self.__res) if index in res]\n",
    "        # append time_local into the cols list\n",
    "        cols.append('time_local')\n",
    "        # get data for the given aliases\n",
    "        df = data_rd[alias_cols].copy()\n",
    "        # assign config file alias column name\n",
    "        df.columns = cols\n",
    "        # in case there is data for the pair\n",
    "        if df.shape[0] > 0:\n",
    "            # assign df to df_master\n",
    "            df_master = df\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                        daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        df = data_rd[['time_local', aliases]].copy()\n",
    "        df.rename(columns={aliases: 'alias_a'}, inplace=True)\n",
    "        if df.shape[0] > 0:\n",
    "            df_master = df\n",
    "\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_pairs(self, data_rd, rule, aliases):\n",
    "        self.__index = [index for index, val in enumerate(aliases)]\n",
    "        col_names = self.__res\n",
    "        col_names = [val for index, val in enumerate(col_names) if index in self.__index]\n",
    "        null_list_index = [index for index, val in enumerate(aliases) if len(val) == 0]\n",
    "        non_null_dfs_list = []\n",
    "        null_dfs_list = []\n",
    "\n",
    "        for i in range(len(aliases)):\n",
    "\n",
    "            non_null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            if len(aliases[i]) > 0:\n",
    "                for alias in aliases[i]:\n",
    "                    keys = alias.split(\"_\")\n",
    "                    for ele in rule[col_names[i]].strip().split('_'):\n",
    "                        keys.remove(ele)\n",
    "\n",
    "                    if len(keys) == 0:\n",
    "\n",
    "                        df = pd.DataFrame([[alias, None]], columns=['alias', 'keys'])\n",
    "\n",
    "                        null_alias_df = pd.concat([null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        key = '_'.join(map(str, keys))\n",
    "                        df = pd.DataFrame([[alias, key]], columns=['alias', 'keys'])\n",
    "                        non_null_alias_df = pd.concat([non_null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "            if non_null_alias_df.shape[0] > 0:\n",
    "                non_null_dfs_list.append(non_null_alias_df)\n",
    "            if null_alias_df.shape[0] > 0:\n",
    "                null_dfs_list.append(null_alias_df)\n",
    "\n",
    "        if len(null_dfs_list) == len(col_names):\n",
    "            nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        else:\n",
    "\n",
    "            if len(non_null_dfs_list) > 1:\n",
    "                non_nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, on=[\"keys\"], how=\"inner\"),\n",
    "                                            non_null_dfs_list)\n",
    "            elif len(non_null_dfs_list) == 1:\n",
    "                non_nulls_df_merge = non_null_dfs_list[0]\n",
    "            else:\n",
    "                non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "            if len(null_dfs_list) > 1:\n",
    "                nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            elif len(null_dfs_list) == 1:\n",
    "                nulls_df_merge = null_dfs_list[0]\n",
    "            else:\n",
    "                nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "        if nulls_df_merge.shape[0] > 0 and non_nulls_df_merge.shape[0] > 0:\n",
    "            alias_pairs_df = pd.merge(nulls_df_merge, non_nulls_df_merge, how='cross')\n",
    "        elif nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = non_nulls_df_merge\n",
    "        elif non_nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = nulls_df_merge\n",
    "\n",
    "        subs = 'keys'\n",
    "        keys_cols = [col for col in alias_pairs_df if subs in col]\n",
    "        alias_pairs_df = alias_pairs_df.drop(keys_cols, axis=1)\n",
    "\n",
    "        if alias_pairs_df.shape[1] == len(col_names):\n",
    "            alias_pairs = alias_pairs_df.values.tolist()\n",
    "        else:\n",
    "            alias_pairs = []\n",
    "\n",
    "        return alias_pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def message_builder(logs, data_end_time, data, client_id, logs_link):\n",
    "        \"\"\"\n",
    "        Gets a dataframe, that has information about the tests that are either passed of failed\n",
    "        Parameters\n",
    "        ----------\n",
    "        logs_df: Dataframe\n",
    "        This is the dataframe, which contains different tests that are performed.\n",
    "        Returns\n",
    "        -------\n",
    "        message: string\n",
    "        It contains\n",
    "        \"\"\"\n",
    "        message = f\"*{client_id}*  {data_end_time}\\n\"\n",
    "        if (len(logs.loc[logs.test == 'fail']) > 0):\n",
    "            df_false = logs.loc[logs['test'] == 'fail']\n",
    "            rule_names = df_false.loc[df_false.criticality == 'high']['rule_name'].unique()\n",
    "            for i in range(len(rule_names)):\n",
    "                message = message + f\"\\n *{rule_names[i]}*\"\n",
    "\n",
    "            if data.shape[0] > 0:\n",
    "                message = message + f\"\\n\\nDuplicate aliases\"\n",
    "                message = message + f\"\\n {data}\"\n",
    "\n",
    "            message = message + f\"\"\"\\n\n",
    "    <{logs_link}>\n",
    "            \"\"\"\n",
    "        return message\n",
    "\n",
    "    def slack_alert(message):\n",
    "        \"\"\"\n",
    "        Gets a string of information about the tests are failed having only high criticality, and\n",
    "        send this information to a slack channel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        message: string\n",
    "        Information about the tests that have high criticality.\n",
    "        Returns\n",
    "        -------\n",
    "        This function returns nothing\n",
    "        \"\"\"\n",
    "\n",
    "        webhook_url = SLACK_WEBHOOK\n",
    "        slack_data = {'text': message}\n",
    "\n",
    "        response = requests.post(\n",
    "            webhook_url, data=json.dumps(slack_data),\n",
    "            headers={'Content-Type': 'application/json'}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(\n",
    "                'Request to slack returned an error %s, the response is:\\n%s'\n",
    "                % (response.status_code, response.text)\n",
    "            )\n",
    "\n",
    "    def return_log(self):\n",
    "        return self.__df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\Thermosphr\\\\QA configs\\\\gera_qa_config_and.json') as file:\n",
    "    config = json.loads(file.read())\n",
    "    #print(config)\n",
    "    file.close()\n",
    "    \n",
    "#pd.DataFrame(config['rules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data between 2023-01-28 15:00:00 and 2023-01-31 15:00:00\n",
      "ready to run function\n",
      "water_almost_boiling_2h is muted\n",
      "water_almost_boiling_4h is muted\n",
      "water_almost_freezing_2h is muted\n",
      "water_almost_freezing_4h is muted\n",
      "negative_values_3h is muted\n",
      "negative_values_6h is muted\n",
      "CPU times: total: 8min 35s\n",
      "Wall time: 8min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kwargs = {\n",
    "    'project' : 'thermosphr-prod',\n",
    "    'dataset_src' : 'bi',\n",
    "    'dataset_dest' : 'raw',\n",
    "    'table_src' : 'datamart_v2',\n",
    "    'table_dest' : 'ws_live_qa_properties_logs',\n",
    "    'tz_info' : 'Europe/Paris',\n",
    "    'config' : config,\n",
    "    'client_id' : 'rosny2',\n",
    "    'device_id' : 'nYnda9jKG'\n",
    "}\n",
    "logs = hvac_function(**kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>device_id</th>\n",
       "      <th>rule_name</th>\n",
       "      <th>aliases</th>\n",
       "      <th>criticality</th>\n",
       "      <th>time_local</th>\n",
       "      <th>test</th>\n",
       "      <th>date_update</th>\n",
       "      <th>qa</th>\n",
       "      <th>mute_alert</th>\n",
       "      <th>data_start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159957</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>zone_4_heatExchanger_2_hot_water_entering_temp_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 03:45:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159958</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>zone_4_heatExchanger_2_hot_water_entering_temp_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159959</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>zone_4_heatExchanger_2_hot_water_entering_temp_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:15:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159960</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>zone_4_heatExchanger_2_hot_water_entering_temp_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:30:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159961</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>zone_4_heatExchanger_2_hot_water_entering_temp_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:45:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281598</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_23_co2_return_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 07:30:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281599</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_23_co2_return_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 07:45:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281600</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_23_co2_return_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 08:00:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281601</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_23_co2_return_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 08:15:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281602</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_23_co2_return_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 08:30:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:16:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121646 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       client_id  device_id          rule_name  \\\n",
       "159957    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159958    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159959    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159960    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159961    rosny2  nYnda9jKG  missing_values_3h   \n",
       "...          ...        ...                ...   \n",
       "281598    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281599    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281600    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281601    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281602    rosny2  nYnda9jKG  missing_values_6h   \n",
       "\n",
       "                                                  aliases criticality  \\\n",
       "159957  zone_4_heatExchanger_2_hot_water_entering_temp_sp         med   \n",
       "159958  zone_4_heatExchanger_2_hot_water_entering_temp_sp         med   \n",
       "159959  zone_4_heatExchanger_2_hot_water_entering_temp_sp         med   \n",
       "159960  zone_4_heatExchanger_2_hot_water_entering_temp_sp         med   \n",
       "159961  zone_4_heatExchanger_2_hot_water_entering_temp_sp         med   \n",
       "...                                                   ...         ...   \n",
       "281598                           ahu_23_co2_return_sensor        high   \n",
       "281599                           ahu_23_co2_return_sensor        high   \n",
       "281600                           ahu_23_co2_return_sensor        high   \n",
       "281601                           ahu_23_co2_return_sensor        high   \n",
       "281602                           ahu_23_co2_return_sensor        high   \n",
       "\n",
       "                 time_local  test          date_update       qa mute_alert  \\\n",
       "159957  2023-01-01 03:45:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "159958  2023-01-01 04:00:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "159959  2023-01-01 04:15:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "159960  2023-01-01 04:30:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "159961  2023-01-01 04:45:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "...                     ...   ...                  ...      ...        ...   \n",
       "281598  2023-01-04 07:30:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "281599  2023-01-04 07:45:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "281600  2023-01-04 08:00:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "281601  2023-01-04 08:15:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "281602  2023-01-04 08:30:00  fail  2023-02-09 13:16:00  data_qa        NaN   \n",
       "\n",
       "            data_start_time  \n",
       "159957  2023-01-28 15:00:00  \n",
       "159958  2023-01-28 15:00:00  \n",
       "159959  2023-01-28 15:00:00  \n",
       "159960  2023-01-28 15:00:00  \n",
       "159961  2023-01-28 15:00:00  \n",
       "...                     ...  \n",
       "281598  2023-01-28 15:00:00  \n",
       "281599  2023-01-28 15:00:00  \n",
       "281600  2023-01-28 15:00:00  \n",
       "281601  2023-01-28 15:00:00  \n",
       "281602  2023-01-28 15:00:00  \n",
       "\n",
       "[121646 rows x 11 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs[logs['rule_name'].str.contains('missing_values')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "939480ed579cbcc9bd95c0bb2f0a271d068ec362d36f1415ed941c7dadb52340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
