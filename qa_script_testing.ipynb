{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import regex as re\n",
    "import warnings\n",
    "import arrow\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "import arrow\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "from string import Template\n",
    "\n",
    "\n",
    "# supress warnings to debugging\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.auth\n",
    "import pandas_gbq\n",
    "import arrow\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from string import Template\n",
    "from datetime import datetime as dt, timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "\n",
    "def hvac_function(project, dataset_src, dataset_dest, table_src, table_dest, tz_info, config, client_id, device_id):\n",
    "    def unique_logs(project, client_id, device_id, client, dataset_dest, table_dest, templogs):\n",
    "        sql = client.query(f\"\"\"\n",
    "            select client_id,device_id,rule_name, aliases,criticality,test,qa,count(*) as count\n",
    "            from `{project}.{dataset_dest}.{table_dest}` where extract(date from time_local) = CURRENT_DATE()\n",
    "            and test='fail' and criticality = 'high'\n",
    "            and client_id = '{client_id}' and device_id = '{device_id}'\n",
    "            group by client_id,device_id,rule_name,aliases,criticality,test,qa\n",
    "        \"\"\")\n",
    "\n",
    "        previous_logs = sql.to_dataframe()\n",
    "        #print(previous_logs)\n",
    "        merged_templogs = pd.merge(templogs, previous_logs,\n",
    "                                   on=['client_id', 'device_id', 'rule_name', 'aliases', 'criticality', 'test', 'qa'],\n",
    "                                   how='left')\n",
    "\n",
    "        merged_templogs['count'] = merged_templogs['count'].fillna(0)\n",
    "        merged_templogs = merged_templogs[merged_templogs['count'] <= 3]\n",
    "        if merged_templogs.shape[0] >= 1:\n",
    "            return merged_templogs[['client_id',\n",
    "                                    'device_id',\n",
    "                                    'rule_name',\n",
    "                                    'aliases',\n",
    "                                    'criticality',\n",
    "                                    'time_local',\n",
    "                                    'test',\n",
    "                                    'date_update',\n",
    "                                    'qa']]\n",
    "        else:\n",
    "            return pd.DataFrame(columns=['client_id',\n",
    "                                         'device_id',\n",
    "                                         'rule_name',\n",
    "                                         'aliases',\n",
    "                                         'criticality',\n",
    "                                         'time_local',\n",
    "                                         'test',\n",
    "                                         'date_update',\n",
    "                                         'qa'])\n",
    "\n",
    "    def create_logs(rule, data_rd, logs, templogs):\n",
    "        obj = geFramework(rule, data_rd)\n",
    "\n",
    "        if obj is not None:\n",
    "            dff = obj.return_log()\n",
    "            logs = pd.concat([logs, dff], axis=0)\n",
    "\n",
    "            dff = dff.reset_index(drop=True)\n",
    "            if rule['mute_alert'] is None:\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "            else:\n",
    "                aliases_list = rule['mute_alert'].split(',')\n",
    "\n",
    "                for j in aliases_list:\n",
    "                    dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "                    logs.loc[logs.aliases.str.contains(j.strip()),\"mute_alert\"] = \"muted\"\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "            if rule['mute_logs'] is None:\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "                clear_logs = logs\n",
    "            elif rule['mute_logs'].lower() == 'all':\n",
    "                dff = dff[~dff.rule_name.str.contains(rule['rule_name'])]\n",
    "\n",
    "                clear_logs = logs[~logs.rule_name.str.contains(rule['rule_name'])]\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "            else:\n",
    "                aliases_list = rule['mute_logs'].split(',')\n",
    "\n",
    "                for j in aliases_list:\n",
    "                    dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "                    clear_logs = logs[~logs.aliases.str.contains(j.strip())]\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "        return clear_logs, templogs\n",
    "\n",
    "    output = config\n",
    "\n",
    "    rules = pd.DataFrame(output['rules'])\n",
    "    rules.index = rules.index.astype(int)\n",
    "    rules.sort_index(inplace=True)\n",
    "    rules = rules[['rule_name',\n",
    "                   'alias_a',\n",
    "                   'alias_b',\n",
    "                   'alias_c',\n",
    "                   'alias_d',\n",
    "                   'equation_1',\n",
    "                   'equation_2',\n",
    "                   'duration_hours',\n",
    "                   'criticality',\n",
    "                   'start_date',\n",
    "                   'end_date',\n",
    "                   'mute_alert',\n",
    "                   'mute_logs',\n",
    "                   'qa']].copy()\n",
    "\n",
    "    rules = rules.where(pd.notnull(rules), None)\n",
    "    settings = output['settings']['mute_rules']\n",
    "    logs_link = output['logs_link']\n",
    "    duration_hours = rules['duration_hours'].to_list()\n",
    "\n",
    "    logs = pd.DataFrame(columns=[\"client_id\",\n",
    "                                 \"device_id\",\n",
    "                                 \"rule_name\",\n",
    "                                 \"aliases\",\n",
    "                                 \"criticality\",\n",
    "                                 \"time_local\",\n",
    "                                 \"test\",\n",
    "                                 \"date_update\",\n",
    "                                 \"qa\",\n",
    "                                 \"mute_alert\",\n",
    "                                 \"data_start_time\"])\n",
    "    templogs = pd.DataFrame(columns=[\"client_id\",\n",
    "                                     \"device_id\",\n",
    "                                     \"rule_name\",\n",
    "                                     \"aliases\",\n",
    "                                     \"criticality\",\n",
    "                                     \"time_local\",\n",
    "                                     \"test\",\n",
    "                                     \"date_update\",\n",
    "                                     \"qa\",\n",
    "                                     \"mute_alert\",\n",
    "                                     \"data_start_time\"])\n",
    "\n",
    "    # credentials, project_id = google.auth.default()\n",
    "\n",
    "    #client = bigquery.Client(project=project)\n",
    "    end_time = arrow.get('2023-01-31 15:00:00')\n",
    "    start = (end_time + datetime.timedelta(hours=-max(duration_hours) - 5)).date()\n",
    "    end = (end_time + datetime.timedelta(hours=max(duration_hours))).date()\n",
    "\n",
    "#     end_time = arrow.utcnow().replace(second=0, microsecond=0).to(tz_info).shift(minutes=-15)\n",
    "#     end_time = arrow.get('2023-01-31 15:00:00')\n",
    "    start_time = end_time + datetime.timedelta(hours=-max(duration_hours))\n",
    "\n",
    "    str_end_time = str(end_time.datetime).split('+')[0]\n",
    "    str_start_time = str(start_time.datetime).split('+')[0]\n",
    "    print(f'Data between {str_start_time} and {str_end_time}')\n",
    "    if settings is False:\n",
    "       \n",
    "        data_rd = pd.read_csv('D:\\Thermosphr\\QA Vectoize task\\main_data.csv')\n",
    "        \n",
    "        date = arrow.utcnow().date()\n",
    "        date = date.replace(year=1900)\n",
    "        if data_rd.shape[0] > 0:\n",
    "\n",
    "            print('ready to run function')\n",
    "\n",
    "            # recursively apply rules over data\n",
    "\n",
    "            for i in range(rules.shape[0]):\n",
    "#                 if rules.iloc[i]['rule_name'] == 'constant_values_24h':\n",
    "                if not rules.iloc[i]['mute_logs']:\n",
    "                    start_date = rules.iloc[i]['start_date']\n",
    "                    end_date = rules.iloc[i]['end_date']\n",
    "\n",
    "                    if start_date == None and end_date == None:\n",
    "\n",
    "                        logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "\n",
    "                    elif start_date != None and end_date != None:\n",
    "\n",
    "                        start_date = dt.strptime(start_date, '%m-%d').date()\n",
    "                        end_date = dt.strptime(end_date, '%m-%d').date()\n",
    "\n",
    "                        if start_date < end_date:\n",
    "                            if (start_date <= date and end_date >= date):\n",
    "                                logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "                        elif start_date > end_date:\n",
    "                            if (start_date >= date and end_date >= date):\n",
    "                                logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "                        else:\n",
    "                            print('Logs not created for this rule  :  ', rules.iloc[i]['rule_name'])\n",
    "                else:\n",
    "                    print(f\"{rules.iloc[i]['rule_name']} is muted\")\n",
    "\n",
    "            current_time = datetime.datetime.utcnow().replace(\n",
    "                second=0, microsecond=0)\n",
    "            # Finding out duplicate aliases if any\n",
    "#             df1 = dataframe[[\"alias\", \"propertyId\"]].groupby([\"alias\", \"propertyId\"]).count().reset_index()\n",
    "#             df2 = df1[['alias']].groupby(['alias'])['alias'].count().reset_index(name='count')\n",
    "#             df3 = df2.loc[df2['count'] > 1].sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "            templogs['client_id'] = client_id\n",
    "            templogs['device_id'] = device_id\n",
    "            templogs = templogs[~templogs.duplicated()].reset_index(drop=True)\n",
    "            templogs = templogs[templogs.time_local == str_end_time]\n",
    "         \n",
    "\n",
    "            logs['client_id'] = client_id\n",
    "            logs['device_id'] = device_id\n",
    "            logs['data_start_time'] = pd.to_datetime(str_start_time, format='%Y-%m-%d %H:%M:%S')\n",
    "            logs['date_update'] = pd.to_datetime(str(current_time), format='%Y-%m-%d %H:%M:%S')\n",
    "            #logs = logs[logs.time_local == str_end_time]\n",
    "            logs = logs[~logs.duplicated()].reset_index(drop=True)\n",
    "            logs[['time_local', 'date_update', 'data_start_time']] = logs[['time_local', 'date_update', 'data_start_time']].astype(str)\n",
    "            #table = client.get_table(f\"{project}.{dataset_dest}.{table_dest}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('No Data for given time period:')\n",
    "            print(start_time)\n",
    "            print(end_time)\n",
    "    else:\n",
    "        print(\"All rules are muted\")\n",
    "        \n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class geFramework():\n",
    "    def __init__(self, rule, batch):\n",
    "#         print(rule)\n",
    "        #print(rule['equation_1'])\n",
    "        \n",
    "        # get subset of data, min-time and max-time for the given start and end time\n",
    "        #batch = self.__batch_subset(batch, rule['duration_hours'])\n",
    "        if 'missing_values' not in rule['rule_name']:\n",
    "            batch.fillna(method='ffill', inplace=True)\n",
    "        # indices of the rule dataframe\n",
    "        alias_cols = rule.index\n",
    "        subs = 'alias'\n",
    "        # get columns of rule book that alias in their names\n",
    "        self.__res = [\n",
    "            alias_col for alias_col in alias_cols if subs in alias_col]\n",
    "        self.__rule = rule\n",
    "\n",
    "        # create empty dataframe for the logs\n",
    "        self.__df_logs = pd.DataFrame(columns=[\n",
    "            'rule_name', 'aliases', 'criticality', 'time_local', 'test', 'date_update','qa'])\n",
    "        aliases = []\n",
    "        # get a list of aliases for the given targets in the rule book and append that lists\n",
    "        # in the aliases list\n",
    "\n",
    "        # in case alias_a is not null\n",
    "        if rule[self.__res[0]] is not None:\n",
    "\n",
    "            for i in self.__res:\n",
    "                if rule[i] is not None and '-' not in rule[i]:\n",
    "                    # get all aliases for given keywords in config columns\n",
    "                    aliases.append(self.__alias_substrings(\n",
    "                        batch, [rule[i]], col_name='alias'))\n",
    "\n",
    "                # In case minus sign is found to exclude that given alias\n",
    "                elif rule[i] is not None and '-' in rule[i]:\n",
    "                    keyword = rule[i].replace('-', '').strip()\n",
    "                    # get all aliases for given keyword in config columns\n",
    "                    excluded_aliases = self.__alias_substrings(\n",
    "                        batch, [keyword], col_name='alias')\n",
    "                    # get all columns(aliases) of the dataframe\n",
    "                    unique_aliases = batch.columns.to_list()\n",
    "                    # drop the given keyword's aliases and time_local from all aliases in dataframe\n",
    "                    required_aliases = list(\n",
    "                        set(unique_aliases) - set(excluded_aliases) - set(['time_local']))\n",
    "                    # append required aliases in the aliases list\n",
    "                    aliases.append(required_aliases)\n",
    "\n",
    "            # in case there is only one group of aliases in aliases list\n",
    "            if len(aliases) == 1:\n",
    "                # iterate over all group of  alaises\n",
    "                for alias in aliases[0]:\n",
    "\n",
    "                    # get a dataframe for each alias\n",
    "                    df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                              resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                    # in case dataframe is not empty\n",
    "                    if df.shape[0] > 0:\n",
    "                        # apply the equation that is given in the rule book (equation_1)\n",
    "                        df = self.apply_equation_1(\n",
    "                            self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                        df = self.apply_equation_2(\n",
    "                            self, df, rule, equation=rule['equation_2'])\n",
    "                        # create logs\n",
    "                        df1 = self.__logs_data(df, rule, [alias])\n",
    "                        self.__df_logs = pd.concat(\n",
    "                            [self.__df_logs, df1], axis=0)\n",
    "\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {alias}')\n",
    "            # if in case there are multiple aliases groups in aliases list\n",
    "            else:\n",
    "\n",
    "                # create a list of different pairs of aliases\n",
    "                alias_pair = self.__alias_pairs(self, batch, rule, aliases)\n",
    "\n",
    "                # iterate for all pairs\n",
    "                for pair in alias_pair:\n",
    "\n",
    "                    # in case length of a pair is not equal to 0\n",
    "                    if len(pair) != 0:\n",
    "                        # get a dataframe for each pair\n",
    "                        df = self.alias_pairs_dataframe(self, data_rd=batch, aliases=pair, kpi=rule['rule_name'],\n",
    "                                                        resample_grain=None, daily_resample=None, agg=None,\n",
    "                                                        non_negative=None)\n",
    "                        # in case dataframe is not empty\n",
    "                        if df.shape[0] > 0:\n",
    "                            # apply the equation that is given in the rule book (equation_1)\n",
    "                            df = self.apply_equation_1(\n",
    "                                self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                            df = self.apply_equation_2(\n",
    "                                self, df, rule, equation=rule['equation_2'])\n",
    "                            #                     # create logs\n",
    "                            df1 = self.__logs_data(df, rule, pair)\n",
    "                            self.__df_logs = pd.concat(\n",
    "                                [self.__df_logs, df1], axis=0)\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {pair}')\n",
    "        # if in case alias_a is null\n",
    "        else:\n",
    "            # get all columns (aliases) of the dataframe except time_local or time\n",
    "            aliases.append(self.__alias_substrings(\n",
    "                batch, None, col_name='alias'))\n",
    "            # iterate for all aliases\n",
    "            for alias in aliases[0]:\n",
    "                # get a dataframe for each pair\n",
    "                df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                          resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                # in case dataframe is not empty\n",
    "                if df.shape[0] > 0:\n",
    "                    # apply the equation that is given in the rule book (equation_1)\n",
    "                    df = self.apply_equation_1(\n",
    "                        self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                    df = self.apply_equation_2(\n",
    "                        self, df, rule, equation=rule['equation_2'])\n",
    "                    #                     # create logs\n",
    "                    df1 = self.__logs_data(df, rule, [alias])\n",
    "                    self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "                # in case dataframe is empty\n",
    "                else:\n",
    "                    print(f'Data is not available for {alias}')\n",
    "\n",
    "    @staticmethod\n",
    "    def __logs_data(df=None, rule=None, alias_list=None, result=None):\n",
    "        \"\"\" Gets resultant dataframe and rules for each kpi and creates logs\n",
    "            in the form of a dataframe.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        Resultant dataframe for each kpi\n",
    "        rule:dataframe\n",
    "        Its the config file\n",
    "        tgt_alias:string\n",
    "        Name of the target alias\n",
    "        ref_alias: string\n",
    "        Name of the reference alias\n",
    "        \"\"\"\n",
    "        # create an empty dataframe\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "        # concate all aliases as one string\n",
    "        if len(alias_list) == 1:\n",
    "            logs_alias = alias_list[0]\n",
    "        else:\n",
    "            logs_alias = \",\".join(alias_list)\n",
    "\n",
    "        # get current utc time (function running time) for date_update\n",
    "        # current_time = datetime.datetime.utcnow().replace(\n",
    "        #     second=0, microsecond=0)\n",
    "\n",
    "        df1 = df[['test', 'time_local']].copy()\n",
    "        df1['aliases'] = logs_alias\n",
    "        df1['rule_name'] = rule['rule_name']\n",
    "        df1['criticality'] = rule['criticality']\n",
    "        df1['qa'] = rule['qa']\n",
    "\n",
    "        return df1\n",
    "\n",
    "    @staticmethod\n",
    "    def count(df, max_delta):\n",
    "\n",
    "        try:\n",
    "            count_true = df['value'].value_counts()[True]\n",
    "        except:\n",
    "            count_true = 0\n",
    "\n",
    "        return count_true\n",
    "\n",
    "    @staticmethod\n",
    "    def streak(df):\n",
    "        \"\"\"This function calculates delta for only true values, in case\n",
    "        this calculated delta is greater or equal to the max_delta then\n",
    "        test will fail otherwise pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dataframe\n",
    "        Dataframe after applying equation\n",
    "        rule: dataframe\n",
    "        Each row of the config file as a dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        df: dataframe\n",
    "        delta: float\n",
    "        \"\"\"\n",
    "\n",
    "        # set True value to 1 and False to 0\n",
    "        df.loc[df[\"value\"] == True, \"value\"] = 1\n",
    "        df.loc[df[\"value\"] == False, \"value\"] = 0\n",
    "        # Check that original values are not equal to shifted values\n",
    "        df['expected_streak'] = df.value.ne(df.value.shift())\n",
    "        # get timelocal for the start of streak\n",
    "        df['streak_time'] = df.loc[df.expected_streak == True]['time_local']\n",
    "        # convert streak start time to datetime\n",
    "        df['streak_time'] = pd.to_datetime(df['streak_time'])\n",
    "        # converte time_local to datetime\n",
    "        df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "        # fill null values from previous values by using forward filling\n",
    "        df = df.fillna(method='ffill')\n",
    "        # calculate delta in minutes\n",
    "        df['delta'] = (df['time_local'] - df['streak_time']\n",
    "                       ).dt.total_seconds() / 60\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_equation_1(self, df=None, equation=None, temp=None):\n",
    "        \"\"\" This function basically takes a dataframe and an equation\n",
    "        and then tries to apply this equation and results in the form of boolean.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: Dataframe\n",
    "        A Dataframe for an aliases or a pair of aliases\n",
    "        equation: String\n",
    "        This string has a set of instruction\n",
    "        Returns\n",
    "        -------\n",
    "        df: Dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        def func(x):\n",
    "            y = eval(equation)\n",
    "            return y\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "            # in case equation is given and not missing_values or constant values\n",
    "            if equation is not None:\n",
    "                # iterate for given given alias columns in the config file\n",
    "                for col in self.__res:\n",
    "                    #add dataframe with alias column\n",
    "                    if col in equation:\n",
    "                        if col in equation:\n",
    "                            #col_name = f\"x['{col}']\"\n",
    "                            col_name = f\"df['{col}'].values\"\n",
    "                            #pint(col_name)\n",
    "                            equation = equation.replace(col,col_name)\n",
    "                            #print(equation)\n",
    "\n",
    "                \n",
    "                # time and date base conditions\n",
    "                if 'day' in equation:\n",
    "                    df['day'] = df.time_local.dt.dayofweek\n",
    "                    equation = equation.replace('day', 'df[\"day\"].values')\n",
    "                if 'date' in equation:\n",
    "                    df['date'] = df.time_local.dt.date\n",
    "                    equation = equation.replace('date', 'df[\"date\"].astype(str).values')\n",
    "                if 'time' in equation:\n",
    "                    df['time'] = df.time_local.apply(lambda x: x.strftime('%H:%M'))\n",
    "                    equation = equation.replace('time', 'df[\"time\"].astype(str).values')\n",
    "                if 'hour' in equation:\n",
    "                    df['hour'] = df.time_local.dt.hour\n",
    "                    equation = equation.replace('hour', 'df[\"hour\"].values')\n",
    "                if 'minute' in equation:\n",
    "                    df['minute'] = df.time_local.dt.minute\n",
    "                    equation = equation.replace('minute', 'df[\"minute\"].values')\n",
    "                if 'month' in equation:\n",
    "                    df['month'] = df.time_local.dt.month\n",
    "                    equation = equation.replace('month', 'df[\"month\"].values')\n",
    "                    \n",
    "\n",
    "                # apply equation\n",
    "                #df['value'] = df.apply(func, axis=1)\n",
    "                df['value'] = eval(equation)\n",
    "                #df['value'] = np.where(eval(equation), 1, 0)\n",
    "                #df['value'] =np.where(df.eval(func), True, False)\n",
    "\n",
    "            # in case equation is None\n",
    "            else:\n",
    "                df['value'] = df['alias_a']\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_equation_2(self, df=None, rule=None, equation=None, kpi=None):\n",
    "#         print(rule['rule_name'])\n",
    "        if df.shape[0] > 0:\n",
    "            \n",
    "            if equation is not None:\n",
    "                if 'streak' in equation:\n",
    "                    df = self.streak(df)\n",
    "                    duration = float(re.findall(\"[0-9.]+\", equation)[0])\n",
    "                    frac, whole = math.modf(duration)\n",
    "                    duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                    equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "                    #df['test'] = eval(equation)\n",
    "#                     df['test'] = np.where((df.delta >= duration) & (df.value == True), True, False)\n",
    "\n",
    "                else:\n",
    "                    aggregation = re.findall('\\w+', equation)[0]\n",
    "                    operator = re.findall(\"[^a-zA-Z0-9_.,\\s\\(\\)\\[\\]]+\", equation)[0]\n",
    "                    window = rule['duration_hours']\n",
    "                    duration = str(window) + 'h'\n",
    "                    threshold = float(re.findall(\"[0-9]+\", equation)[0])\n",
    "                    df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "                    df.set_index('time_local', inplace=True)\n",
    "                    equation = f\"df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\"\n",
    "                    df.reset_index(inplace=True)\n",
    "                    \n",
    "\n",
    "#                 else:\n",
    "#                     aggregation = re.findall('\\w+', equation)[0]\n",
    "#                     operator = re.findall(\"[^a-zA-Z0-9_.,\\s\\(\\)\\[\\]]+\", equation)[0]\n",
    "#                     window = (rule['duration_hours'])\n",
    "                    \n",
    "#                     duration = str(window) + 'h'\n",
    "        \n",
    "#                     threshold = float(re.findall(\"[0-9]+\", equation)[0])\n",
    "                    \n",
    "#                     df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "#                     df.set_index('time_local', inplace=True)\n",
    "                    \n",
    "#                     #equation = f\"df['value'].rolling({window}*4,min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\"\n",
    "#                     equation = f\"df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\"\n",
    "#                     #df['test'] = np.where(df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') + {operator} + {threshold}, True, False)\n",
    "#                     #equation = f\"df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\")\n",
    "#                     df.reset_index(inplace=True)\n",
    "\n",
    "            \n",
    "    \n",
    "            else:\n",
    "                df = self.streak(df)\n",
    "                duration = float(rule['duration_hours'])\n",
    "                frac, whole = math.modf(duration)\n",
    "                duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "#                 df['test'] = np.where((df.delta >= duration) & (df.value == True), True, False)\n",
    "                #df['test'] = eval(equation)\n",
    "\n",
    "            df['test'] = eval(equation)\n",
    "            df = df.loc[df.test == True]\n",
    "            df.test = 'fail'\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_substrings(df, in_list, col_name='alias'):\n",
    "        \"\"\" This function returns a list of aliases for a given reference in the\n",
    "            the config file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        It's the source dataframe\n",
    "        in_list: list\n",
    "        It's the list of given aliases in the config file\n",
    "        col_name: string\n",
    "        It's the column name in the source dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_aliases: list\n",
    "        This list contains the aliases that are filtered out\n",
    "        \"\"\"\n",
    "\n",
    "        groups = in_list\n",
    "\n",
    "        if in_list is not None:\n",
    "            filtered_aliases = []\n",
    "            substrings = []\n",
    "\n",
    "            # Creating list of substrings\n",
    "            for group_id in groups:\n",
    "                # print(group_id)\n",
    "                substrings.append(group_id.split('_'))\n",
    "\n",
    "            unique_aliases = df.columns\n",
    "            # print(unique_aliases)\n",
    "            # Removing None from the list of unique aliases\n",
    "            unique_aliases = list(filter(None, unique_aliases))\n",
    "            # Filter the aliases that are to be used for the KPI\n",
    "            for alias in unique_aliases:\n",
    "                for sub in substrings:\n",
    "                    if all(a.strip() in alias for a in sub):\n",
    "                        # print(alias)\n",
    "                        filtered_aliases.append(alias)\n",
    "        else:\n",
    "\n",
    "            remove_cols = ['time_local']\n",
    "            filtered_aliases = list(set(df.columns.to_list()) - set(remove_cols))\n",
    "\n",
    "        return filtered_aliases\n",
    "\n",
    "    @staticmethod\n",
    "    def __batch_subset(batch, duration):\n",
    "        \"\"\"\n",
    "        Gets a dataframe and a specific duration, where it takes a subset\n",
    "        of dataframe that falls in the specific hours of duration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: dataframe\n",
    "        It's the main dataframe whose subset is to be taken based on time column.\n",
    "        duration:\n",
    "        A time period for which a subset of the dataframe\n",
    "        will be taken.\n",
    "        Returns\n",
    "        -------\n",
    "        batch_subset: dataframe\n",
    "        A subset of the main dataframe for which timing is in between\n",
    "        the specific duration of hours.\n",
    "        batch_min_time: time stamp\n",
    "        It's the start time of the subset dataframe\n",
    "        batch_max_time: time stamp\n",
    "        It's the end time of the subset dataframe.\n",
    "        \"\"\"\n",
    "        # set min and max time variable as global\n",
    "        global batch_min_time\n",
    "        global batch_max_time\n",
    "\n",
    "        # get maximum timestamp of the batch\n",
    "        batch_max_time = str(arrow.get(batch['time_local'].max()).datetime).split('+')[0]\n",
    "        # convert string formated timestamp to datetime object\n",
    "        batch_max_time = datetime.datetime.strptime(batch_max_time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # get minimum timestamp by considering duration hours\n",
    "        batch_min_time = batch_max_time - datetime.timedelta(hours=int(duration),\n",
    "                                                             minutes=int(round(duration % 1, 4) * 60))\n",
    "        # get subset of data for the minimum and maximum timestamps\n",
    "        batch_subset = batch[(batch.time_local >= str(batch_min_time)) & (batch.time_local <= str(batch_max_time))]\n",
    "\n",
    "        return batch_subset\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_pairs_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                              daily_resample=None, agg=None, non_negative=None):\n",
    "        # create an empty dataframe\n",
    "        df_master = pd.DataFrame()\n",
    "        # get indices of aliases's elements\n",
    "        res = [index for index, val in enumerate(aliases)]\n",
    "        # create copy of the aliases\n",
    "        alias_cols = aliases.copy()\n",
    "        # append time_local into the copied alaises\n",
    "        alias_cols.append('time_local')\n",
    "        # get config file aliases columns having same index as res list's elements\n",
    "        cols = [val for index, val in enumerate(self.__res) if index in res]\n",
    "        # append time_local into the cols list\n",
    "        cols.append('time_local')\n",
    "        # get data for the given aliases\n",
    "        df = data_rd[alias_cols].copy()\n",
    "        # assign config file alias column name\n",
    "        df.columns = cols\n",
    "        # in case there is data for the pair\n",
    "        if df.shape[0] > 0:\n",
    "            # assign df to df_master\n",
    "            df_master = df\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                        daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        df = data_rd[['time_local', aliases]].copy()\n",
    "        df.rename(columns={aliases: 'alias_a'}, inplace=True)\n",
    "        if df.shape[0] > 0:\n",
    "            df_master = df\n",
    "\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_pairs(self, data_rd, rule, aliases):\n",
    "        self.__index = [index for index, val in enumerate(aliases)]\n",
    "        col_names = self.__res\n",
    "        col_names = [val for index, val in enumerate(col_names) if index in self.__index]\n",
    "        null_list_index = [index for index, val in enumerate(aliases) if len(val) == 0]\n",
    "        non_null_dfs_list = []\n",
    "        null_dfs_list = []\n",
    "\n",
    "        for i in range(len(aliases)):\n",
    "\n",
    "            non_null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            if len(aliases[i]) > 0:\n",
    "                for alias in aliases[i]:\n",
    "                    keys = alias.split(\"_\")\n",
    "                    for ele in rule[col_names[i]].strip().split('_'):\n",
    "                        keys.remove(ele)\n",
    "\n",
    "                    if len(keys) == 0:\n",
    "\n",
    "                        df = pd.DataFrame([[alias, None]], columns=['alias', 'keys'])\n",
    "\n",
    "                        null_alias_df = pd.concat([null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        key = '_'.join(map(str, keys))\n",
    "                        df = pd.DataFrame([[alias, key]], columns=['alias', 'keys'])\n",
    "                        non_null_alias_df = pd.concat([non_null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "            if non_null_alias_df.shape[0] > 0:\n",
    "                non_null_dfs_list.append(non_null_alias_df)\n",
    "            if null_alias_df.shape[0] > 0:\n",
    "                null_dfs_list.append(null_alias_df)\n",
    "\n",
    "        if len(null_dfs_list) == len(col_names):\n",
    "            nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        else:\n",
    "\n",
    "            if len(non_null_dfs_list) > 1:\n",
    "                non_nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, on=[\"keys\"], how=\"inner\"),\n",
    "                                            non_null_dfs_list)\n",
    "            elif len(non_null_dfs_list) == 1:\n",
    "                non_nulls_df_merge = non_null_dfs_list[0]\n",
    "            else:\n",
    "                non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "            if len(null_dfs_list) > 1:\n",
    "                nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            elif len(null_dfs_list) == 1:\n",
    "                nulls_df_merge = null_dfs_list[0]\n",
    "            else:\n",
    "                nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "        if nulls_df_merge.shape[0] > 0 and non_nulls_df_merge.shape[0] > 0:\n",
    "            alias_pairs_df = pd.merge(nulls_df_merge, non_nulls_df_merge, how='cross')\n",
    "        elif nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = non_nulls_df_merge\n",
    "        elif non_nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = nulls_df_merge\n",
    "\n",
    "        subs = 'keys'\n",
    "        keys_cols = [col for col in alias_pairs_df if subs in col]\n",
    "        alias_pairs_df = alias_pairs_df.drop(keys_cols, axis=1)\n",
    "\n",
    "        if alias_pairs_df.shape[1] == len(col_names):\n",
    "            alias_pairs = alias_pairs_df.values.tolist()\n",
    "        else:\n",
    "            alias_pairs = []\n",
    "\n",
    "        return alias_pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def message_builder(logs, data_end_time, data, client_id, logs_link):\n",
    "        \"\"\"\n",
    "        Gets a dataframe, that has information about the tests that are either passed of failed\n",
    "        Parameters\n",
    "        ----------\n",
    "        logs_df: Dataframe\n",
    "        This is the dataframe, which contains different tests that are performed.\n",
    "        Returns\n",
    "        -------\n",
    "        message: string\n",
    "        It contains\n",
    "        \"\"\"\n",
    "        message = f\"*{client_id}*  {data_end_time}\\n\"\n",
    "        if (len(logs.loc[logs.test == 'fail']) > 0):\n",
    "            df_false = logs.loc[logs['test'] == 'fail']\n",
    "            rule_names = df_false.loc[df_false.criticality == 'high']['rule_name'].unique()\n",
    "            for i in range(len(rule_names)):\n",
    "                message = message + f\"\\n *{rule_names[i]}*\"\n",
    "\n",
    "            if data.shape[0] > 0:\n",
    "                message = message + f\"\\n\\nDuplicate aliases\"\n",
    "                message = message + f\"\\n {data}\"\n",
    "\n",
    "            message = message + f\"\"\"\\n\n",
    "    <{logs_link}>\n",
    "            \"\"\"\n",
    "        return message\n",
    "\n",
    "    def slack_alert(message):\n",
    "        \"\"\"\n",
    "        Gets a string of information about the tests are failed having only high criticality, and\n",
    "        send this information to a slack channel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        message: string\n",
    "        Information about the tests that have high criticality.\n",
    "        Returns\n",
    "        -------\n",
    "        This function returns nothing\n",
    "        \"\"\"\n",
    "\n",
    "        webhook_url = SLACK_WEBHOOK\n",
    "        slack_data = {'text': message}\n",
    "\n",
    "        response = requests.post(\n",
    "            webhook_url, data=json.dumps(slack_data),\n",
    "            headers={'Content-Type': 'application/json'}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(\n",
    "                'Request to slack returned an error %s, the response is:\\n%s'\n",
    "                % (response.status_code, response.text)\n",
    "            )\n",
    "\n",
    "    def return_log(self):\n",
    "        return self.__df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\Thermosphr\\\\QA configs\\\\gera_qa_config_update.json') as file:\n",
    "    config = json.loads(file.read())\n",
    "    file.close()\n",
    "#pd.set_option('display.max_rows',100)\n",
    "#pd.DataFrame(config['rules'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data between 2023-01-28 15:00:00 and 2023-01-31 15:00:00\n",
      "ready to run function\n",
      "water_almost_boiling_2h is muted\n",
      "water_almost_boiling_4h is muted\n",
      "water_almost_freezing_2h is muted\n",
      "water_almost_freezing_4h is muted\n",
      "negative_values_3h is muted\n",
      "negative_values_6h is muted\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "window must be an integer 0 or greater",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:12\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 176\u001b[0m, in \u001b[0;36mhvac_function\u001b[1;34m(project, dataset_src, dataset_dest, table_src, table_dest, tz_info, config, client_id, device_id)\u001b[0m\n\u001b[0;32m    172\u001b[0m end_date \u001b[38;5;241m=\u001b[39m rules\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_date \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m end_date \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     logs, templogs \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_rd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m start_date \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m end_date \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     start_date \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mstrptime(start_date, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdate()\n",
      "Cell \u001b[1;32mIn[2], line 54\u001b[0m, in \u001b[0;36mhvac_function.<locals>.create_logs\u001b[1;34m(rule, data_rd, logs, templogs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_logs\u001b[39m(rule, data_rd, logs, templogs):\n\u001b[1;32m---> 54\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mgeFramework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_rd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         dff \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mreturn_log()\n",
      "Cell \u001b[1;32mIn[28], line 62\u001b[0m, in \u001b[0;36mgeFramework.__init__\u001b[1;34m(self, rule, batch)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# apply the equation that is given in the rule book (equation_1)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_equation_1(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m, df, equation\u001b[38;5;241m=\u001b[39mrule[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequation_1\u001b[39m\u001b[38;5;124m'\u001b[39m], temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 62\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_equation_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrule\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mequation_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# create logs\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logs_data(df, rule, [alias])\n",
      "Cell \u001b[1;32mIn[28], line 334\u001b[0m, in \u001b[0;36mgeFramework.apply_equation_2\u001b[1;34m(self, df, rule, equation, kpi)\u001b[0m\n\u001b[0;32m    330\u001b[0m                 equation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(df.delta >=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) & (df.value == True)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m#                 df['test'] = np.where((df.delta >= duration) & (df.value == True), True, False)\u001b[39;00m\n\u001b[0;32m    332\u001b[0m                 \u001b[38;5;66;03m#df['test'] = eval(equation)\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m             df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(equation)\n\u001b[0;32m    335\u001b[0m             df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[df\u001b[38;5;241m.\u001b[39mtest \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[0;32m    336\u001b[0m             df\u001b[38;5;241m.\u001b[39mtest \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfail\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:11999\u001b[0m, in \u001b[0;36mNDFrame.rolling\u001b[1;34m(self, window, min_periods, center, win_type, on, axis, closed, step, method)\u001b[0m\n\u001b[0;32m  11985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m win_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m  11986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Window(\n\u001b[0;32m  11987\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11988\u001b[0m         window\u001b[38;5;241m=\u001b[39mwindow,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11996\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m  11997\u001b[0m     )\n\u001b[1;32m> 11999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRolling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  12000\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_periods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12006\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclosed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12010\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:165\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[1;34m(self, obj, window, min_periods, center, win_type, axis, on, closed, step, method, selection)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid on specified as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be a column (of DataFrame), an Index or None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;241m=\u001b[39m selection\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1862\u001b[0m, in \u001b[0;36mRolling._validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow must be an integer 0 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: window must be an integer 0 or greater"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kwargs = {\n",
    "    'project' : 'thermosphr-prod',\n",
    "    'dataset_src' : 'bi',\n",
    "    'dataset_dest' : 'raw',\n",
    "    'table_src' : 'datamart_v2',\n",
    "    'table_dest' : 'ws_live_qa_properties_logs',\n",
    "    'tz_info' : 'Europe/Paris',\n",
    "    'config' : config,\n",
    "    'client_id' : 'rosny2',\n",
    "    'device_id' : 'nYnda9jKG'\n",
    "}\n",
    "hvac_function(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand_num_1</th>\n",
       "      <th>rand_num_2</th>\n",
       "      <th>rand_num_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>884</td>\n",
       "      <td>6630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>602</td>\n",
       "      <td>3398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>462</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>668</td>\n",
       "      <td>5807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>84</td>\n",
       "      <td>5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>83</td>\n",
       "      <td>664</td>\n",
       "      <td>6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>58</td>\n",
       "      <td>426</td>\n",
       "      <td>6602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>22</td>\n",
       "      <td>918</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>5</td>\n",
       "      <td>336</td>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>51</td>\n",
       "      <td>814</td>\n",
       "      <td>5264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rand_num_1  rand_num_2  rand_num_3\n",
       "0                70         884        6630\n",
       "1                91         602        3398\n",
       "2                 7         462        7424\n",
       "3                37         668        5807\n",
       "4                56          84        5574\n",
       "...             ...         ...         ...\n",
       "9999995          83         664        6652\n",
       "9999996          58         426        6602\n",
       "9999997          22         918        1106\n",
       "9999998           5         336        6044\n",
       "9999999          51         814        5264\n",
       "\n",
       "[10000000 rows x 3 columns]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "rand_num_1 = np.random.randint(10e1, size= 10000000)\n",
    "rand_num_2 = np.random.randint(10e2, size = 10000000)\n",
    "rand_num_3 = np.random.randint(10e3, size = 10000000)\n",
    "data_df=pd.DataFrame({'rand_num_1': rand_num_1,\n",
    "                     'rand_num_2': rand_num_2,\n",
    "                     'rand_num_3':rand_num_3})\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.11 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def standard_scalar_map(pandas_element: int, mean_pandas_series: float, std_pandas_series: float)->float:\n",
    "    scaled_pandas_element = (pandas_element - mean_pandas_series) / std_pandas_series\n",
    "    return scaled_pandas_element\n",
    "\n",
    "mean_pandas_series = np.mean(data_df['rand_num_2'])\n",
    "std_pandas_series = np.std(data_df['rand_num_2'])\n",
    "data_df['scaled_rand_num_2'] = \\\n",
    "                data_df['rand_num_2'].map(lambda x:\n",
    "                                         standard_scalar_map(\n",
    "                                         x,\n",
    "                                         mean_pandas_series = mean_pandas_series,\n",
    "                                         std_pandas_series = std_pandas_series\n",
    "                                         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 253 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\CBT'"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Vectorized Series\n",
    "def standard_scalar_vectorized_series(pandas_series: pd.Series) -> pd.Series:\n",
    "    scaled_series = (pandas_series - np.mean(pandas_series)) / np.std(pandas_series)\n",
    "    return scaled_series\n",
    "data_df['scaled_rand_num_2'] = standard_scalar_vectorized_series(data_df['rand_num_2'])\n",
    "data_df['scaled_rand_num_2']\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "cmd": "Other",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autoawait": "AsyncMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "cls": "KernelMagics",
        "colors": "BasicMagics",
        "conda": "PackagingMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "copy": "Other",
        "ddir": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "echo": "Other",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "PackagingMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "ren": "Other",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %conda  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((4,6)).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "939480ed579cbcc9bd95c0bb2f0a271d068ec362d36f1415ed941c7dadb52340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
