{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import regex as re\n",
    "import warnings\n",
    "import arrow\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "import arrow\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "from string import Template\n",
    "\n",
    "\n",
    "# supress warnings to debugging\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.auth\n",
    "import pandas_gbq\n",
    "import arrow\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "from string import Template\n",
    "from datetime import datetime as dt, timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "\n",
    "def hvac_function(project, dataset_src, dataset_dest, table_src, table_dest, tz_info, config, client_id, device_id):\n",
    "    def unique_logs(project, client_id, device_id, client, dataset_dest, table_dest, templogs):\n",
    "        sql = client.query(f\"\"\"\n",
    "            select client_id,device_id,rule_name, aliases,criticality,test,qa,count(*) as count\n",
    "            from `{project}.{dataset_dest}.{table_dest}` where extract(date from time_local) = CURRENT_DATE()\n",
    "            and test='fail' and criticality = 'high'\n",
    "            and client_id = '{client_id}' and device_id = '{device_id}'\n",
    "            group by client_id,device_id,rule_name,aliases,criticality,test,qa\n",
    "        \"\"\")\n",
    "\n",
    "        previous_logs = sql.to_dataframe()\n",
    "        merged_templogs = pd.merge(templogs, previous_logs,\n",
    "                                   on=['client_id', 'device_id', 'rule_name', 'aliases', 'criticality', 'test', 'qa'],\n",
    "                                   how='left')\n",
    "\n",
    "        merged_templogs['count'] = merged_templogs['count'].fillna(0)\n",
    "        merged_templogs = merged_templogs[merged_templogs['count'] <= 3]\n",
    "        if merged_templogs.shape[0] >= 1:\n",
    "            return merged_templogs[['client_id',\n",
    "                                    'device_id',\n",
    "                                    'rule_name',\n",
    "                                    'aliases',\n",
    "                                    'criticality',\n",
    "                                    'time_local',\n",
    "                                    'test',\n",
    "                                    'date_update',\n",
    "                                    'qa']]\n",
    "        else:\n",
    "            return pd.DataFrame(columns=['client_id',\n",
    "                                         'device_id',\n",
    "                                         'rule_name',\n",
    "                                         'aliases',\n",
    "                                         'criticality',\n",
    "                                         'time_local',\n",
    "                                         'test',\n",
    "                                         'date_update',\n",
    "                                         'qa'])\n",
    "\n",
    "    def create_logs(rule, data_rd, logs, templogs):\n",
    "        obj = geFramework(rule, data_rd)\n",
    "\n",
    "        if obj is not None:\n",
    "            dff = obj.return_log()\n",
    "            logs = pd.concat([logs, dff], axis=0)\n",
    "\n",
    "            dff = dff.reset_index(drop=True)\n",
    "            if rule['mute_alert'] is None:\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "            else:\n",
    "                aliases_list = rule['mute_alert'].split(',')\n",
    "\n",
    "                for j in aliases_list:\n",
    "                    dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "                    logs.loc[logs.aliases.str.contains(j.strip()),\"mute_alert\"] = \"muted\"\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "            if rule['mute_logs'] is None:\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "                clear_logs = logs\n",
    "            elif rule['mute_logs'].lower() == 'all':\n",
    "                dff = dff[~dff.rule_name.str.contains(rule['rule_name'])]\n",
    "\n",
    "                clear_logs = logs[~logs.rule_name.str.contains(rule['rule_name'])]\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "            else:\n",
    "                aliases_list = rule['mute_logs'].split(',')\n",
    "\n",
    "                for j in aliases_list:\n",
    "                    dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "                    clear_logs = logs[~logs.aliases.str.contains(j.strip())]\n",
    "\n",
    "                templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "        return clear_logs, templogs\n",
    "\n",
    "    output = config\n",
    "\n",
    "    rules = pd.DataFrame(output['rules'])\n",
    "    rules.index = rules.index.astype(int)\n",
    "    rules.sort_index(inplace=True)\n",
    "    rules = rules[['rule_name',\n",
    "                   'alias_a',\n",
    "                   'alias_b',\n",
    "                   'alias_c',\n",
    "                   'alias_d',\n",
    "                   'equation_1',\n",
    "                   'equation_2',\n",
    "                   'duration_hours',\n",
    "                   'criticality',\n",
    "                   'start_date',\n",
    "                   'end_date',\n",
    "                   'mute_alert',\n",
    "                   'mute_logs',\n",
    "                   'qa']].copy()\n",
    "\n",
    "    rules = rules.where(pd.notnull(rules), None)\n",
    "    settings = output['settings']['mute_rules']\n",
    "    logs_link = output['logs_link']\n",
    "    duration_hours = rules['duration_hours'].to_list()\n",
    "\n",
    "    logs = pd.DataFrame(columns=[\"client_id\",\n",
    "                                 \"device_id\",\n",
    "                                 \"rule_name\",\n",
    "                                 \"aliases\",\n",
    "                                 \"criticality\",\n",
    "                                 \"time_local\",\n",
    "                                 \"test\",\n",
    "                                 \"date_update\",\n",
    "                                 \"qa\",\n",
    "                                 \"mute_alert\",\n",
    "                                 \"data_start_time\"])\n",
    "    templogs = pd.DataFrame(columns=[\"client_id\",\n",
    "                                     \"device_id\",\n",
    "                                     \"rule_name\",\n",
    "                                     \"aliases\",\n",
    "                                     \"criticality\",\n",
    "                                     \"time_local\",\n",
    "                                     \"test\",\n",
    "                                     \"date_update\",\n",
    "                                     \"qa\",\n",
    "                                     \"mute_alert\",\n",
    "                                     \"data_start_time\"])\n",
    "\n",
    "    # credentials, project_id = google.auth.default()\n",
    "\n",
    "    #client = bigquery.Client(project=project)\n",
    "    end_time = arrow.get('2023-01-31 15:00:00')\n",
    "    start = (end_time + datetime.timedelta(hours=-max(duration_hours) - 5)).date()\n",
    "    end = (end_time + datetime.timedelta(hours=max(duration_hours))).date()\n",
    "\n",
    "#     end_time = arrow.utcnow().replace(second=0, microsecond=0).to(tz_info).shift(minutes=-15)\n",
    "#     end_time = arrow.get('2023-01-31 15:00:00')\n",
    "    start_time = end_time + datetime.timedelta(hours=-max(duration_hours))\n",
    "\n",
    "    str_end_time = str(end_time.datetime).split('+')[0]\n",
    "    str_start_time = str(start_time.datetime).split('+')[0]\n",
    "    print(f'Data between {str_start_time} and {str_end_time}')\n",
    "\n",
    "    if settings is False:\n",
    "        data_rd = pd.read_csv('D:\\\\Thermosphr\\\\QA Vectoize task\\\\gera-arcaden-germany.csv')\n",
    "        \n",
    "        date = arrow.utcnow().date()\n",
    "        date = date.replace(year=1900)\n",
    "        if data_rd.shape[0] > 0:\n",
    "\n",
    "            print('ready to run function')\n",
    "\n",
    "            # recursively apply rules over data\n",
    "\n",
    "            for i in range(rules.shape[0]):\n",
    "                if not rules.iloc[i]['mute_logs']:\n",
    "                    start_date = rules.iloc[i]['start_date']\n",
    "                    end_date = rules.iloc[i]['end_date']\n",
    "\n",
    "                    if start_date == None and end_date == None:\n",
    "\n",
    "                        logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "\n",
    "                    elif start_date != None and end_date != None:\n",
    "\n",
    "                        start_date = dt.strptime(start_date, '%m-%d').date()\n",
    "                        end_date = dt.strptime(end_date, '%m-%d').date()\n",
    "\n",
    "                        if start_date < end_date:\n",
    "                            if (start_date <= date and end_date >= date):\n",
    "                                logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "                        elif start_date > end_date:\n",
    "                            if (start_date >= date and end_date >= date):\n",
    "                                logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "                        else:\n",
    "                            print('Logs not created for this rule  :  ', rules.iloc[i]['rule_name'])\n",
    "                else:\n",
    "                    print(f\"{rules.iloc[i]['rule_name']} is muted\")\n",
    "\n",
    "            current_time = datetime.datetime.utcnow().replace(\n",
    "                second=0, microsecond=0)\n",
    "            # Finding out duplicate aliases if any\n",
    "#             df1 = dataframe[[\"alias\", \"propertyId\"]].groupby([\"alias\", \"propertyId\"]).count().reset_index()\n",
    "#             df2 = df1[['alias']].groupby(['alias'])['alias'].count().reset_index(name='count')\n",
    "#             df3 = df2.loc[df2['count'] > 1].sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "            templogs['client_id'] = client_id\n",
    "            templogs['device_id'] = device_id\n",
    "            templogs = templogs[~templogs.duplicated()].reset_index(drop=True)\n",
    "            templogs = templogs[templogs.time_local == str_end_time]\n",
    "         \n",
    "\n",
    "            logs['client_id'] = client_id\n",
    "            logs['device_id'] = device_id\n",
    "            logs['data_start_time'] = pd.to_datetime(str_start_time, format='%Y-%m-%d %H:%M:%S')\n",
    "            logs['date_update'] = pd.to_datetime(str(current_time), format='%Y-%m-%d %H:%M:%S')\n",
    "            #logs = logs[logs.time_local == str_end_time]\n",
    "            logs = logs[~logs.duplicated()].reset_index(drop=True)\n",
    "            logs[['time_local', 'date_update', 'data_start_time']] = logs[['time_local', 'date_update', 'data_start_time']].astype(str)\n",
    "            #table = client.get_table(f\"{project}.{dataset_dest}.{table_dest}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('No Data for given time period:')\n",
    "            print(start_time)\n",
    "            print(end_time)\n",
    "    else:\n",
    "        print(\"All rules are muted\")\n",
    "        \n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class geFramework():\n",
    "    def __init__(self, rule, batch):\n",
    "        #print(rule['rule_name'])\n",
    "        #print(rule['equation_1'])\n",
    "        \n",
    "        # get subset of data, min-time and max-time for the given start and end time\n",
    "        #batch = self.__batch_subset(batch, rule['duration_hours'])\n",
    "        if 'missing_values' not in rule['rule_name']:\n",
    "            batch.fillna(method='ffill', inplace=True)\n",
    "        # indices of the rule dataframe\n",
    "        alias_cols = rule.index\n",
    "        subs = 'alias'\n",
    "        # get columns of rule book that alias in their names\n",
    "        self.__res = [\n",
    "            alias_col for alias_col in alias_cols if subs in alias_col]\n",
    "        self.__rule = rule\n",
    "\n",
    "        # create empty dataframe for the logs\n",
    "        self.__df_logs = pd.DataFrame(columns=[\n",
    "            'rule_name', 'aliases', 'criticality', 'time_local', 'test', 'date_update','qa'])\n",
    "        aliases = []\n",
    "        # get a list of aliases for the given targets in the rule book and append that lists\n",
    "        # in the aliases list\n",
    "\n",
    "        # in case alias_a is not null\n",
    "        if rule[self.__res[0]] is not None:\n",
    "\n",
    "            for i in self.__res:\n",
    "                if rule[i] is not None and '-' not in rule[i]:\n",
    "                    # get all aliases for given keywords in config columns\n",
    "                    aliases.append(self.__alias_substrings(\n",
    "                        batch, [rule[i]], col_name='alias'))\n",
    "\n",
    "                # In case minus sign is found to exclude that given alias\n",
    "                elif rule[i] is not None and '-' in rule[i]:\n",
    "                    keyword = rule[i].replace('-', '').strip()\n",
    "                    # get all aliases for given keyword in config columns\n",
    "                    excluded_aliases = self.__alias_substrings(\n",
    "                        batch, [keyword], col_name='alias')\n",
    "                    # get all columns(aliases) of the dataframe\n",
    "                    unique_aliases = batch.columns.to_list()\n",
    "                    # drop the given keyword's aliases and time_local from all aliases in dataframe\n",
    "                    required_aliases = list(\n",
    "                        set(unique_aliases) - set(excluded_aliases) - set(['time_local']))\n",
    "                    # append required aliases in the aliases list\n",
    "                    aliases.append(required_aliases)\n",
    "\n",
    "            # in case there is only one group of aliases in aliases list\n",
    "            if len(aliases) == 1:\n",
    "                # iterate over all group of  alaises\n",
    "                for alias in aliases[0]:\n",
    "\n",
    "                    # get a dataframe for each alias\n",
    "                    df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                              resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                    # in case dataframe is not empty\n",
    "                    if df.shape[0] > 0:\n",
    "                        # apply the equation that is given in the rule book (equation_1)\n",
    "                        df = self.apply_equation_1(\n",
    "                            self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                        df = self.apply_equation_2(\n",
    "                            self, df, rule, equation=rule['equation_2'])\n",
    "                        # create logs\n",
    "                        df1 = self.__logs_data(df, rule, [alias])\n",
    "                        self.__df_logs = pd.concat(\n",
    "                            [self.__df_logs, df1], axis=0)\n",
    "\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {alias}')\n",
    "            # if in case there are multiple aliases groups in aliases list\n",
    "            else:\n",
    "\n",
    "                # create a list of different pairs of aliases\n",
    "                alias_pair = self.__alias_pairs(self, batch, rule, aliases)\n",
    "\n",
    "                # iterate for all pairs\n",
    "                for pair in alias_pair:\n",
    "\n",
    "                    # in case length of a pair is not equal to 0\n",
    "                    if len(pair) != 0:\n",
    "                        # get a dataframe for each pair\n",
    "                        df = self.alias_pairs_dataframe(self, data_rd=batch, aliases=pair, kpi=rule['rule_name'],\n",
    "                                                        resample_grain=None, daily_resample=None, agg=None,\n",
    "                                                        non_negative=None)\n",
    "                        # in case dataframe is not empty\n",
    "                        if df.shape[0] > 0:\n",
    "                            # apply the equation that is given in the rule book (equation_1)\n",
    "                            df = self.apply_equation_1(\n",
    "                                self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                            df = self.apply_equation_2(\n",
    "                                self, df, rule, equation=rule['equation_2'])\n",
    "                            #                     # create logs\n",
    "                            df1 = self.__logs_data(df, rule, pair)\n",
    "                            self.__df_logs = pd.concat(\n",
    "                                [self.__df_logs, df1], axis=0)\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {pair}')\n",
    "        # if in case alias_a is null\n",
    "        else:\n",
    "            # get all columns (aliases) of the dataframe except time_local or time\n",
    "            aliases.append(self.__alias_substrings(\n",
    "                batch, None, col_name='alias'))\n",
    "            # iterate for all aliases\n",
    "            for alias in aliases[0]:\n",
    "                # get a dataframe for each pair\n",
    "                df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                          resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                # in case dataframe is not empty\n",
    "                if df.shape[0] > 0:\n",
    "                    # apply the equation that is given in the rule book (equation_1)\n",
    "                    df = self.apply_equation_1(\n",
    "                        self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "\n",
    "                    df = self.apply_equation_2(\n",
    "                        self, df, rule, equation=rule['equation_2'])\n",
    "                    #                     # create logs\n",
    "                    df1 = self.__logs_data(df, rule, [alias])\n",
    "                    self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "                # in case dataframe is empty\n",
    "                else:\n",
    "                    print(f'Data is not available for {alias}')\n",
    "\n",
    "    @staticmethod\n",
    "    def __logs_data(df=None, rule=None, alias_list=None, result=None):\n",
    "        \"\"\" Gets resultant dataframe and rules for each kpi and creates logs\n",
    "            in the form of a dataframe.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        Resultant dataframe for each kpi\n",
    "        rule:dataframe\n",
    "        Its the config file\n",
    "        tgt_alias:string\n",
    "        Name of the target alias\n",
    "        ref_alias: string\n",
    "        Name of the reference alias\n",
    "        \"\"\"\n",
    "        # create an empty dataframe\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "        # concate all aliases as one string\n",
    "        if len(alias_list) == 1:\n",
    "            logs_alias = alias_list[0]\n",
    "        else:\n",
    "            logs_alias = \",\".join(alias_list)\n",
    "\n",
    "        # get current utc time (function running time) for date_update\n",
    "        # current_time = datetime.datetime.utcnow().replace(\n",
    "        #     second=0, microsecond=0)\n",
    "\n",
    "        df1 = df[['test', 'time_local']].copy()\n",
    "        df1['aliases'] = logs_alias\n",
    "        df1['rule_name'] = rule['rule_name']\n",
    "        df1['criticality'] = rule['criticality']\n",
    "        df1['qa'] = rule['qa']\n",
    "\n",
    "        return df1\n",
    "\n",
    "    @staticmethod\n",
    "    def count(df, max_delta):\n",
    "\n",
    "        try:\n",
    "            count_true = df['value'].value_counts()[True]\n",
    "        except:\n",
    "            count_true = 0\n",
    "\n",
    "        return count_true\n",
    "\n",
    "    @staticmethod\n",
    "    def streak(df):\n",
    "        \"\"\"This function calculates delta for only true values, in case\n",
    "        this calculated delta is greater or equal to the max_delta then\n",
    "        test will fail otherwise pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dataframe\n",
    "        Dataframe after applying equation\n",
    "        rule: dataframe\n",
    "        Each row of the config file as a dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        df: dataframe\n",
    "        delta: float\n",
    "        \"\"\"\n",
    "\n",
    "        # set True value to 1 and False to 0\n",
    "        df.loc[df[\"value\"] == True, \"value\"] = 1\n",
    "        df.loc[df[\"value\"] == False, \"value\"] = 0\n",
    "        # Check that original values are not equal to shifted values\n",
    "        df['expected_streak'] = df.value.ne(df.value.shift())\n",
    "        # get timelocal for the start of streak\n",
    "        df['streak_time'] = df.loc[df.expected_streak == True]['time_local']\n",
    "        # convert streak start time to datetime\n",
    "        df['streak_time'] = pd.to_datetime(df['streak_time'])\n",
    "        # converte time_local to datetime\n",
    "        df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "        # fill null values from previous values by using forward filling\n",
    "        df = df.fillna(method='ffill')\n",
    "        # calculate delta in minutes\n",
    "        df['delta'] = (df['time_local'] - df['streak_time']\n",
    "                       ).dt.total_seconds() / 60\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_equation_1(self, df=None, equation=None, temp=None):\n",
    "        \"\"\" This function basically takes a dataframe and an equation\n",
    "        and then tries to apply this equation and results in the form of boolean.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: Dataframe\n",
    "        A Dataframe for an aliases or a pair of aliases\n",
    "        equation: String\n",
    "        This string has a set of instruction\n",
    "        Returns\n",
    "        -------\n",
    "        df: Dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        def func(x):\n",
    "            y = eval(equation)\n",
    "            return y\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "            # in case equation is given and not missing_values or constant values\n",
    "            if equation is not None:\n",
    "                # iterate for given given alias columns in the config file\n",
    "                for col in self.__res:\n",
    "                    #add dataframe with alias column\n",
    "                    if col in equation:\n",
    "                        if col in equation:\n",
    "                            #col_name = f\"x['{col}']\"\n",
    "                            col_name = f\"df['{col}'].values\"\n",
    "                            #pint(col_name)\n",
    "                            equation = equation.replace(col,col_name)\n",
    "                            #print(equation)\n",
    "\n",
    "                \n",
    "                # time and date base conditions\n",
    "                if 'day' in equation:\n",
    "                    df['day'] = df.time_local.dt.dayofweek\n",
    "                    equation = equation.replace('day', 'df[\"day\"].values')\n",
    "                if 'date' in equation:\n",
    "                    df['date'] = df.time_local.dt.date\n",
    "                    equation = equation.replace('date', 'df[\"date\"].astype(str).values')\n",
    "                if 'time' in equation:\n",
    "                    df['time'] = df.time_local.apply(lambda x: x.strftime('%H:%M'))\n",
    "                    equation = equation.replace('time', 'df[\"time\"].astype(str).values')\n",
    "                if 'hour' in equation:\n",
    "                    df['hour'] = df.time_local.dt.hour\n",
    "                    equation = equation.replace('hour', 'df[\"hour\"].values')\n",
    "                if 'minute' in equation:\n",
    "                    df['minute'] = df.time_local.dt.minute\n",
    "                    equation = equation.replace('minute', 'df[\"minute\"].values')\n",
    "                if 'month' in equation:\n",
    "                    df['month'] = df.time_local.dt.month\n",
    "                    equation = equation.replace('month', 'df[\"month\"].values')\n",
    "                    \n",
    "\n",
    "                # apply equation\n",
    "                #df['value'] = df.apply(func, axis=1)\n",
    "                df['value'] = eval(equation)\n",
    "                #df['value'] = np.where(eval(equation), 1, 0)\n",
    "                #df['value'] =np.where(df.eval(func), True, False)\n",
    "\n",
    "            # in case equation is None\n",
    "            else:\n",
    "                df['value'] = df['alias_a']\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_equation_2(self, df=None, rule=None, equation=None, kpi=None):\n",
    "        if df.shape[0] > 0:\n",
    "            \n",
    "            if equation is not None:\n",
    "                if 'streak' in equation:\n",
    "                    df = self.streak(df)\n",
    "                    duration = float(re.findall(\"[0-9.]+\", equation)[0])\n",
    "                    frac, whole = math.modf(duration)\n",
    "                    duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                    equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "                    #df['test'] = eval(equation)\n",
    "                    df['test'] = np.where((df.delta >= duration) & (df.value == True), True, False)\n",
    "                    \n",
    "\n",
    "                else:\n",
    "                    aggregation = re.findall('\\w+', equation)[0]\n",
    "                    operator = re.findall(\"[^a-zA-Z0-9_.,\\s\\(\\)\\[\\]]+\", equation)[0]\n",
    "                    window = rule['duration_hours']\n",
    "                    duration = str(window) + 'h'\n",
    "                    threshold = float(re.findall(\"[0-9]+\", equation)[0])\n",
    "                    df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "                    df.set_index('time_local', inplace=True)\n",
    "                    equation= f\"df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\"\n",
    "                    df['test'] = eval(f\"df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\")\n",
    "                    #df['test'] = np.where(df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') + {operator} + {threshold}, True, False)\n",
    "                    df.reset_index(inplace=True)\n",
    "            \n",
    "    \n",
    "            else:\n",
    "                df = self.streak(df)\n",
    "                duration = float(rule['duration_hours'])\n",
    "                frac, whole = math.modf(duration)\n",
    "                duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "                df['test'] = np.where((df.delta >= duration) & (df.value == True), True, False)\n",
    "                #df['test'] = eval(equation)\n",
    "\n",
    "            #             display(df)\n",
    "            df = df.loc[df.test == True]\n",
    "            df.test = 'fail'\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_substrings(df, in_list, col_name='alias'):\n",
    "        \"\"\" This function returns a list of aliases for a given reference in the\n",
    "            the config file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        It's the source dataframe\n",
    "        in_list: list\n",
    "        It's the list of given aliases in the config file\n",
    "        col_name: string\n",
    "        It's the column name in the source dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_aliases: list\n",
    "        This list contains the aliases that are filtered out\n",
    "        \"\"\"\n",
    "\n",
    "        groups = in_list\n",
    "\n",
    "        if in_list is not None:\n",
    "            filtered_aliases = []\n",
    "            substrings = []\n",
    "\n",
    "            # Creating list of substrings\n",
    "            for group_id in groups:\n",
    "                # print(group_id)\n",
    "                substrings.append(group_id.split('_'))\n",
    "\n",
    "            unique_aliases = df.columns\n",
    "            # print(unique_aliases)\n",
    "            # Removing None from the list of unique aliases\n",
    "            unique_aliases = list(filter(None, unique_aliases))\n",
    "            # Filter the aliases that are to be used for the KPI\n",
    "            for alias in unique_aliases:\n",
    "                for sub in substrings:\n",
    "                    if all(a.strip() in alias for a in sub):\n",
    "                        # print(alias)\n",
    "                        filtered_aliases.append(alias)\n",
    "        else:\n",
    "\n",
    "            remove_cols = ['time_local']\n",
    "            filtered_aliases = list(set(df.columns.to_list()) - set(remove_cols))\n",
    "\n",
    "        return filtered_aliases\n",
    "\n",
    "    @staticmethod\n",
    "    def __batch_subset(batch, duration):\n",
    "        \"\"\"\n",
    "        Gets a dataframe and a specific duration, where it takes a subset\n",
    "        of dataframe that falls in the specific hours of duration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: dataframe\n",
    "        It's the main dataframe whose subset is to be taken based on time column.\n",
    "        duration:\n",
    "        A time period for which a subset of the dataframe\n",
    "        will be taken.\n",
    "        Returns\n",
    "        -------\n",
    "        batch_subset: dataframe\n",
    "        A subset of the main dataframe for which timing is in between\n",
    "        the specific duration of hours.\n",
    "        batch_min_time: time stamp\n",
    "        It's the start time of the subset dataframe\n",
    "        batch_max_time: time stamp\n",
    "        It's the end time of the subset dataframe.\n",
    "        \"\"\"\n",
    "        # set min and max time variable as global\n",
    "        global batch_min_time\n",
    "        global batch_max_time\n",
    "\n",
    "        # get maximum timestamp of the batch\n",
    "        batch_max_time = str(arrow.get(batch['time_local'].max()).datetime).split('+')[0]\n",
    "        # convert string formated timestamp to datetime object\n",
    "        batch_max_time = datetime.datetime.strptime(batch_max_time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # get minimum timestamp by considering duration hours\n",
    "        batch_min_time = batch_max_time - datetime.timedelta(hours=int(duration),\n",
    "                                                             minutes=int(round(duration % 1, 4) * 60))\n",
    "        # get subset of data for the minimum and maximum timestamps\n",
    "        batch_subset = batch[(batch.time_local >= str(batch_min_time)) & (batch.time_local <= str(batch_max_time))]\n",
    "\n",
    "        return batch_subset\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_pairs_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                              daily_resample=None, agg=None, non_negative=None):\n",
    "        # create an empty dataframe\n",
    "        df_master = pd.DataFrame()\n",
    "        # get indices of aliases's elements\n",
    "        res = [index for index, val in enumerate(aliases)]\n",
    "        # create copy of the aliases\n",
    "        alias_cols = aliases.copy()\n",
    "        # append time_local into the copied alaises\n",
    "        alias_cols.append('time_local')\n",
    "        # get config file aliases columns having same index as res list's elements\n",
    "        cols = [val for index, val in enumerate(self.__res) if index in res]\n",
    "        # append time_local into the cols list\n",
    "        cols.append('time_local')\n",
    "        # get data for the given aliases\n",
    "        df = data_rd[alias_cols].copy()\n",
    "        # assign config file alias column name\n",
    "        df.columns = cols\n",
    "        # in case there is data for the pair\n",
    "        if df.shape[0] > 0:\n",
    "            # assign df to df_master\n",
    "            df_master = df\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                        daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        df = data_rd[['time_local', aliases]].copy()\n",
    "        df.rename(columns={aliases: 'alias_a'}, inplace=True)\n",
    "        if df.shape[0] > 0:\n",
    "            df_master = df\n",
    "\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_pairs(self, data_rd, rule, aliases):\n",
    "        self.__index = [index for index, val in enumerate(aliases)]\n",
    "        col_names = self.__res\n",
    "        col_names = [val for index, val in enumerate(col_names) if index in self.__index]\n",
    "        null_list_index = [index for index, val in enumerate(aliases) if len(val) == 0]\n",
    "        non_null_dfs_list = []\n",
    "        null_dfs_list = []\n",
    "\n",
    "        for i in range(len(aliases)):\n",
    "\n",
    "            non_null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            if len(aliases[i]) > 0:\n",
    "                for alias in aliases[i]:\n",
    "                    keys = alias.split(\"_\")\n",
    "                    for ele in rule[col_names[i]].strip().split('_'):\n",
    "                        keys.remove(ele)\n",
    "\n",
    "                    if len(keys) == 0:\n",
    "\n",
    "                        df = pd.DataFrame([[alias, None]], columns=['alias', 'keys'])\n",
    "\n",
    "                        null_alias_df = pd.concat([null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        key = '_'.join(map(str, keys))\n",
    "                        df = pd.DataFrame([[alias, key]], columns=['alias', 'keys'])\n",
    "                        non_null_alias_df = pd.concat([non_null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "            if non_null_alias_df.shape[0] > 0:\n",
    "                non_null_dfs_list.append(non_null_alias_df)\n",
    "            if null_alias_df.shape[0] > 0:\n",
    "                null_dfs_list.append(null_alias_df)\n",
    "\n",
    "        if len(null_dfs_list) == len(col_names):\n",
    "            nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        else:\n",
    "\n",
    "            if len(non_null_dfs_list) > 1:\n",
    "                non_nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, on=[\"keys\"], how=\"inner\"),\n",
    "                                            non_null_dfs_list)\n",
    "            elif len(non_null_dfs_list) == 1:\n",
    "                non_nulls_df_merge = non_null_dfs_list[0]\n",
    "            else:\n",
    "                non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "            if len(null_dfs_list) > 1:\n",
    "                nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            elif len(null_dfs_list) == 1:\n",
    "                nulls_df_merge = null_dfs_list[0]\n",
    "            else:\n",
    "                nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "        if nulls_df_merge.shape[0] > 0 and non_nulls_df_merge.shape[0] > 0:\n",
    "            alias_pairs_df = pd.merge(nulls_df_merge, non_nulls_df_merge, how='cross')\n",
    "        elif nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = non_nulls_df_merge\n",
    "        elif non_nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = nulls_df_merge\n",
    "\n",
    "        subs = 'keys'\n",
    "        keys_cols = [col for col in alias_pairs_df if subs in col]\n",
    "        alias_pairs_df = alias_pairs_df.drop(keys_cols, axis=1)\n",
    "\n",
    "        if alias_pairs_df.shape[1] == len(col_names):\n",
    "            alias_pairs = alias_pairs_df.values.tolist()\n",
    "        else:\n",
    "            alias_pairs = []\n",
    "\n",
    "        return alias_pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def message_builder(logs, data_end_time, data, client_id, logs_link):\n",
    "        \"\"\"\n",
    "        Gets a dataframe, that has information about the tests that are either passed of failed\n",
    "        Parameters\n",
    "        ----------\n",
    "        logs_df: Dataframe\n",
    "        This is the dataframe, which contains different tests that are performed.\n",
    "        Returns\n",
    "        -------\n",
    "        message: string\n",
    "        It contains\n",
    "        \"\"\"\n",
    "        message = f\"*{client_id}*  {data_end_time}\\n\"\n",
    "        if (len(logs.loc[logs.test == 'fail']) > 0):\n",
    "            df_false = logs.loc[logs['test'] == 'fail']\n",
    "            rule_names = df_false.loc[df_false.criticality == 'high']['rule_name'].unique()\n",
    "            for i in range(len(rule_names)):\n",
    "                message = message + f\"\\n *{rule_names[i]}*\"\n",
    "\n",
    "            if data.shape[0] > 0:\n",
    "                message = message + f\"\\n\\nDuplicate aliases\"\n",
    "                message = message + f\"\\n {data}\"\n",
    "\n",
    "            message = message + f\"\"\"\\n\n",
    "    <{logs_link}>\n",
    "            \"\"\"\n",
    "        return message\n",
    "\n",
    "    def slack_alert(message):\n",
    "        \"\"\"\n",
    "        Gets a string of information about the tests are failed having only high criticality, and\n",
    "        send this information to a slack channel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        message: string\n",
    "        Information about the tests that have high criticality.\n",
    "        Returns\n",
    "        -------\n",
    "        This function returns nothing\n",
    "        \"\"\"\n",
    "\n",
    "        webhook_url = SLACK_WEBHOOK\n",
    "        slack_data = {'text': message}\n",
    "\n",
    "        response = requests.post(\n",
    "            webhook_url, data=json.dumps(slack_data),\n",
    "            headers={'Content-Type': 'application/json'}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(\n",
    "                'Request to slack returned an error %s, the response is:\\n%s'\n",
    "                % (response.status_code, response.text)\n",
    "            )\n",
    "\n",
    "    def return_log(self):\n",
    "        return self.__df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\Thermosphr\\\\QA configs\\\\gera_qa_config_update.json') as file:\n",
    "    config = json.loads(file.read())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data between 2023-01-28 15:00:00 and 2023-01-31 15:00:00\n",
      "ready to run function\n",
      "water_almost_boiling_2h is muted\n",
      "water_almost_boiling_4h is muted\n",
      "water_almost_freezing_2h is muted\n",
      "water_almost_freezing_4h is muted\n",
      "negative_values_3h is muted\n",
      "negative_values_6h is muted\n",
      "CPU times: total: 3min 55s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kwargs = {\n",
    "    'project' : 'thermosphr-prod',\n",
    "    'dataset_src' : 'bi',\n",
    "    'dataset_dest' : 'raw',\n",
    "    'table_src' : 'datamart_v2',\n",
    "    'table_dest' : 'ws_live_qa_properties_logs',\n",
    "    'tz_info' : 'Europe/Paris',\n",
    "    'config' : config,\n",
    "    'client_id' : 'rosny2',\n",
    "    'device_id' : 'nYnda9jKG'\n",
    "}\n",
    "logs = hvac_function(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "missing_values_3h                        61567\n",
       "missing_values_6h                        60079\n",
       "constant_ahu_air_flow_sp                 53450\n",
       "constant_ahu_air_discharge_temp_sp       48180\n",
       "ahu_cmd_enabled_24h                      14309\n",
       "ahu_cmd_enabled_72h                      13157\n",
       "ahu_sensor_enabled_24h                    7772\n",
       "ahu_sensor_enabled_72h                    7196\n",
       "constant_hot_water_temp_sp                6587\n",
       "hot_water_pump_enabled_72h                4041\n",
       "air_flow_>75perc_4h                       1798\n",
       "fan_speed_>90perc_4h                      1798\n",
       "customer_center_hot_water_temp_low_1h      963\n",
       "hx_1_hot_water_temp_low_1h                 881\n",
       "hx_2_hot_water_temp_low_1h                 881\n",
       "zone_office_hot_water_temp_low_1h          658\n",
       "ahu_air_return_temp_<16C_2h                414\n",
       "air_flow_>75perc_8h                        372\n",
       "fan_speed_>90perc_8h                       372\n",
       "hot_wt_valve_>90perc_2h                    353\n",
       "zone_air_temp_<15C_2h                      294\n",
       "zone_air_temp_<15C_4h                      196\n",
       "hot_wt_valve_>90perc_8h                     82\n",
       "constant_values_24h                         67\n",
       "hx_pump_pressure_<0.5bar_1h                 19\n",
       "Name: rule_name, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs['rule_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>device_id</th>\n",
       "      <th>rule_name</th>\n",
       "      <th>aliases</th>\n",
       "      <th>criticality</th>\n",
       "      <th>time_local</th>\n",
       "      <th>test</th>\n",
       "      <th>date_update</th>\n",
       "      <th>qa</th>\n",
       "      <th>mute_alert</th>\n",
       "      <th>data_start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159957</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>ahu_30_hot_water_valve_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 03:45:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159958</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>ahu_30_hot_water_valve_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159959</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>ahu_30_hot_water_valve_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:15:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159960</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>ahu_30_hot_water_valve_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:30:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159961</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_3h</td>\n",
       "      <td>ahu_30_hot_water_valve_sp</td>\n",
       "      <td>med</td>\n",
       "      <td>2023-01-01 04:45:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281598</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_102_air_discharge_temp_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 07:30:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281599</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_102_air_discharge_temp_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 07:45:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281600</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_102_air_discharge_temp_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 08:00:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281601</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_102_air_discharge_temp_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 08:15:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281602</th>\n",
       "      <td>rosny2</td>\n",
       "      <td>nYnda9jKG</td>\n",
       "      <td>missing_values_6h</td>\n",
       "      <td>ahu_102_air_discharge_temp_sensor</td>\n",
       "      <td>high</td>\n",
       "      <td>2023-01-04 08:30:00</td>\n",
       "      <td>fail</td>\n",
       "      <td>2023-02-09 13:01:00</td>\n",
       "      <td>data_qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-28 15:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121646 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       client_id  device_id          rule_name  \\\n",
       "159957    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159958    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159959    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159960    rosny2  nYnda9jKG  missing_values_3h   \n",
       "159961    rosny2  nYnda9jKG  missing_values_3h   \n",
       "...          ...        ...                ...   \n",
       "281598    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281599    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281600    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281601    rosny2  nYnda9jKG  missing_values_6h   \n",
       "281602    rosny2  nYnda9jKG  missing_values_6h   \n",
       "\n",
       "                                  aliases criticality           time_local  \\\n",
       "159957          ahu_30_hot_water_valve_sp         med  2023-01-01 03:45:00   \n",
       "159958          ahu_30_hot_water_valve_sp         med  2023-01-01 04:00:00   \n",
       "159959          ahu_30_hot_water_valve_sp         med  2023-01-01 04:15:00   \n",
       "159960          ahu_30_hot_water_valve_sp         med  2023-01-01 04:30:00   \n",
       "159961          ahu_30_hot_water_valve_sp         med  2023-01-01 04:45:00   \n",
       "...                                   ...         ...                  ...   \n",
       "281598  ahu_102_air_discharge_temp_sensor        high  2023-01-04 07:30:00   \n",
       "281599  ahu_102_air_discharge_temp_sensor        high  2023-01-04 07:45:00   \n",
       "281600  ahu_102_air_discharge_temp_sensor        high  2023-01-04 08:00:00   \n",
       "281601  ahu_102_air_discharge_temp_sensor        high  2023-01-04 08:15:00   \n",
       "281602  ahu_102_air_discharge_temp_sensor        high  2023-01-04 08:30:00   \n",
       "\n",
       "        test          date_update       qa mute_alert      data_start_time  \n",
       "159957  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "159958  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "159959  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "159960  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "159961  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "...      ...                  ...      ...        ...                  ...  \n",
       "281598  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "281599  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "281600  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "281601  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "281602  fail  2023-02-09 13:01:00  data_qa        NaN  2023-01-28 15:00:00  \n",
       "\n",
       "[121646 rows x 11 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs[logs['rule_name'].str.contains('missing_values')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand_num_1</th>\n",
       "      <th>rand_num_2</th>\n",
       "      <th>rand_num_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>884</td>\n",
       "      <td>6630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>602</td>\n",
       "      <td>3398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>462</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>668</td>\n",
       "      <td>5807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>84</td>\n",
       "      <td>5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>83</td>\n",
       "      <td>664</td>\n",
       "      <td>6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>58</td>\n",
       "      <td>426</td>\n",
       "      <td>6602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>22</td>\n",
       "      <td>918</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999998</th>\n",
       "      <td>5</td>\n",
       "      <td>336</td>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999999</th>\n",
       "      <td>51</td>\n",
       "      <td>814</td>\n",
       "      <td>5264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rand_num_1  rand_num_2  rand_num_3\n",
       "0                70         884        6630\n",
       "1                91         602        3398\n",
       "2                 7         462        7424\n",
       "3                37         668        5807\n",
       "4                56          84        5574\n",
       "...             ...         ...         ...\n",
       "9999995          83         664        6652\n",
       "9999996          58         426        6602\n",
       "9999997          22         918        1106\n",
       "9999998           5         336        6044\n",
       "9999999          51         814        5264\n",
       "\n",
       "[10000000 rows x 3 columns]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "rand_num_1 = np.random.randint(10e1, size= 10000000)\n",
    "rand_num_2 = np.random.randint(10e2, size = 10000000)\n",
    "rand_num_3 = np.random.randint(10e3, size = 10000000)\n",
    "data_df=pd.DataFrame({'rand_num_1': rand_num_1,\n",
    "                     'rand_num_2': rand_num_2,\n",
    "                     'rand_num_3':rand_num_3})\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.11 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def standard_scalar_map(pandas_element: int, mean_pandas_series: float, std_pandas_series: float)->float:\n",
    "    scaled_pandas_element = (pandas_element - mean_pandas_series) / std_pandas_series\n",
    "    return scaled_pandas_element\n",
    "\n",
    "mean_pandas_series = np.mean(data_df['rand_num_2'])\n",
    "std_pandas_series = np.std(data_df['rand_num_2'])\n",
    "data_df['scaled_rand_num_2'] = \\\n",
    "                data_df['rand_num_2'].map(lambda x:\n",
    "                                         standard_scalar_map(\n",
    "                                         x,\n",
    "                                         mean_pandas_series = mean_pandas_series,\n",
    "                                         std_pandas_series = std_pandas_series\n",
    "                                         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 253 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          1.331686\n",
       "1          0.354661\n",
       "2         -0.130387\n",
       "3          0.583326\n",
       "4         -1.440017\n",
       "             ...   \n",
       "9999995    0.569468\n",
       "9999996   -0.255114\n",
       "9999997    1.449484\n",
       "9999998   -0.566931\n",
       "9999999    1.089162\n",
       "Name: scaled_rand_num_2, Length: 10000000, dtype: float64"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Vectorized Series\n",
    "def standard_scalar_vectorized_series(pandas_series: pd.Series) -> pd.Series:\n",
    "    scaled_series = (pandas_series - np.mean(pandas_series)) / np.std(pandas_series)\n",
    "    return scaled_series\n",
    "data_df['scaled_rand_num_2'] = standard_scalar_vectorized_series(data_df['rand_num_2'])\n",
    "data_df['scaled_rand_num_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "cmd": "Other",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autoawait": "AsyncMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "cls": "KernelMagics",
        "colors": "BasicMagics",
        "conda": "PackagingMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "copy": "Other",
        "ddir": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "echo": "Other",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "matplotlib": "PylabMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "PackagingMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "ren": "Other",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %conda  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((4,6)).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "939480ed579cbcc9bd95c0bb2f0a271d068ec362d36f1415ed941c7dadb52340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
