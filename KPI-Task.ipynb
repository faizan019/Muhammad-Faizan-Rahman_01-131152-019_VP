{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a1f598",
   "metadata": {},
   "source": [
    "###        To Do.\n",
    "- Currently the alias_pairs function is matching the exact keywords ie. for **ahu_enable_sensor** and **ahu_chiller_enable_sensor** we have ```ahu_1_enable_sensor``` and ``ahu_1_chiller_enable_sensor``, respectively. These aliases would match because both contain the same key \"1\". But if the config provided kewyords are changed like **ahu_enable_sensor** and **ahu_chiller** then the available aliases would not match. So your task is to update alias_pair function accordingly.\n",
    "    \n",
    "- Make sure all KPIs do not throw any error.\n",
    "\n",
    "- There are some bugs in the code, so fix that before you start your main task.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f8789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow\n",
    "import pandas as pd\n",
    "import json\n",
    "import google.auth\n",
    "import google.auth\n",
    "import logging\n",
    "from string import Template\n",
    "from meteostat import Daily\n",
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from datetime import time\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d1718a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_local</th>\n",
       "      <th>alias</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-23 01:00:00</td>\n",
       "      <td>sp_zenith</td>\n",
       "      <td>150.928824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-23 01:00:00</td>\n",
       "      <td>lat</td>\n",
       "      <td>50.875698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-23 01:00:00</td>\n",
       "      <td>sp_apparent_zenith</td>\n",
       "      <td>150.928824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-23 01:00:00</td>\n",
       "      <td>dew_point</td>\n",
       "      <td>2.971764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-23 01:00:00</td>\n",
       "      <td>dhi</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344757</th>\n",
       "      <td>2022-09-30 02:00:00</td>\n",
       "      <td>sp_azimuth</td>\n",
       "      <td>19.193106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344758</th>\n",
       "      <td>2022-09-30 02:00:00</td>\n",
       "      <td>precipitation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344759</th>\n",
       "      <td>2022-09-30 02:00:00</td>\n",
       "      <td>cloud_cover</td>\n",
       "      <td>91.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344760</th>\n",
       "      <td>2022-09-30 02:00:00</td>\n",
       "      <td>elevation</td>\n",
       "      <td>194.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344761</th>\n",
       "      <td>2022-09-30 02:00:00</td>\n",
       "      <td>lng</td>\n",
       "      <td>12.077743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1344762 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time_local               alias       value\n",
       "0        2022-12-23 01:00:00           sp_zenith  150.928824\n",
       "1        2022-12-23 01:00:00                 lat   50.875698\n",
       "2        2022-12-23 01:00:00  sp_apparent_zenith  150.928824\n",
       "3        2022-12-23 01:00:00           dew_point    2.971764\n",
       "4        2022-12-23 01:00:00                 dhi    0.000000\n",
       "...                      ...                 ...         ...\n",
       "1344757  2022-09-30 02:00:00          sp_azimuth   19.193106\n",
       "1344758  2022-09-30 02:00:00       precipitation    0.000000\n",
       "1344759  2022-09-30 02:00:00         cloud_cover   91.600000\n",
       "1344760  2022-09-30 02:00:00           elevation  194.000000\n",
       "1344761  2022-09-30 02:00:00                 lng   12.077743\n",
       "\n",
       "[1344762 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main=pd.read_csv('D:\\\\Cloud-Track\\\\assign1\\\\main_data.csv')\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1795792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_comment_</th>\n",
       "      <th>settings</th>\n",
       "      <th>rules</th>\n",
       "      <th>logs_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mute_rules</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>10567.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kpi_name</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': 'run_time_hours', '2': 'run_time_fractio...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alias_a</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': 'enable_cmd', '2': 'enable_cmd', '3': 'a...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alias_b</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': 'ahu_fan_air_enabl...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alias_c</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': None, '4': 'heatEx...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alias_d</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': None, '4': None, '...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alias_e</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': None, '4': None, '...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alias_f</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': None, '4': None, '...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equation_1</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': 'alias_a*24', '2': 'alias_a', '3': '(1 i...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equation_2</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': '0 if pd.isna(valu...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aggregation</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': 'mean', '2': 'mean', '3': 'mean', '4': '...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': None, '4': None, '...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_time</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': None, '4': None, '...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mute_logs</th>\n",
       "      <td>This is KPI settings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'1': None, '2': None, '3': None, '4': None, '...</td>\n",
       "      <td>https://colab.research.google.com/drive/16lMaX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _comment_  settings  \\\n",
       "mute_rules   This is KPI settings       0.0   \n",
       "station_id   This is KPI settings   10567.0   \n",
       "kpi_name     This is KPI settings       NaN   \n",
       "alias_a      This is KPI settings       NaN   \n",
       "alias_b      This is KPI settings       NaN   \n",
       "alias_c      This is KPI settings       NaN   \n",
       "alias_d      This is KPI settings       NaN   \n",
       "alias_e      This is KPI settings       NaN   \n",
       "alias_f      This is KPI settings       NaN   \n",
       "equation_1   This is KPI settings       NaN   \n",
       "equation_2   This is KPI settings       NaN   \n",
       "aggregation  This is KPI settings       NaN   \n",
       "start_time   This is KPI settings       NaN   \n",
       "end_time     This is KPI settings       NaN   \n",
       "mute_logs    This is KPI settings       NaN   \n",
       "\n",
       "                                                         rules  \\\n",
       "mute_rules                                                 NaN   \n",
       "station_id                                                 NaN   \n",
       "kpi_name     {'1': 'run_time_hours', '2': 'run_time_fractio...   \n",
       "alias_a      {'1': 'enable_cmd', '2': 'enable_cmd', '3': 'a...   \n",
       "alias_b      {'1': None, '2': None, '3': 'ahu_fan_air_enabl...   \n",
       "alias_c      {'1': None, '2': None, '3': None, '4': 'heatEx...   \n",
       "alias_d      {'1': None, '2': None, '3': None, '4': None, '...   \n",
       "alias_e      {'1': None, '2': None, '3': None, '4': None, '...   \n",
       "alias_f      {'1': None, '2': None, '3': None, '4': None, '...   \n",
       "equation_1   {'1': 'alias_a*24', '2': 'alias_a', '3': '(1 i...   \n",
       "equation_2   {'1': None, '2': None, '3': '0 if pd.isna(valu...   \n",
       "aggregation  {'1': 'mean', '2': 'mean', '3': 'mean', '4': '...   \n",
       "start_time   {'1': None, '2': None, '3': None, '4': None, '...   \n",
       "end_time     {'1': None, '2': None, '3': None, '4': None, '...   \n",
       "mute_logs    {'1': None, '2': None, '3': None, '4': None, '...   \n",
       "\n",
       "                                                     logs_link  \n",
       "mute_rules   https://colab.research.google.com/drive/16lMaX...  \n",
       "station_id   https://colab.research.google.com/drive/16lMaX...  \n",
       "kpi_name     https://colab.research.google.com/drive/16lMaX...  \n",
       "alias_a      https://colab.research.google.com/drive/16lMaX...  \n",
       "alias_b      https://colab.research.google.com/drive/16lMaX...  \n",
       "alias_c      https://colab.research.google.com/drive/16lMaX...  \n",
       "alias_d      https://colab.research.google.com/drive/16lMaX...  \n",
       "alias_e      https://colab.research.google.com/drive/16lMaX...  \n",
       "alias_f      https://colab.research.google.com/drive/16lMaX...  \n",
       "equation_1   https://colab.research.google.com/drive/16lMaX...  \n",
       "equation_2   https://colab.research.google.com/drive/16lMaX...  \n",
       "aggregation  https://colab.research.google.com/drive/16lMaX...  \n",
       "start_time   https://colab.research.google.com/drive/16lMaX...  \n",
       "end_time     https://colab.research.google.com/drive/16lMaX...  \n",
       "mute_logs    https://colab.research.google.com/drive/16lMaX...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json = pd.read_json('D:\\\\Cloud-Track\\\\assign1\\\\kpi_config.json')\n",
    "df_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "13982a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kpi():\n",
    "    def __init__(self, rule, batch):\n",
    "        # get subset of data, min-time and max-time for the given start and end time\n",
    "        batch, batch_min_time, batch_max_time = self.__batch_subset(batch, rule['start_time'], rule['end_time'])\n",
    "        # indices of the rule dataframe\n",
    "        alias_cols = rule.index\n",
    "        subs = 'alias'\n",
    "        # get columns of rule book that alias in their names\n",
    "        self.__res = [alias_col for alias_col in alias_cols if subs in alias_col]\n",
    "        self.__rule = rule\n",
    "\n",
    "        # default logs columns\n",
    "        log_columns = ['kpi_name', 'time', 'aliases', 'value', 'aggregation']\n",
    "        for col in self.__res:\n",
    "            log_columns.append(col)\n",
    "\n",
    "        # create empty dataframe for the logs\n",
    "        self.__df_logs = pd.DataFrame(columns=['kpi_name', 'time', 'aliases', 'value', 'aggregation'])\n",
    "        aliases = []\n",
    "        # get a list of aliases for the given targets in the rule book and append that lists\n",
    "        # in the aliases list\n",
    "        for i in self.__res:\n",
    "            if rule[i] is not None:\n",
    "                print(\"column name ==> \", i)\n",
    "                aliases.append(self.__alias_substrings(batch, [rule[i]], col_name='alias'))\n",
    "        # in case aliases list length is 1\n",
    "        # create logs for only one target (alias_a)\n",
    "        if len(aliases) == 1:\n",
    "            for alias in aliases[0]:\n",
    "                # get a dataframe for each alias\n",
    "                df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['kpi_name'],\n",
    "                                          resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                # apply the equation that is given in the rule book (equation_1)\n",
    "                df = self.apply_equation(self, df, equation=rule['equation_1'], temp=True)\n",
    "                # Aggregate the data after applying equation_1\n",
    "                df = self.apply_aggregation(self, df, aggregation=rule['aggregation'], kpi=rule['kpi_name'])\n",
    "                # In case equation_2 is given then apply it after aggregation\n",
    "                if rule['equation_2'] is not None:\n",
    "                    df = self.apply_equation(self, df, equation=rule['equation_2'])\n",
    "\n",
    "                df1 = self.__logs_data(df, rule, [alias])\n",
    "                self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "\n",
    "        else:\n",
    "            alias_pair = self.__alias_pairs(self, batch, rule, aliases)\n",
    "\n",
    "            for pair in alias_pair:\n",
    "                if len(pair) != 0:\n",
    "\n",
    "                    df = self.alias_pairs_dataframe(self, data_rd=batch, aliases=pair, kpi=rule['kpi_name'],\n",
    "                                                    resample_grain=None, daily_resample=None, agg=None,\n",
    "                                                    non_negative=None)\n",
    "                    df = self.apply_equation(self, df, equation=rule['equation_1'], temp=True)\n",
    "                    df = self.apply_aggregation(self, df, aggregation=rule['aggregation'], kpi=rule['kpi_name'])\n",
    "                    #print(df)\n",
    "                    # In case equation_2 is given then apply it after aggregation\n",
    "                    if rule['equation_2'] is not None:\n",
    "                        df = self.apply_equation(self, df, equation=rule['equation_2'])\n",
    "                        #print(df)\n",
    "\n",
    "\n",
    "                    df1 = self.__logs_data(df, rule, pair)\n",
    "                    self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def __logs_data(df=None, rule=None, alias_list=None):\n",
    "        \"\"\" Gets resultant dataframe and rules for each kpi and creates logs\n",
    "            in the form of a dataframe.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        Resultant dataframe for each kpi\n",
    "        rule:dataframe\n",
    "        Its the config file\n",
    "        tgt_alias:string\n",
    "        Name of the target alias\n",
    "        ref_alias: string\n",
    "        Name of the reference alias\n",
    "        \"\"\"\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "        if len(alias_list) == 1:\n",
    "            logs_alias = alias_list[0]\n",
    "        else:\n",
    "            logs_alias = \",\".join(alias_list)\n",
    "\n",
    "        new_row = {'kpi_name': rule['kpi_name'],\n",
    "                   'time': df['time'],\n",
    "                   'aliases': logs_alias,\n",
    "                   'value': df['value'],\n",
    "                   'equation_1': rule['equation_1'],\n",
    "                   'equation_2': rule['equation_2'],\n",
    "                   'aggregation': rule['aggregation'],\n",
    "                   }\n",
    "\n",
    "        df1 = pd.DataFrame(new_row)\n",
    "        return df1\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_equation(self, df=None, equation=None, temp=None):\n",
    "\n",
    "        def func(x):\n",
    "            y = eval(equation)\n",
    "            return y\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "\n",
    "            if equation is not None:\n",
    "\n",
    "                for col in self.__res:\n",
    "                    if col in equation:\n",
    "                        col_name = f\"x['{col}']\"\n",
    "                        equation = equation.replace(col, col_name)\n",
    "                if 'value' in equation:\n",
    "                    equation = equation.replace('value',\"x['value']\")\n",
    "\n",
    "                if 'time_local' in equation:\n",
    "                    df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "                    df['time_only'] = df['time_local'].dt.time.astype('str')\n",
    "                    equation = equation.replace('time_local', 'x[\"time_only\"]')\n",
    "                    time_list = re.findall(r'\\d{2}\\:\\d{2}\\:\\d{2}', equation)\n",
    "                    for tt in time_list:\n",
    "                        time_split = tt.split(\":\")\n",
    "                        updated_time = time(int(time_split[0]), int(time_split[1]), int(time_split[2]))\n",
    "                        equation = re.sub(tt, str(updated_time), equation)\n",
    "\n",
    "                df['value'] = df.apply(func, axis=1)\n",
    "\n",
    "            else:\n",
    "                df['value'] = df['alias_a']\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_aggregation(self, df=None, aggregation='mean', kpi=None):\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            if aggregation == 'perc':\n",
    "                aggregation = 'mean'\n",
    "\n",
    "            df.set_index('time', inplace=True)\n",
    "\n",
    "            x = f\"df.resample('D').{aggregation}()\"\n",
    "\n",
    "            df = eval(x)\n",
    "\n",
    "            df.reset_index(inplace=True)\n",
    "\n",
    "            df['kpi'] = kpi\n",
    "\n",
    "        df.loc[df[\"value\"] == True, \"value\"] = 1\n",
    "        df.loc[df[\"value\"] == False, \"value\"] = 0\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_substrings(df, in_list, col_name='alias'):\n",
    "        \"\"\" This function returns a list of aliases for a given reference in the\n",
    "            the config file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        It's the source dataframe\n",
    "        in_list: list\n",
    "        It's the list of given aliases in the config file\n",
    "        col_name: string\n",
    "        It's the column name in the source dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_aliases: list\n",
    "        This list contains the aliases that are filtered out\n",
    "        \"\"\"\n",
    "        groups = in_list\n",
    "\n",
    "        substrings = []\n",
    "        filtered_aliases = []\n",
    "        # Creating list of substrings\n",
    "        for group_id in groups:\n",
    "            print(\"group_id ==> \", group_id)\n",
    "            print(\"group_id data type ==> \",type(group_id))\n",
    "            substrings.append(group_id.split('_'))\n",
    "\n",
    "        unique_aliases = df.columns\n",
    "        # print(unique_aliases)\n",
    "        # Removing None from the list of unique aliases\n",
    "        unique_aliases = list(filter(None, unique_aliases))\n",
    "        # Filter the aliases that are to be used for the KPI\n",
    "        for alias in unique_aliases:\n",
    "            for sub in substrings:\n",
    "                if all(a in alias for a in sub):\n",
    "                    filtered_aliases.append(alias)\n",
    "\n",
    "        return filtered_aliases\n",
    "\n",
    "    @staticmethod\n",
    "    def __batch_subset(batch, start_time, end_time):\n",
    "        \"\"\"Gets a dataframe and a specific duration, where it takes a subset\n",
    "        of dataframe that falls in the start and end times.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: dataframe\n",
    "        The source dataframe\n",
    "        start_time: timestamp\n",
    "        This will be the start or minimum time of the dataframe\n",
    "        end_time: timestamp\n",
    "        This be the end or maximum time of the dataframe\n",
    "        Returns\n",
    "        ------\n",
    "        batch: dataframe\n",
    "        This is resultant dataframe between the start and end times\n",
    "        batch_min_time: timestamp\n",
    "        The minimum time of the dataframe\n",
    "        batch_max_time: timestamp\n",
    "        The maximum time of the dataframe\n",
    "        \"\"\"\n",
    "        # in case start time is given\n",
    "        if start_time is not None:\n",
    "            # start_time = str(start_time)\n",
    "            # split the time(string) in hour,min and secs\n",
    "            hour = start_time.split(':')[0]\n",
    "            minute = start_time.split(':')[1]\n",
    "            second = start_time.split(':')[2]\n",
    "            print(f'start_time {time(int(hour), int(minute), int(second))}')\n",
    "            # filter dataframe having time  greater or equal  to start time\n",
    "            batch = batch.loc[batch['time'].dt.time >= time(int(hour), int(minute), int(second))]\n",
    "        # in case end time is given\n",
    "        if end_time is not None:\n",
    "            # end_time = str(end_time)\n",
    "            # split the time(string) in hour,min and secs\n",
    "            hour = end_time.split(':')[0]\n",
    "            minute = end_time.split(':')[1]\n",
    "            second = end_time.split(':')[2]\n",
    "            print(f\"end_time {time(int(hour), int(minute), int(second))}\")\n",
    "            # filter dataframe having time  smaller than the  end time\n",
    "            batch = batch.loc[batch['time'].dt.time < time(int(hour), int(minute), int(second))]\n",
    "        # get miminium and maximum time of the dataframe\n",
    "        batch_min_time = batch.time.min()\n",
    "        batch_max_time = batch.time.max()\n",
    "        return batch, batch_min_time, batch_max_time\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_pairs_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                              daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        res = [index for index, val in enumerate(aliases)]\n",
    "        alias_cols = aliases.copy()\n",
    "        alias_cols.append('time')\n",
    "        cols = [val for index, val in enumerate(self.__res) if index in res]\n",
    "        cols.append('time')\n",
    "        df = data_rd[alias_cols].copy()\n",
    "        df.columns = cols\n",
    "        if df.shape[0] > 0:\n",
    "            df_master = df\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                        daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        df = data_rd[['time', aliases]].copy()\n",
    "        df.rename(columns={aliases: 'alias_a'}, inplace=True)\n",
    "        df_master = df\n",
    "\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_pairs(self, data_rd, rule, aliases):\n",
    "        #print(data_rd)\n",
    "        self.__index = [index for index, val in enumerate(aliases)]  #creating a new list i.e list of elements indeces of aliases\n",
    "        col_names = self.__res   #assign sef.__res to col_names variable\n",
    "        #print(col_names)\n",
    "        # col_names list will only contain elements from the original col_names list, where the index of the element \n",
    "        #is present in the __index list. enumerate() returns iterator that produces the rows with index\n",
    "        col_names = [val for index, val in enumerate(col_names) if index in self.__index]\n",
    "        #print(col_names)\n",
    "        \n",
    "        #creating a list that is iterating over a list of aliases, enumerate() returns an iterator that produces the row\n",
    "        #with index, it then iterates over the rows produced by the enumerate(aliases), it then assisgn the 1st element\n",
    "        #(index) to the variable index and the 2nd element(value) to the variable val, and then if the legth of the \n",
    "        #element ==0, the index of the current element is appended to the null_list_index list.\n",
    "        null_list_index = [index for index, val in enumerate(aliases) if len(val) == 0]\n",
    "        print(null_list_index)\n",
    "        non_null_dfs_list = []  #empty list for non-null dfs\n",
    "        null_dfs_list = []  #empthy list for null dfs\n",
    "        \n",
    "\n",
    "        #iterate over the elements of the aliases\n",
    "        for i in range(len(aliases)):\n",
    "\n",
    "            non_null_alias_df = pd.DataFrame(columns=['alias', 'keys']) #df with alias and key column\n",
    "            null_alias_df = pd.DataFrame(columns=['alias', 'keys']) #df with alias and key column\n",
    "            #checks if the length of the current element of the aliases list is greater than zero,\n",
    "            if len(aliases[i]) > 0:\n",
    "                #it enters another for loop which iterates over the elements of the current element\n",
    "                for alias in aliases[i]:\n",
    "                    #It then splits the current element of the inner for loop into a list called keys\n",
    "                    keys = alias.split(\"_\")\n",
    "                    #print(keys)\n",
    "                    \n",
    "                    #It is likely that the rule list contains some rules or criteria that are used to process the data, and this \n",
    "                    #loop is used to remove any elements from keys that are specified in the rules. The variable col_names is a \n",
    "                    #subset of __res variable and contains the columns that matched the indexes in the __index variable\n",
    "                    for ele in rule[col_names[i]].strip().split('_'):\n",
    "                        keys.remove(ele)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #If the length of the keys list is zero, it appends the current alias to the null_alias_df DataFrame\n",
    "                    if len(keys) == 0:\n",
    "\n",
    "                        df = pd.DataFrame([[alias, None]], columns=['alias', 'keys'])\n",
    "                        null_alias_df = pd.concat([null_alias_df, df], axis=0, ignore_index=True)\n",
    "                        #print(null_alias_df)\n",
    "\n",
    "\n",
    "                    #it combines the remaining elements of the keys list into a single string and appends the \n",
    "                    #current alias and the combined string to the non_null_alias_df DataFrame\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        key = '_'.join(map(str, keys))\n",
    "                        df = pd.DataFrame([[alias, key]], columns=['alias', 'keys'])\n",
    "                        non_null_alias_df = pd.concat([non_null_alias_df, df], axis=0, ignore_index=True)\n",
    "                        #print(non_null_alias_df)\n",
    "\n",
    "            \n",
    "            #Then it checks the shape of non_null_alias_df and null_alias_df and appends it to the non_null_dfs_list \n",
    "            #and null_dfs_list respectively\n",
    "            if non_null_alias_df.shape[0] > 0:\n",
    "                non_null_dfs_list.append(non_null_alias_df)\n",
    "            if null_alias_df.shape[0] > 0:\n",
    "                null_dfs_list.append(null_alias_df)\n",
    "\n",
    "        #Then it checks the length of the null_dfs_list and non_null_dfs_list\n",
    "        if len(null_dfs_list) == len(col_names):\n",
    "            #nulls_df_merge, contains the Cartesian product of all the DataFrames in the null_dfs_list.\n",
    "            nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            \n",
    "            #It creates a new DataFrame non_nulls_df_merge with columns alias, keys and no rows.\n",
    "            non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        else:\n",
    "\n",
    "            if len(non_null_dfs_list) > 1:\n",
    "                non_nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, on=[\"keys\"], how=\"inner\"),\n",
    "                                            non_null_dfs_list)\n",
    "            elif len(non_null_dfs_list) == 1:\n",
    "                non_nulls_df_merge = non_null_dfs_list[0]\n",
    "            else:\n",
    "                non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "            if len(null_dfs_list) > 1:\n",
    "                nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            elif len(null_dfs_list) == 1:\n",
    "                nulls_df_merge = null_dfs_list[0]\n",
    "            else:\n",
    "                nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "        #checking the no of rows of nulls_df_merge.shape and non_nulls_df_merge.shape\n",
    "        if nulls_df_merge.shape[0] > 0 and non_nulls_df_merge.shape[0] > 0:\n",
    "            alias_pairs_df = pd.merge(nulls_df_merge, non_nulls_df_merge, how='cross')\n",
    "        elif nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = non_nulls_df_merge\n",
    "        elif non_nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = nulls_df_merge\n",
    "            #print(alias_pairs_df)\n",
    "\n",
    "        #removing all the columns with names containing the string 'keys' from the alias_pairs_df DataFrame.\n",
    "        subs = 'keys'\n",
    "        keys_cols = [col for col in alias_pairs_df if subs in col]\n",
    "        alias_pairs_df = alias_pairs_df.drop(keys_cols, axis=1)\n",
    "\n",
    "        if alias_pairs_df.shape[1] == len(col_names):\n",
    "            alias_pairs = alias_pairs_df.values.tolist()\n",
    "            print(alias_pairs)\n",
    "        else:\n",
    "            alias_pairs = []\n",
    "\n",
    "        return alias_pairs\n",
    "\n",
    "    def return_log(self):\n",
    "        return self.__df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b896d06f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3812942811.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[138], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    @staticmethod\n",
    "    '''\n",
    "    def __alias_pairs1(self, data_rd, rule, aliases):\n",
    "        self.__index = [index for index, val in enumerate(aliases)]\n",
    "        col_names = self.__res\n",
    "        col_names = [val for index, val in enumerate(col_names) if index in self.__index]\n",
    "        null_list_index = [index for index, val in enumerate(aliases) if len(val) == 0]\n",
    "        non_null_dfs_list = []\n",
    "        null_dfs_list = []\n",
    "\n",
    "        for i in range(len(aliases)):\n",
    "\n",
    "            non_null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            if len(aliases[i]) > 0:\n",
    "                for alias in aliases[i]:\n",
    "                    keys = alias.split(\"_\")\n",
    "                    for ele in rule[col_names[i]].strip().split('_'):\n",
    "                        keys.remove(ele)\n",
    "\n",
    "                    if len(keys) == 0:\n",
    "\n",
    "                        df = pd.DataFrame([[alias, None]], columns=['alias', 'keys'])\n",
    "\n",
    "                        null_alias_df = pd.concat([null_alias_df, df], axis=0, ignore_index=True)\n",
    "                        print(null_alias_df)\n",
    "\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        key = '_'.join(map(str, keys))\n",
    "                        df = pd.DataFrame([[alias, key]], columns=['alias', 'keys'])\n",
    "                        non_null_alias_df = pd.concat([non_null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "            if non_null_alias_df.shape[0] > 0:\n",
    "                non_null_dfs_list.append(non_null_alias_df)\n",
    "            if null_alias_df.shape[0] > 0:\n",
    "                null_dfs_list.append(null_alias_df)\n",
    "\n",
    "        if len(null_dfs_list) == len(col_names):\n",
    "            nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        else:\n",
    "\n",
    "            if len(non_null_dfs_list) > 1:\n",
    "                non_nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, on=[\"keys\"], how=\"inner\"),\n",
    "                                            non_null_dfs_list)\n",
    "            elif len(non_null_dfs_list) == 1:\n",
    "                non_nulls_df_merge = non_null_dfs_list[0]\n",
    "            else:\n",
    "                non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "            if len(null_dfs_list) > 1:\n",
    "                nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            elif len(null_dfs_list) == 1:\n",
    "                nulls_df_merge = null_dfs_list[0]\n",
    "            else:\n",
    "                nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "        if nulls_df_merge.shape[0] > 0 and non_nulls_df_merge.shape[0] > 0:\n",
    "            alias_pairs_df = pd.merge(nulls_df_merge, non_nulls_df_merge, how='cross')\n",
    "        elif nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = non_nulls_df_merge\n",
    "        elif non_nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = nulls_df_merge\n",
    "\n",
    "        subs = 'keys'\n",
    "        keys_cols = [col for col in alias_pairs_df if subs in col]\n",
    "        alias_pairs_df = alias_pairs_df.drop(keys_cols, axis=1)\n",
    "\n",
    "        if alias_pairs_df.shape[1] == len(col_names):\n",
    "            alias_pairs = alias_pairs_df.values.tolist()\n",
    "        else:\n",
    "            alias_pairs = []\n",
    "\n",
    "        return alias_pairs\n",
    "\n",
    "    def return_log(self):\n",
    "        return self.__df_logs\n",
    "    '''\n",
    "    '''\n",
    "    with open('new_config.json') as file:\n",
    "    config = json.loads(file.read())\n",
    "    file.close()\n",
    "    station_id = config['settings']['station_id']\n",
    "    print('station id ==> ',station_id)\n",
    "    df = pd.DataFrame(config['rules'])\n",
    "    df\n",
    "    \n",
    "    #station_id = config['settings']['station_id']\n",
    "    #print('station id ==> ',station_id)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aa839351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpi_function(kwargs):\n",
    "    \"\"\" This function actually reads data from the source table and perform operations on it and then\n",
    "        sends back the resultant logs to the destination table in Bigquery.\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_time: timestamp\n",
    "        It the time from which data will start\n",
    "        end_time: timestamp\n",
    "        It the end of the data\n",
    "        project: GCP project id\n",
    "        dataset_src: Bigquery source dataset\n",
    "        location: Project location\n",
    "        dataset_dest: Bigquery destination dataset\n",
    "        table_src: Bigquery source table\n",
    "        table_dest: Bigquery destination table\n",
    "        Returns\n",
    "        -------\n",
    "        This function doesn't return anything\n",
    "        -------\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = kwargs['start_time']\n",
    "    end_time = kwargs['end_time']\n",
    "    project = kwargs['project']\n",
    "    client_id = kwargs['client_id']\n",
    "    device_id = kwargs['device_id']\n",
    "    dataset_src = kwargs['dataset_src']\n",
    "    dataset_dest = kwargs['dataset_dest']\n",
    "    table_src = kwargs['table_src']\n",
    "    table_dest = kwargs['table_dest']\n",
    "    location = kwargs['location']\n",
    "    config = kwargs['config']\n",
    "\n",
    "    def station_data(station_id, start, end):\n",
    "        data = Daily(station_id, start, end)\n",
    "        data = data.fetch()\n",
    "        data.reset_index(inplace=True)\n",
    "        data = data[['time', 'tavg', 'tmin', 'tmax']]\n",
    "        #     print(data)\n",
    "        return data\n",
    "\n",
    "    current_time = arrow.utcnow().floor('day')\n",
    "    # if end_time <= current_time:\n",
    "    output = config\n",
    "\n",
    "    rules = pd.DataFrame(output['rules'])\n",
    "    rules.index = rules.index.astype(int)\n",
    "    rules.sort_index(inplace=True)\n",
    "    rules = rules[['kpi_name',\n",
    "                   'alias_a',\n",
    "                   'alias_b',\n",
    "                   'alias_c',\n",
    "                   'alias_d',\n",
    "                   'alias_e',\n",
    "                   'alias_f',\n",
    "                   'equation_1',\n",
    "                   'equation_2',\n",
    "                   'aggregation',\n",
    "                   'start_time',\n",
    "                   'end_time',\n",
    "                   'mute_logs']].copy()\n",
    "    rules = rules.where(pd.notnull(rules), None)\n",
    "    settings = output['settings']['mute_rules']\n",
    "    try:\n",
    "        station_id = output['settings']['station_id']\n",
    "    except:\n",
    "        print(f'Station ID is not present for {client_id} ')\n",
    "    df = rules\n",
    "\n",
    "    # create an empty dataframe\n",
    "    logs = pd.DataFrame()\n",
    "    # get access to bigqyery\n",
    "#     credentials, project_id = google.auth.default()\n",
    "    #client = bigquery.Client()\n",
    "\n",
    "    if settings is False:\n",
    "\n",
    "        #dataframe = pd.read_csv('main_data.csv')\n",
    "        dataframe=pd.read_csv('D:\\\\Cloud-Track\\\\assign1\\\\main_data.csv')\n",
    "        #print(dataframe)\n",
    "\n",
    "        data_rd = dataframe\n",
    "        data_rd['value'] = data_rd['value'].astype('float')\n",
    "        data_rd['time'] = data_rd['time_local']\n",
    "        data_rd['time'] = pd.to_datetime(data_rd['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        dff2 = data_rd[['time', 'alias','value']].copy()\n",
    "        dff2 = dff2[dff2.alias == 'outside_air_temp']\n",
    "        dff2.set_index('time',inplace=True)\n",
    "        dff2 = dff2.resample('1h').first()\n",
    "\n",
    "        dff2['alias'] = 'air_temperature_1h'\n",
    "        dff2.reset_index(inplace=True)\n",
    "\n",
    "        dff2['time_local'] = dff2['time']\n",
    "\n",
    "        frames = [data_rd, dff2[['time_local', 'time', 'alias', 'value']]]\n",
    "        data_rd = pd.concat(frames, axis=0).reset_index(drop=True)\n",
    "        if data_rd.shape[0] > 0:\n",
    "            left_table = data_rd.drop(['alias','value','time_local'],axis=1)\n",
    "            left_table = left_table.groupby(\"time\").min().reset_index()\n",
    "            sub_df = data_rd[['time','alias','value']].copy()\n",
    "            sub_df['value'] = pd.to_numeric(sub_df['value'])\n",
    "            right_table = pd.pivot_table(sub_df,index='time',columns='alias',values='value').reset_index()\n",
    "            data_rd = pd.merge(left_table,right_table,on='time', how='outer')\n",
    "            df_data_rd= data_rd\n",
    "            #print(df_data_rd)\n",
    "\n",
    "        if data_rd.shape[0] > 0:\n",
    "            min_time = data_rd.time.min()\n",
    "            max_time = data_rd.time.max()\n",
    "            try:\n",
    "                if station_id is not None:\n",
    "                    meteo_data = station_data(station_id, min_time, max_time)\n",
    "                    data_rd = data_rd.merge(meteo_data, on='time', how='left')\n",
    "                    data_rd[['tavg', 'tmax', 'tmin']] = data_rd[['tavg', 'tmax', 'tmin']].astype('float')\n",
    "                else:\n",
    "                    print('Station is None')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "            for i in range(df.shape[0]):\n",
    "                obj = kpi(df.iloc[i], data_rd)\n",
    "                dff = obj.return_log()\n",
    "                logs = pd.concat([logs, dff], axis=0)\n",
    "\n",
    "                if df.iloc[i]['mute_logs']:\n",
    "                    aliases_list = df.iloc[i]['mute_logs'].split(',')\n",
    "                    for alias in aliases_list:\n",
    "                        logs = logs[~logs.aliases.str.contains(alias.strip())]\n",
    "        else:\n",
    "            print(f\"Data is not available from {start_time} to {end_time}\")\n",
    "\n",
    "\n",
    "        if logs.shape[0]>0:\n",
    "\n",
    "            logs['time'] = logs['time'].dt.date\n",
    "\n",
    "            logs.rename(columns={\"time\":\"date\"},inplace=True)\n",
    "            logs['client_id'] = client_id\n",
    "            logs['device_id'] = device_id\n",
    "            logs['date'] = logs['date'].astype(str)\n",
    "            # load the logs dataframe to destination table in the bigquery\n",
    "            #table = client.get_table(f\"{project}.{dataset_dest}.{table_dest}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('no logs saved')\n",
    "    else:\n",
    "        print(\"All rules are muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4cec7fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_comment_': 'This is KPI settings', 'settings': {'mute_rules': False, 'station_id': '10567'}, 'rules': {'kpi_name': {'1': 'run_time_hours', '2': 'run_time_fraction', '3': 'ahu_air_return_temp_fraction_18C-24C', '4': 'hx_1_hot_water_delta_temp_sum', '5': 'hx_2_hot_water_delta_temp_sum', '6': 'outside_air_temp_mean', '7': 'outside_air_temp_max', '8': 'HDD_15C', '9': 'CDD_15C', '10': 'co2_ppm_fraction_<500ppm', '11': 'co2_ppm_mean', '12': 'ahu_air_return_temp_mean', '13': 'hot_water_temp_mean', '14': 'predicted_heat_kwh', '15': 'baseline_gas_kwh'}, 'alias_a': {'1': 'enable_cmd', '2': 'enable_cmd', '3': 'ahu_air_return_temp_sensor', '4': 'heatExchanger_1_primaryLoop_hot_water_leaving_temp_sensor', '5': 'heatExchanger_2_primaryLoop_hot_water_leaving_temp_sensor', '6': 'air_temperature_1h', '7': 'air_temperature_1h', '8': 'air_temperature_1h', '9': 'air_temperature_1h', '10': 'ahu_co2_return_sensor', '11': 'ahu_co2_return_sensor', '12': 'ahu_air_return_temp_sensor', '13': 'heatExchanger_secondaryLoop_hot_water_leaving_temp_sensor', '14': 'heatExchanger_primaryLoop_hot_water_entering_temp_sensor', '15': 'tmin'}, 'alias_b': {'1': None, '2': None, '3': 'ahu_fan_air_enable_return_cmd', '4': 'heatExchanger_primaryLoop_hot_water_entering_temp_sensor', '5': 'heatExchanger_primaryLoop_hot_water_entering_temp_sensor', '6': None, '7': None, '8': None, '9': None, '10': 'ahu_fan_air_enable_return_cmd', '11': None, '12': None, '13': None, '14': 'heatExchanger_1_primaryLoop_hot_water_leaving_temp_sensor', '15': 'tmax'}, 'alias_c': {'1': None, '2': None, '3': None, '4': 'heatExchanger_1_primaryLoop_hot_water_valve_sp', '5': 'heatExchanger_2_primaryLoop_hot_water_valve_sp', '6': None, '7': None, '8': None, '9': None, '10': None, '11': None, '12': None, '13': None, '14': 'heatExchanger_2_primaryLoop_hot_water_leaving_temp_sensor', '15': None}, 'alias_d': {'1': None, '2': None, '3': None, '4': None, '5': None, '6': None, '7': None, '8': None, '9': None, '10': None, '11': None, '12': None, '13': None, '14': 'heatExchanger_1_primaryLoop_hot_water_valve_sp', '15': None}, 'alias_e': {'1': None, '2': None, '3': None, '4': None, '5': None, '6': None, '7': None, '8': None, '9': None, '10': None, '11': None, '12': None, '13': None, '14': 'heatExchanger_2_primaryLoop_hot_water_valve_sp', '15': None}, 'alias_f': {'1': None, '2': None, '3': None, '4': None, '5': None, '6': None, '7': None, '8': None, '9': None, '10': None, '11': None, '12': None, '13': None, '14': None, '15': None}, 'equation_1': {'1': 'alias_a*24', '2': 'alias_a', '3': '(1 if alias_a>18 and alias_a<27 else 0) if alias_b==1 else np.nan', '4': 'max((alias_b-alias_a)*alias_c,0)', '5': 'max((alias_b-alias_a)*alias_c,0)', '6': None, '7': None, '8': 'max(15-alias_a,0)', '9': 'max(alias_a-15,0)', '10': 'alias_a<500 and alias_b==1', '11': None, '12': None, '13': None, '14': '1000 * (((alias_a - alias_b)*alias_d) + ((alias_a - alias_c) * alias_e)) / 12276.2', '15': '(15-(alias_a + alias_b)/2) if (15>alias_b) else (0 if 15<=alias_a else (15-alias_a)*(0.08+0.42*(15-alias_a)/(alias_b-alias_a)))'}, 'equation_2': {'1': None, '2': None, '3': '0 if pd.isna(value) else value', '4': None, '5': None, '6': None, '7': None, '8': None, '9': None, '10': None, '11': None, '12': None, '13': None, '14': None, '15': '54.05 * value * 5 - 2907.5'}, 'aggregation': {'1': 'mean', '2': 'mean', '3': 'mean', '4': 'sum', '5': 'sum', '6': 'mean', '7': 'max', '8': 'mean', '9': 'mean', '10': 'mean', '11': 'mean', '12': 'mean', '13': 'mean', '14': 'mean', '15': 'mean'}, 'start_time': {'1': None, '2': None, '3': None, '4': None, '5': None, '6': None, '7': None, '8': None, '9': None, '10': None, '11': None, '12': None, '13': None, '14': None, '15': None}, 'end_time': {'1': None, '2': None, '3': None, '4': None, '5': None, '6': None, '7': None, '8': None, '9': None, '10': None, '11': None, '12': None, '13': None, '14': None, '15': None}, 'mute_logs': {'1': None, '2': None, '3': None, '4': None, '5': None, '6': None, '7': None, '8': None, '9': None, '10': None, '11': 'ahu_2_co2_return_sensor_max, ahu_2_co2_return_sensor_min', '12': None, '13': None, '14': None, '15': None}}, 'logs_link': 'https://colab.research.google.com/drive/16lMaXuYFvn-BCOy2ZuAUO8aYpUyEneEQ'}\n",
      "            kpi_name     alias_a alias_b alias_c alias_d alias_e alias_f  \\\n",
      "2  run_time_fraction  enable_cmd    None    None    None    None    None   \n",
      "\n",
      "  equation_1 equation_2 aggregation start_time end_time mute_logs  \n",
      "2    alias_a       None        mean       None     None      None  \n"
     ]
    }
   ],
   "source": [
    "## read the config file here \n",
    "# ======================================\n",
    "with open('D:\\\\Cloud-Track\\\\assign1\\\\kpi_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    print(config)\n",
    "    \n",
    "    rules=config['rules']\n",
    "    rules=pd.DataFrame(rules)\n",
    "    #print(rules)\n",
    "    rules['mute_logs'] = None\n",
    "    rules=rules[rules.kpi_name == 'run_time_fraction']\n",
    "    print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4f573b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\Cloud-Track\\\\assign1\\\\kpi_config.json') as file:\n",
    "    config = json.loads(file.read())\n",
    "    file.close()\n",
    "df = pd.DataFrame(config['rules'])\n",
    "#print(df)\n",
    "df\n",
    "rules= rules[rules.kpi_name =='outside_air_temp_mean']\n",
    "#rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a8b86de9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column name ==>  alias_a\n",
      "group_id ==>  enable_cmd\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  enable_cmd\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  ahu_air_return_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_b\n",
      "group_id ==>  ahu_fan_air_enable_return_cmd\n",
      "group_id data type ==>  <class 'str'>\n",
      "[]\n",
      "[['ahu_10_air_return_temp_sensor', 'ahu_10_fan_air_return_enable_cmd'], ['ahu_11_air_return_temp_sensor', 'ahu_11_fan_air_return_enable_cmd'], ['ahu_12_air_return_temp_sensor', 'ahu_12_fan_air_return_enable_cmd'], ['ahu_1_air_return_temp_sensor', 'ahu_1_fan_air_return_enable_cmd'], ['ahu_2_air_return_temp_sensor', 'ahu_2_fan_air_return_enable_cmd'], ['ahu_3_air_return_temp_sensor', 'ahu_3_fan_air_return_enable_cmd'], ['ahu_4_air_return_temp_sensor', 'ahu_4_fan_air_return_enable_cmd'], ['ahu_5_air_return_temp_sensor', 'ahu_5_fan_air_return_enable_cmd'], ['ahu_6_air_return_temp_sensor', 'ahu_6_fan_air_return_enable_cmd'], ['ahu_7_air_return_temp_sensor', 'ahu_7_fan_air_return_enable_cmd']]\n",
      "column name ==>  alias_a\n",
      "group_id ==>  heatExchanger_1_primaryLoop_hot_water_leaving_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_b\n",
      "group_id ==>  heatExchanger_primaryLoop_hot_water_entering_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_c\n",
      "group_id ==>  heatExchanger_1_primaryLoop_hot_water_valve_sp\n",
      "group_id data type ==>  <class 'str'>\n",
      "[]\n",
      "[['heatExchanger_1_primaryLoop_hot_water_leaving_temp_sensor', 'heatExchanger_primaryLoop_hot_water_entering_temp_sensor', 'heatExchanger_1_primaryLoop_hot_water_valve_sp']]\n",
      "column name ==>  alias_a\n",
      "group_id ==>  heatExchanger_2_primaryLoop_hot_water_leaving_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_b\n",
      "group_id ==>  heatExchanger_primaryLoop_hot_water_entering_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_c\n",
      "group_id ==>  heatExchanger_2_primaryLoop_hot_water_valve_sp\n",
      "group_id data type ==>  <class 'str'>\n",
      "[]\n",
      "[['heatExchanger_2_primaryLoop_hot_water_leaving_temp_sensor', 'heatExchanger_primaryLoop_hot_water_entering_temp_sensor', 'heatExchanger_2_primaryLoop_hot_water_valve_sp']]\n",
      "column name ==>  alias_a\n",
      "group_id ==>  air_temperature_1h\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  air_temperature_1h\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  air_temperature_1h\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  air_temperature_1h\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  ahu_co2_return_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_b\n",
      "group_id ==>  ahu_fan_air_enable_return_cmd\n",
      "group_id data type ==>  <class 'str'>\n",
      "[]\n",
      "[['ahu_10_co2_return_sensor', 'ahu_10_fan_air_return_enable_cmd'], ['ahu_11_co2_return_sensor', 'ahu_11_fan_air_return_enable_cmd'], ['ahu_12_co2_return_sensor', 'ahu_12_fan_air_return_enable_cmd'], ['ahu_1_co2_return_sensor', 'ahu_1_fan_air_return_enable_cmd'], ['ahu_2_co2_return_sensor', 'ahu_2_fan_air_return_enable_cmd'], ['ahu_3_co2_return_sensor', 'ahu_3_fan_air_return_enable_cmd'], ['ahu_4_co2_return_sensor', 'ahu_4_fan_air_return_enable_cmd'], ['ahu_5_co2_return_sensor', 'ahu_5_fan_air_return_enable_cmd'], ['ahu_6_co2_return_sensor', 'ahu_6_fan_air_return_enable_cmd'], ['ahu_7_co2_return_sensor', 'ahu_7_fan_air_return_enable_cmd']]\n",
      "column name ==>  alias_a\n",
      "group_id ==>  ahu_co2_return_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  ahu_air_return_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  heatExchanger_secondaryLoop_hot_water_leaving_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_a\n",
      "group_id ==>  heatExchanger_primaryLoop_hot_water_entering_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_b\n",
      "group_id ==>  heatExchanger_1_primaryLoop_hot_water_leaving_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_c\n",
      "group_id ==>  heatExchanger_2_primaryLoop_hot_water_leaving_temp_sensor\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_d\n",
      "group_id ==>  heatExchanger_1_primaryLoop_hot_water_valve_sp\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_e\n",
      "group_id ==>  heatExchanger_2_primaryLoop_hot_water_valve_sp\n",
      "group_id data type ==>  <class 'str'>\n",
      "[]\n",
      "[['heatExchanger_primaryLoop_hot_water_entering_temp_sensor', 'heatExchanger_1_primaryLoop_hot_water_leaving_temp_sensor', 'heatExchanger_2_primaryLoop_hot_water_leaving_temp_sensor', 'heatExchanger_1_primaryLoop_hot_water_valve_sp', 'heatExchanger_2_primaryLoop_hot_water_valve_sp']]\n",
      "column name ==>  alias_a\n",
      "group_id ==>  tmin\n",
      "group_id data type ==>  <class 'str'>\n",
      "column name ==>  alias_b\n",
      "group_id ==>  tmax\n",
      "group_id data type ==>  <class 'str'>\n",
      "[]\n",
      "[['tmin', 'tmax']]\n"
     ]
    }
   ],
   "source": [
    "# data for not more than 5 days\n",
    "start_time = arrow.get('2022-10-01 00:00:00')\n",
    "end_time  = arrow.get('2022-12-31 00:00:00')\n",
    "\n",
    "# You should not edit kwargs, except your config variable\n",
    "kwargs = {\n",
    "      'start_time'      : start_time,\n",
    "      'end_time'        : end_time,\n",
    "      'project'         : 'thermosphr-prod',\n",
    "      'client_id'       : 'gera-arcaden-germany',\n",
    "      'dataset_src'     : 'bi',\n",
    "      'location'        : 'Europe/Berlin',\n",
    "      'dataset_dest'    : 'bi',\n",
    "      'table_src'       : 'datamart_v2',\n",
    "      'table_dest'      : 'bi',\n",
    "      'config'          : config,                      # your config should be given here.\n",
    "      'device_id'       : 'ayQAWQgc'\n",
    "}\n",
    "\n",
    "kpi_function(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9256d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc336b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
