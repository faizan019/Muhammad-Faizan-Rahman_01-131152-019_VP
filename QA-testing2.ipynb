{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e49c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from functools import reduce\n",
    "import regex as re\n",
    "import warnings\n",
    "import arrow\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c82dd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>alias</th>\n",
       "      <th>time_local</th>\n",
       "      <th>ahu_10_air_discharge_flow_sensor</th>\n",
       "      <th>ahu_10_air_discharge_flow_sp</th>\n",
       "      <th>ahu_10_air_discharge_temp_sensor</th>\n",
       "      <th>ahu_10_air_discharge_temp_sp</th>\n",
       "      <th>ahu_10_air_return_flow_sensor</th>\n",
       "      <th>ahu_10_air_return_flow_sp</th>\n",
       "      <th>ahu_10_air_return_temp_sensor</th>\n",
       "      <th>ahu_10_chilled_water_valve_sp</th>\n",
       "      <th>ahu_10_co2_return_sensor</th>\n",
       "      <th>...</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>pressure</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>specific_humidity</th>\n",
       "      <th>visibility</th>\n",
       "      <th>wetBulb</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_speed_max</th>\n",
       "      <th>zenith_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.246119</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.024424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>428.914886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.9100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-0.282678</td>\n",
       "      <td>283.3600</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>148.651836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-01 01:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.188492</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.989847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>426.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.9425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003398</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-0.300238</td>\n",
       "      <td>294.0075</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.3275</td>\n",
       "      <td>147.457077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-01 01:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.200016</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.966795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.147064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.9750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-0.317798</td>\n",
       "      <td>304.6550</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>146.055529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-01 01:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.153915</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.966795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>428.985596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1025.0075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-0.335358</td>\n",
       "      <td>315.3025</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>144.473877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-12-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.119339</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.989847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>429.763458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1025.0400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-0.352918</td>\n",
       "      <td>325.9500</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>142.737577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "alias           time_local  ahu_10_air_discharge_flow_sensor  \\\n",
       "0      2022-12-01 01:00:00                               0.0   \n",
       "1      2022-12-01 01:15:00                               0.0   \n",
       "2      2022-12-01 01:30:00                               0.0   \n",
       "3      2022-12-01 01:45:00                               0.0   \n",
       "4      2022-12-01 02:00:00                               0.0   \n",
       "\n",
       "alias  ahu_10_air_discharge_flow_sp  ahu_10_air_discharge_temp_sensor  \\\n",
       "0                               0.0                         19.246119   \n",
       "1                               0.0                         19.188492   \n",
       "2                               0.0                         19.200016   \n",
       "3                               0.0                         19.153915   \n",
       "4                               0.0                         19.119339   \n",
       "\n",
       "alias  ahu_10_air_discharge_temp_sp  ahu_10_air_return_flow_sensor  \\\n",
       "0                              15.5                            0.0   \n",
       "1                              15.5                            0.0   \n",
       "2                              15.5                            0.0   \n",
       "3                              15.5                            0.0   \n",
       "4                              15.5                            0.0   \n",
       "\n",
       "alias  ahu_10_air_return_flow_sp  ahu_10_air_return_temp_sensor  \\\n",
       "0                            0.0                      18.024424   \n",
       "1                            0.0                      17.989847   \n",
       "2                            0.0                      17.966795   \n",
       "3                            0.0                      17.966795   \n",
       "4                            0.0                      17.989847   \n",
       "\n",
       "alias  ahu_10_chilled_water_valve_sp  ahu_10_co2_return_sensor  ...  \\\n",
       "0                                0.0                428.914886  ...   \n",
       "1                                0.0                426.015625  ...   \n",
       "2                                0.0                427.147064  ...   \n",
       "3                                0.0                428.985596  ...   \n",
       "4                                0.0                429.763458  ...   \n",
       "\n",
       "alias  precipitation   pressure  snow_depth  specific_humidity  visibility  \\\n",
       "0                0.0  1024.9100         0.0           0.003401       24.14   \n",
       "1                0.0  1024.9425         0.0           0.003398       24.14   \n",
       "2                0.0  1024.9750         0.0           0.003395       24.14   \n",
       "3                0.0  1025.0075         0.0           0.003392       24.14   \n",
       "4                0.0  1025.0400         0.0           0.003390       24.14   \n",
       "\n",
       "alias   wetBulb  wind_direction  wind_speed  wind_speed_max  zenith_angle  \n",
       "0     -0.282678        283.3600        0.32          0.3000    148.651836  \n",
       "1     -0.300238        294.0075        0.35          0.3275    147.457077  \n",
       "2     -0.317798        304.6550        0.38          0.3550    146.055529  \n",
       "3     -0.335358        315.3025        0.41          0.3825    144.473877  \n",
       "4     -0.352918        325.9500        0.44          0.4100    142.737577  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'thermosphr_key.json', scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id,)\n",
    "\n",
    "project_id = 'thermosphr-prod'\n",
    "client_id = 'gera-arcaden-germany'\n",
    "device_id = 'ayQAWQgc'\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "start_time = '2022-12-01'\n",
    "end_time = '2022-12-02'\n",
    "\n",
    " \n",
    "\n",
    "#client = bigquery.Client(project = project_id)\n",
    "query = client.query(\n",
    "            f\"\"\"\n",
    "            SELECT STRING(time_local) AS time_local,\n",
    "            alias,value, date_update, propertyId\n",
    "            FROM `{project_id}.bi.datamart_v2` ('{device_id}','{start_time}','{end_time}')\n",
    "            WHERE alias IS NOT NULL\n",
    "            AND time_local >= '{start_time}'\n",
    "            AND time_local < '{end_time}'\n",
    "            AND client_id = '{client_id}'\n",
    "            ORDER BY time_local \n",
    "                \"\"\"\n",
    "        )\n",
    "dataframe = query.to_dataframe()\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "dataframe['value'] = dataframe['value'].astype('float')\n",
    "dff2 = dataframe[['time_local', 'alias', 'value']].copy()\n",
    "data_rd = pd.pivot_table(dff2, index='time_local', columns='alias', values='value').reset_index()\n",
    "data_rd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c76e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import regex as re\n",
    "import warnings\n",
    "import arrow\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "\n",
    "# supress warnings to debugging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "class geFramework():\n",
    "    def __init__(self, rule, batch):\n",
    "        # get subset of data, min-time and max-time for the given start and end time\n",
    "        # batch = self.__batch_subset(batch, rule['duration_hours'])\n",
    "        if 'missing_values' not in rule['rule_name']:\n",
    "            batch.fillna(method='ffill', inplace=True)\n",
    "        # indices of the rule dataframe\n",
    "        alias_cols = rule.index\n",
    "        subs = 'alias'\n",
    "        # get columns of rule book that alias in their names\n",
    "        self.__res = [\n",
    "            alias_col for alias_col in alias_cols if subs in alias_col]\n",
    "        self.__rule = rule\n",
    "        # create empty dataframe for the logs\n",
    "        self.__df_logs = pd.DataFrame(columns=[\n",
    "            'rule_name', 'aliases', 'criticality', 'time_local', 'test', 'date_update','qa'])\n",
    "        aliases = []\n",
    "        # get a list of aliases for the given targets in the rule book and append that lists\n",
    "        # in the aliases list\n",
    "        # in case alias_a is not null\n",
    "        if rule[self.__res[0]] is not None:\n",
    "            for i in self.__res:\n",
    "                if rule[i] is not None and '-' not in rule[i]:\n",
    "                    # get all aliases for given keywords in config columns\n",
    "                    aliases.append(self.__alias_substrings(\n",
    "                        batch, [rule[i]], col_name='alias'))\n",
    "                # In case minus sign is found to exclude that given alias\n",
    "                elif rule[i] is not None and '-' in rule[i]:\n",
    "                    keyword = rule[i].replace('-', '').strip()\n",
    "                    # get all aliases for given keyword in config columns\n",
    "                    excluded_aliases = self.__alias_substrings(\n",
    "                        batch, [keyword], col_name='alias')\n",
    "                    # get all columns(aliases) of the dataframe\n",
    "                    unique_aliases = batch.columns.to_list()\n",
    "                    # drop the given keyword's aliases and time_local from all aliases in dataframe\n",
    "                    required_aliases = list(\n",
    "                        set(unique_aliases) - set(excluded_aliases) - set(['time_local']))\n",
    "                    # append required aliases in the aliases list\n",
    "                    aliases.append(required_aliases)\n",
    "            # in case there is only one group of aliases in aliases list\n",
    "            if len(aliases) == 1:\n",
    "                # iterate over all group of  alaises\n",
    "                for alias in aliases[0]:\n",
    "                    # get a dataframe for each alias\n",
    "                    df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                              resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                    # in case dataframe is not empty\n",
    "                    if df.shape[0] > 0:\n",
    "                        # apply the equation that is given in the rule book (equation_1)\n",
    "                        df = self.apply_equation_1(\n",
    "                            self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "                        df = self.apply_equation_2(\n",
    "                            self, df, rule, equation=rule['equation_2'])\n",
    "                        # create logs\n",
    "                        df1 = self.__logs_data(df, rule, [alias])\n",
    "                        self.__df_logs = pd.concat(\n",
    "                            [self.__df_logs, df1], axis=0)\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {alias}')\n",
    "            # if in case there are multiple aliases groups in aliases list\n",
    "            else:\n",
    "                # create a list of different pairs of aliases\n",
    "                alias_pair = self.__alias_pairs(self, batch, rule, aliases)\n",
    "                # iterate for all pairs\n",
    "                for pair in alias_pair:\n",
    "                    # in case length of a pair is not equal to 0\n",
    "                    if len(pair) != 0:\n",
    "                        # get a dataframe for each pair\n",
    "                        df = self.alias_pairs_dataframe(self, data_rd=batch, aliases=pair, kpi=rule['rule_name'],\n",
    "                                                        resample_grain=None, daily_resample=None, agg=None,\n",
    "                                                        non_negative=None)\n",
    "                        # in case dataframe is not empty\n",
    "                        if df.shape[0] > 0:\n",
    "                            # apply the equation that is given in the rule book (equation_1)\n",
    "                            df = self.apply_equation_1(\n",
    "                                self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "                            df = self.apply_equation_2(\n",
    "                                self, df, rule, equation=rule['equation_2'])\n",
    "                            #                     # create logs\n",
    "                            df1 = self.__logs_data(df, rule, pair)\n",
    "                            self.__df_logs = pd.concat(\n",
    "                                [self.__df_logs, df1], axis=0)\n",
    "                    # in case dataframe is empty\n",
    "                    else:\n",
    "                        print(f'Data is not available for {pair}')\n",
    "        # if in case alias_a is null\n",
    "        else:\n",
    "            # get all columns (aliases) of the dataframe except time_local or time\n",
    "            aliases.append(self.__alias_substrings(\n",
    "                batch, None, col_name='alias'))\n",
    "            # iterate for all aliases\n",
    "            for alias in aliases[0]:\n",
    "                # get a dataframe for each pair\n",
    "                df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['rule_name'],\n",
    "                                          resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                # in case dataframe is not empty\n",
    "                if df.shape[0] > 0:\n",
    "                    # apply the equation that is given in the rule book (equation_1)\n",
    "                    df = self.apply_equation_1(\n",
    "                        self, df, equation=rule['equation_1'], temp=True).reset_index(drop=True)\n",
    "                    df = self.apply_equation_2(\n",
    "                        self, df, rule, equation=rule['equation_2'])\n",
    "                    #                     # create logs\n",
    "                    df1 = self.__logs_data(df, rule, [alias])\n",
    "                    self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "                # in case dataframe is empty\n",
    "                else:\n",
    "                    print(f'Data is not available for {alias}')\n",
    "    @staticmethod\n",
    "    def __logs_data(df=None, rule=None, alias_list=None, result=None):\n",
    "        \"\"\" Gets resultant dataframe and rules for each kpi and creates logs\n",
    "            in the form of a dataframe.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        Resultant dataframe for each kpi\n",
    "        rule:dataframe\n",
    "        Its the config file\n",
    "        tgt_alias:string\n",
    "        Name of the target alias\n",
    "        ref_alias: string\n",
    "        Name of the reference alias\n",
    "        \"\"\"\n",
    "        # create an empty dataframe\n",
    "        df1 = pd.DataFrame()\n",
    "        # concate all aliases as one string\n",
    "        if len(alias_list) == 1:\n",
    "            logs_alias = alias_list[0]\n",
    "        else:\n",
    "            logs_alias = \",\".join(alias_list)\n",
    "        # get current utc time (function running time) for date_update\n",
    "        current_time = datetime.datetime.utcnow().replace(second=0, microsecond=0)\n",
    "        df1 = df[['test', 'time_local']].copy()\n",
    "        df1['aliases'] = logs_alias\n",
    "        df1['rule_name'] = rule['rule_name']\n",
    "        df1['criticality'] = rule['criticality']\n",
    "        df1['date_update'] = current_time\n",
    "        df1['qa'] = rule['qa']\n",
    "        if rule.mute_alert is not None:\n",
    "            for x in rule.mute_alert[0].split(','):\n",
    "                df1['mute_alert'] = np.where(df1.aliases.str.contains(x), \"muted\", None)\n",
    "        else:\n",
    "            df1['mute_alert'] = None\n",
    "        return df1\n",
    "    @staticmethod\n",
    "    def count(df, max_delta):\n",
    "        try:\n",
    "            count_true = df['value'].value_counts()[True]\n",
    "        except:\n",
    "            count_true = 0\n",
    "        return count_true\n",
    "    @staticmethod\n",
    "    def streak(df):\n",
    "        \"\"\"This function calculates delta for only true values, in case\n",
    "        this calculated delta is greater or equal to the max_delta then\n",
    "        test will fail otherwise pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dataframe\n",
    "        Dataframe after applying equation\n",
    "        rule: dataframe\n",
    "        Each row of the config file as a dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        df: dataframe\n",
    "        delta: float\n",
    "        \"\"\"\n",
    "        # set True value to 1 and False to 0\n",
    "        df.loc[df[\"value\"] == True, \"value\"] = 1\n",
    "        df.loc[df[\"value\"] == False, \"value\"] = 0\n",
    "        # Check that original values are not equal to shifted values\n",
    "        df['expected_streak'] = df.value.ne(df.value.shift())\n",
    "        # get timelocal for the start of streak\n",
    "        df['streak_time'] = df.loc[df.expected_streak == True]['time_local']\n",
    "        # convert streak start time to datetime\n",
    "        df['streak_time'] = pd.to_datetime(df['streak_time'])\n",
    "        # converte time_local to datetime\n",
    "        df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "        # fill null values from previous values by using forward filling\n",
    "        df = df.fillna(method='ffill')\n",
    "        # calculate delta in minutes\n",
    "        df['delta'] = (df['time_local'] - df['streak_time']\n",
    "                       ).dt.total_seconds() / 60\n",
    "        return df\n",
    "    @staticmethod\n",
    "    def apply_equation_1(self, df=None, equation=None, temp=None):\n",
    "        \"\"\" This function basically takes a dataframe and an equation\n",
    "        and then tries to apply this equation and results in the form of boolean.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: Dataframe\n",
    "        A Dataframe for an aliases or a pair of aliases\n",
    "        equation: String\n",
    "        This string has a set of instruction\n",
    "        Returns\n",
    "        -------\n",
    "        df: Dataframe\n",
    "        \"\"\"\n",
    "        def func(x):\n",
    "            y = eval(equation)\n",
    "            return y\n",
    "        if df.shape[0] > 0:\n",
    "            df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "            # in case equation is given and not missing_values or constant values\n",
    "            if equation is not None:\n",
    "                # iterate for given given alias columns in the config file\n",
    "                for col in self.__res:\n",
    "                    # add dataframe with alias column\n",
    "                    if col in equation:\n",
    "                        col_name = f\"x['{col}']\"\n",
    "                        equation = equation.replace(col, col_name)\n",
    "                # time and date based conditions\n",
    "                if 'day' in equation:\n",
    "                    df['day'] = df.time_local.dt.dayofweek\n",
    "                    equation = equation.replace('day', 'x[\"day\"]')\n",
    "                if 'date' in equation:\n",
    "                    df['date'] = df.time_local.dt.date\n",
    "                    equation = equation.replace('date', 'str(x[\"date\"])')\n",
    "                if 'time' in equation:\n",
    "                    df['time'] = df.time_local.apply(lambda x: x.strftime('%H:%M'))\n",
    "                    equation = equation.replace('time', 'str(x[\"time\"])')\n",
    "                if 'hour' in equation:\n",
    "                    df['hour'] = df.time_local.dt.hour\n",
    "                    equation = equation.replace('hour', 'x[\"hour\"]')\n",
    "                if 'minute' in equation:\n",
    "                    df['minute'] = df.time_local.dt.minute\n",
    "                    equation = equation.replace('minute', 'x[\"minute\"]')\n",
    "                if 'month' in equation:\n",
    "                    df['month'] = df.time_local.dt.month\n",
    "                    equation = equation.replace('month', 'x[\"month\"]')\n",
    "                # apply equation\n",
    "                df['value'] = df.apply(func, axis=1)\n",
    "            # in case equation is None\n",
    "            else:\n",
    "                df['value'] = df['alias_a']\n",
    "        return df\n",
    "    @staticmethod\n",
    "    def apply_equation_2(self, df=None, rule=None, equation=None, kpi=None):\n",
    "        if df.shape[0] > 0:\n",
    "            if equation is not None:\n",
    "                if 'streak' in equation:\n",
    "                    df = self.streak(df)\n",
    "                    duration = float(re.findall(\"[0-9.]+\", equation)[0])\n",
    "                    frac, whole = math.modf(duration)\n",
    "                    duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                    equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "                    df['test'] = eval(equation)\n",
    "                else:\n",
    "                    aggregation = re.findall('\\w+', equation)[0]\n",
    "                    operator = re.findall(\"[^a-zA-Z0-9_.,\\s\\(\\)\\[\\]]+\", equation)[0]\n",
    "                    window = rule['duration_hours']\n",
    "                    duration = str(window) + 'h'\n",
    "                    threshold = float(re.findall(\"[0-9]+\", equation)[0])\n",
    "                    df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "                    df.set_index('time_local', inplace=True)\n",
    "                    df['test'] = eval(f\"df['value'].rolling('{duration}',min_periods = {window} * 4 ).agg('{aggregation}') {operator} {threshold}\")\n",
    "                    df.reset_index(inplace=True)\n",
    "            else:\n",
    "                df.self.streak(df)\n",
    "                duration = float(rule['duration_hours'])\n",
    "                frac, whole = math.modf(duration)\n",
    "                duration = (whole * 60 - 15 if whole != 0 else 0) + (frac * 60)\n",
    "                equation = f'(df.delta >={duration}) & (df.value == True)'\n",
    "                df['test'] = eval(equation)\n",
    "            #             display(df)\n",
    "            df = df.loc[df.test == True]\n",
    "            df.test = 'fail'\n",
    "        return df\n",
    "    @staticmethod\n",
    "    def __alias_substrings(df, in_list, col_name='alias'):\n",
    "        \"\"\" This function returns a list of aliases for a given reference in the\n",
    "            the config file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        It's the source dataframe\n",
    "        in_list: list\n",
    "        It's the list of given aliases in the config file\n",
    "        col_name: string\n",
    "        It's the column name in the source dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_aliases: list\n",
    "        This list contains the aliases that are filtered out\n",
    "        \"\"\"\n",
    "        groups = in_list\n",
    "        if in_list is not None:\n",
    "            filtered_aliases = []\n",
    "            substrings = []\n",
    "            # Creating list of substrings\n",
    "            for group_id in groups:\n",
    "                #print(group_id)\n",
    "                substrings.append(group_id.split('_'))\n",
    "            unique_aliases = df.columns\n",
    "            #print(unique_aliases)\n",
    "            # Removing None from the list of unique aliases\n",
    "            unique_aliases = list(filter(None, unique_aliases))\n",
    "            # Filter the aliases that are to be used for the KPI\n",
    "            for alias in unique_aliases:\n",
    "                for sub in substrings:\n",
    "                    if all(a.strip() in alias for a in sub):\n",
    "                        #print(alias)\n",
    "                        filtered_aliases.append(alias)\n",
    "        else:\n",
    "            remove_cols = ['time_local']\n",
    "            filtered_aliases = list(set(df.columns.to_list()) - set(remove_cols))\n",
    "        return filtered_aliases\n",
    "    @staticmethod\n",
    "    def __batch_subset(batch, duration):\n",
    "        \"\"\"\n",
    "        Gets a dataframe and a specific duration, where it takes a subset\n",
    "        of dataframe that falls in the specific hours of duration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: dataframe\n",
    "        It's the main dataframe whose subset is to be taken based on time column.\n",
    "        duration:\n",
    "        A time period for which a subset of the dataframe\n",
    "        will be taken.\n",
    "        Returns\n",
    "        -------\n",
    "        batch_subset: dataframe\n",
    "        A subset of the main dataframe for which timing is in between\n",
    "        the specific duration of hours.\n",
    "        batch_min_time: time stamp\n",
    "        It's the start time of the subset dataframe\n",
    "        batch_max_time: time stamp\n",
    "        It's the end time of the subset dataframe.\n",
    "        \"\"\"\n",
    "        # set min and max time variable as global\n",
    "        global batch_min_time\n",
    "        global batch_max_time\n",
    "        # get maximum timestamp of the batch\n",
    "        batch_max_time = str(arrow.get(batch['time_local'].max()).datetime).split('+')[0]\n",
    "        # convert string formated timestamp to datetime object\n",
    "        batch_max_time = datetime.datetime.strptime(batch_max_time, '%Y-%m-%d %H:%M:%S')\n",
    "        # get minimum timestamp by considering duration hours\n",
    "        batch_min_time = batch_max_time - datetime.timedelta(hours=int(duration),\n",
    "                                                             minutes=int(round(duration % 1, 4) * 60))\n",
    "        # get subset of data for the minimum and maximum timestamps\n",
    "        batch_subset = batch[(batch.time_local >= str(batch_min_time)) & (batch.time_local <= str(batch_max_time))]\n",
    "        return batch_subset\n",
    "    @staticmethod\n",
    "    def alias_pairs_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                              daily_resample=None, agg=None, non_negative=None):\n",
    "        # create an empty dataframe\n",
    "        df_master = pd.DataFrame()\n",
    "        # get indices of aliases's elements\n",
    "        res = [index for index, val in enumerate(aliases)]\n",
    "        # create copy of the aliases\n",
    "        alias_cols = aliases.copy()\n",
    "        # append time_local into the copied alaises\n",
    "        alias_cols.append('time_local')\n",
    "        # get config file aliases columns having same index as res list's elements\n",
    "        cols = [val for index, val in enumerate(self.__res) if index in res]\n",
    "        # append time_local into the cols list\n",
    "        cols.append('time_local')\n",
    "        # get data for the given aliases\n",
    "        df = data_rd[alias_cols].copy()\n",
    "        # assign config file alias column name\n",
    "        df.columns = cols\n",
    "        # in case there is data for the pair\n",
    "        if df.shape[0] > 0:\n",
    "            # assign df to df_master\n",
    "            df_master = df\n",
    "        return df_master\n",
    "    @staticmethod\n",
    "    def alias_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                        daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        df = data_rd[['time_local', aliases]].copy()\n",
    "        df.rename(columns={aliases: 'alias_a'}, inplace=True)\n",
    "        if df.shape[0] > 0:\n",
    "            df_master = df\n",
    "        return df_master\n",
    "    @staticmethod\n",
    "    def __alias_pairs(self, data_rd, rule, aliases):\n",
    "        self.__index = [index for index, val in enumerate(aliases)]\n",
    "        col_names = self.__res\n",
    "        col_names = [val for index, val in enumerate(col_names) if index in self.__index]\n",
    "        null_list_index = [index for index, val in enumerate(aliases) if len(val) == 0]\n",
    "        non_null_dfs_list = []\n",
    "        null_dfs_list = []\n",
    "        for i in range(len(aliases)):\n",
    "            non_null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            if len(aliases[i]) > 0:\n",
    "                for alias in aliases[i]:\n",
    "                    keys = alias.split(\"_\")\n",
    "                    for ele in rule[col_names[i]].strip().split('_'):\n",
    "                        keys.remove(ele)\n",
    "                    if len(keys) == 0:\n",
    "                        df = pd.DataFrame([[alias, None]], columns=['alias', 'keys'])\n",
    "                        null_alias_df = pd.concat([null_alias_df, df], axis=0, ignore_index=True)\n",
    "                    else:\n",
    "                        key = '_'.join(map(str, keys))\n",
    "                        df = pd.DataFrame([[alias, key]], columns=['alias', 'keys'])\n",
    "                        non_null_alias_df = pd.concat([non_null_alias_df, df], axis=0, ignore_index=True)\n",
    "            if non_null_alias_df.shape[0] > 0:\n",
    "                non_null_dfs_list.append(non_null_alias_df)\n",
    "            if null_alias_df.shape[0] > 0:\n",
    "                null_dfs_list.append(null_alias_df)\n",
    "        if len(null_dfs_list) == len(col_names):\n",
    "            nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        else:\n",
    "            if len(non_null_dfs_list) > 1:\n",
    "                non_nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, on=[\"keys\"], how=\"inner\"),\n",
    "                                            non_null_dfs_list)\n",
    "            elif len(non_null_dfs_list) == 1:\n",
    "                non_nulls_df_merge = non_null_dfs_list[0]\n",
    "            else:\n",
    "                non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            if len(null_dfs_list) > 1:\n",
    "                nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            elif len(null_dfs_list) == 1:\n",
    "                nulls_df_merge = null_dfs_list[0]\n",
    "            else:\n",
    "                nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        if nulls_df_merge.shape[0] > 0 and non_nulls_df_merge.shape[0] > 0:\n",
    "            alias_pairs_df = pd.merge(nulls_df_merge, non_nulls_df_merge, how='cross')\n",
    "        elif nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = non_nulls_df_merge\n",
    "        elif non_nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = nulls_df_merge\n",
    "        subs = 'keys'\n",
    "        keys_cols = [col for col in alias_pairs_df if subs in col]\n",
    "        alias_pairs_df = alias_pairs_df.drop(keys_cols, axis=1)\n",
    "        if alias_pairs_df.shape[1] == len(col_names):\n",
    "            alias_pairs = alias_pairs_df.values.tolist()\n",
    "        else:\n",
    "            alias_pairs = []\n",
    "        return alias_pairs\n",
    "    @staticmethod\n",
    "    def message_builder(logs, data_start_time, data_end_time, data, client_id, logs_link):\n",
    "        \"\"\"\n",
    "        Gets a dataframe, that has information about the tests that are either passed of failed\n",
    "        Parameters\n",
    "        ----------\n",
    "        logs_df: Dataframe\n",
    "        This is the dataframe, which contains different tests that are performed.\n",
    "        Returns\n",
    "        -------\n",
    "        message: string\n",
    "        It contains\n",
    "        \"\"\"\n",
    "        current_time = str(datetime.datetime.utcnow().replace(second=0, microsecond=0)) + ' UTC'\n",
    "        message = f\"*{client_id}*  {data_end_time}\\n\"\n",
    "        if (len(logs.loc[logs.test == 'fail']) > 0):\n",
    "            df_false = logs.loc[logs['test'] == 'fail']\n",
    "            rule_names = df_false.loc[df_false.criticality == 'high']['rule_name'].unique()\n",
    "            for i in range(len(rule_names)):\n",
    "                message = message + f\"\\n *{rule_names[i]}*\"\n",
    "            if data.shape[0] > 0:\n",
    "                message = message + f\"\\n\\nDuplicate aliases\"\n",
    "                message = message + f\"\\n {data}\"\n",
    "            message = message + f\"\"\"\\n\n",
    "    <{logs_link}>\n",
    "            \"\"\"\n",
    "        return message\n",
    "    def slack_alert(message, slack_webhook):\n",
    "        \"\"\"\n",
    "        Gets a string of information about the tests are failed having only high criticality, and\n",
    "        send this information to a slack channel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        message: string\n",
    "        Information about the tests that have high criticality.\n",
    "        Returns\n",
    "        -------\n",
    "        This function returns nothing\n",
    "        \"\"\"\n",
    "        webhook_url = slack_webhook\n",
    "        slack_data = {'text': message}\n",
    "        response = requests.post(\n",
    "            webhook_url, data=json.dumps(slack_data),\n",
    "            headers={'Content-Type': 'application/json'}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(\n",
    "                'Request to slack returned an error %s, the response is:\\n%s'\n",
    "                % (response.status_code, response.text)\n",
    "            )\n",
    "    def return_log(self):\n",
    "        return self.__df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e32a5b9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule_name</th>\n",
       "      <th>alias_a</th>\n",
       "      <th>alias_b</th>\n",
       "      <th>alias_c</th>\n",
       "      <th>alias_d</th>\n",
       "      <th>equation_1</th>\n",
       "      <th>equation_2</th>\n",
       "      <th>duration_hours</th>\n",
       "      <th>criticality</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>mute_alert</th>\n",
       "      <th>mute_logs</th>\n",
       "      <th>qa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>pump_speed_&gt;90perc_48h</td>\n",
       "      <td>water_flow_sp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>alias_a &gt; 90</td>\n",
       "      <td>streak &gt;= 48</td>\n",
       "      <td>48</td>\n",
       "      <td>high</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>hvac_qa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 rule_name        alias_a alias_b alias_c alias_d  \\\n",
       "66  pump_speed_>90perc_48h  water_flow_sp    None    None    None   \n",
       "\n",
       "      equation_1    equation_2  duration_hours criticality start_date  \\\n",
       "66  alias_a > 90  streak >= 48              48        high       None   \n",
       "\n",
       "   end_date mute_alert mute_logs       qa  \n",
       "66     None       None      None  hvac_qa  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows=None\n",
    "with open('D:\\Thermosphr\\QA & KPI\\QA\\QA\\qa_config.json') as file:\n",
    "    config  = json.loads(file.read())\n",
    "    file.close()\n",
    "rules = config['rules']\n",
    "rules = pd.DataFrame(rules)\n",
    "rules['mute_logs'] = None\n",
    "rules= rules[rules.rule_name=='pump_speed_>90perc_48h']\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9be10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logs(rule, data_rd, logs, templogs):\n",
    "    obj = geFramework(rule, data_rd)\n",
    "\n",
    "    if obj is not None:\n",
    "        dff = obj.return_log()\n",
    "        logs = pd.concat([logs, dff], axis=0)\n",
    "\n",
    "        dff = dff.reset_index(drop=True)\n",
    "        if rule['mute_alert'] is None:\n",
    "\n",
    "            templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "        else:\n",
    "            aliases_list = rule['mute_alert'].split(',')\n",
    "\n",
    "            for j in aliases_list:\n",
    "                dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "\n",
    "            templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "        if rule['mute_logs'] is None:\n",
    "\n",
    "            templogs = pd.concat([templogs, dff], axis=0)\n",
    "            clear_logs = logs\n",
    "        elif rule['mute_logs'].lower() == 'all':\n",
    "            dff = dff[~dff.rule_name.str.contains(rule['rule_name'])]\n",
    "\n",
    "            clear_logs  = logs[~logs.rule_name.str.contains(rule['rule_name'])]\n",
    "            templogs = pd.concat([templogs, dff], axis=0)\n",
    "        else:\n",
    "            aliases_list = rule['mute_logs'].split(',')\n",
    "\n",
    "            for j in aliases_list:\n",
    "                dff = dff[~dff.aliases.str.contains(j.strip())]\n",
    "                clear_logs = logs[~logs.aliases.str.contains(j.strip())]\n",
    "\n",
    "            templogs = pd.concat([templogs, dff], axis=0)\n",
    "\n",
    "\n",
    "    return clear_logs, templogs\n",
    "    \n",
    "logs = pd.DataFrame()\n",
    "templogs = pd.DataFrame()\n",
    "date = arrow.utcnow().date()\n",
    "date = date.replace(year=1900)\n",
    "\n",
    "\n",
    "for i in range(rules.shape[0]):\n",
    "    if not rules.iloc[i]['mute_logs']:\n",
    "        start_date = rules.iloc[i]['start_date']\n",
    "        end_date = rules.iloc[i]['end_date']\n",
    "\n",
    "        if start_date is None and end_date is None:\n",
    "\n",
    "            logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "\n",
    "        elif start_date is not None and end_date is not None:\n",
    "\n",
    "            start_date = dt.strptime(start_date, '%m-%d').date()\n",
    "            end_date = dt.strptime(end_date, '%m-%d').date()\n",
    "\n",
    "            if start_date < end_date:\n",
    "                if start_date <= date <= end_date:\n",
    "                    logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "            elif start_date > end_date:\n",
    "                if start_date >= date and end_date >= date:\n",
    "                    logs, templogs = create_logs(rules.iloc[i], data_rd, logs, templogs)\n",
    "            else:\n",
    "                print('Logs not created for this rule  :  ', rules.iloc[i]['rule_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "404d0600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule_name</th>\n",
       "      <th>aliases</th>\n",
       "      <th>criticality</th>\n",
       "      <th>time_local</th>\n",
       "      <th>test</th>\n",
       "      <th>date_update</th>\n",
       "      <th>qa</th>\n",
       "      <th>mute_alert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [rule_name, aliases, criticality, time_local, test, date_update, qa, mute_alert]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070f456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e997d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5823c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82c891e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABC</th>\n",
       "      <th>BCD</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ABC  BCD  E\n",
       "a    1    4  7\n",
       "b    2    5  8\n",
       "c    3    6  9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"ABC\":[1,2,3],\"BCD\":[4,5,6],\"E\":[7,8,9]}, index=[\"a\",\"b\",\"c\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b5be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce358a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7ffb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
