{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a1f598",
   "metadata": {},
   "source": [
    "###        To Do.\n",
    "- Currently the alias_pairs function is matching the exact keywords ie. for **ahu_enable_sensor** and **ahu_chiller_enable_sensor** we have ```ahu_1_enable_sensor``` and ``ahu_1_chiller_enable_sensor``, respectively. These aliases would match because both contain the same key \"1\". But if the config provided kewyords are changed like **ahu_enable_sensor** and **ahu_chiller** then the available aliases would not match. So your task is to update alias_pair function accordingly.\n",
    "    \n",
    "- Make sure all KPIs do not throw any error.\n",
    "\n",
    "- There are some bugs in the code, so fix that before you start your main task.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f8789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow\n",
    "import pandas as pd\n",
    "import json\n",
    "import google.auth\n",
    "import google.auth\n",
    "import logging\n",
    "from string import Template\n",
    "from meteostat import Daily\n",
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from datetime import time\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13982a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kpi():\n",
    "    def __init__(self, rule, batch):\n",
    "        # get subset of data, min-time and max-time for the given start and end time\n",
    "        batch, batch_min_time, batch_max_time = self.__batch_subset(batch, rule['start_time'], rule['end_time'])\n",
    "        # indices of the rule dataframe\n",
    "        alias_cols = rule.index\n",
    "        subs = 'alias'\n",
    "        # get columns of rule book that alias in their names\n",
    "        self.__res = [alias_col for alias_col in alias_cols if subs in alias_col]\n",
    "        self.__rule = rule\n",
    "\n",
    "        # default logs columns\n",
    "        log_columns = ['kpi_name', 'time', 'aliases', 'value', 'aggregation']\n",
    "        for col in self.__res:\n",
    "            log_columns.append(col)\n",
    "\n",
    "        # create empty dataframe for the logs\n",
    "        self.__df_logs = pd.DataFrame(columns=['kpi_name', 'time', 'aliases', 'value', 'aggregation'])\n",
    "        aliases = []\n",
    "        # get a list of aliases for the given targets in the rule book and append that lists\n",
    "        # in the aliases list\n",
    "        for i in self.__res:\n",
    "            if rule[i] is not None:\n",
    "                print(\"column name ==> \", i)\n",
    "                aliases.append(self.__alias_substrings(batch, [rule[i]], col_name='alias'))\n",
    "        # in case aliases list length is 1\n",
    "        # create logs for only one target (alias_a)\n",
    "        if len(aliases) == 1:\n",
    "            for alias in aliases[0]:\n",
    "                # get a dataframe for each alias\n",
    "                df = self.alias_dataframe(self, data_rd=batch, aliases=alias, kpi=rule['kpi_name'],\n",
    "                                          resample_grain=None, daily_resample=None, agg=None, non_negative=None)\n",
    "                # apply the equation that is given in the rule book (equation_1)\n",
    "                df = self.apply_equation(self, df, equation=rule['equation_1'], temp=True)\n",
    "                # Aggregate the data after applying equation_1\n",
    "                df = self.apply_aggregation(self, df, aggregation=rule['aggregation'], kpi=rule['kpi_name'])\n",
    "                # In case equation_2 is given then apply it after aggregation\n",
    "                if rule['equation_2'] is not None:\n",
    "                    df = self.apply_equation(self, df, equation=rule['equation_2'])\n",
    "\n",
    "                df1 = self.__logs_data(df, rule, [alias])\n",
    "                self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "\n",
    "        else:\n",
    "            alias_pair = self.__alias_pairs(self, batch, rule, aliases)\n",
    "\n",
    "            for pair in alias_pair:\n",
    "                if len(pair) != 0:\n",
    "\n",
    "                    df = self.alias_pairs_dataframe(self, data_rd=batch, aliases=pair, kpi=rule['kpi_name'],\n",
    "                                                    resample_grain=None, daily_resample=None, agg=None,\n",
    "                                                    non_negative=None)\n",
    "                    df = self.apply_equation(self, df, equation=rule['equation_1'], temp=True)\n",
    "                    df = self.apply_aggregation(self, df, aggregation=rule['aggregation'], kpi=rule['kpi_name'])\n",
    "                    # In case equation_2 is given then apply it after aggregation\n",
    "                    if rule['equation_2'] is not None:\n",
    "                        df = self.apply_equation(self, df, equation=rule['equation_2'])\n",
    "\n",
    "\n",
    "                    df1 = self.__logs_data(df, rule, pair)\n",
    "                    self.__df_logs = pd.concat([self.__df_logs, df1], axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def __logs_data(df=None, rule=None, alias_list=None):\n",
    "        \"\"\" Gets resultant dataframe and rules for each kpi and creates logs\n",
    "            in the form of a dataframe.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        Resultant dataframe for each kpi\n",
    "        rule:dataframe\n",
    "        Its the config file\n",
    "        tgt_alias:string\n",
    "        Name of the target alias\n",
    "        ref_alias: string\n",
    "        Name of the reference alias\n",
    "        \"\"\"\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "        if len(alias_list) == 1:\n",
    "            logs_alias = alias_list[0]\n",
    "        else:\n",
    "            logs_alias = \",\".join(alias_list)\n",
    "\n",
    "        new_row = {'kpi_name': rule['kpi_name'],\n",
    "                   'time': df['time'],\n",
    "                   'aliases': logs_alias,\n",
    "                   'value': df['value'],\n",
    "                   'equation_1': rule['equation_1'],\n",
    "                   'equation_2': rule['equation_2'],\n",
    "                   'aggregation': rule['aggregation'],\n",
    "                   }\n",
    "\n",
    "        df1 = pd.DataFrame(new_row)\n",
    "        return df1\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_equation(self, df=None, equation=None, temp=None):\n",
    "\n",
    "        def func(x):\n",
    "            y = eval(equation)\n",
    "            return y\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "\n",
    "            if equation is not None:\n",
    "\n",
    "                for col in self.__res:\n",
    "                    if col in equation:\n",
    "                        col_name = f\"x['{col}']\"\n",
    "                        equation = equation.replace(col, col_name)\n",
    "                if 'value' in equation:\n",
    "                    equation = equation.replace('value',\"x['value']\")\n",
    "\n",
    "                if 'time_local' in equation:\n",
    "                    df['time_local'] = pd.to_datetime(df['time_local'])\n",
    "                    df['time_only'] = df['time_local'].dt.time.astype('str')\n",
    "                    equation = equation.replace('time_local', 'x[\"time_only\"]')\n",
    "                    time_list = re.findall(r'\\d{2}\\:\\d{2}\\:\\d{2}', equation)\n",
    "                    for tt in time_list:\n",
    "                        time_split = tt.split(\":\")\n",
    "                        updated_time = time(int(time_split[0]), int(time_split[1]), int(time_split[2]))\n",
    "                        equation = re.sub(tt, str(updated_time), equation)\n",
    "\n",
    "                df['value'] = df.apply(func, axis=1)\n",
    "\n",
    "            else:\n",
    "                df['value'] = df['alias_a']\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_aggregation(self, df=None, aggregation='mean', kpi=None):\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            if aggregation == 'perc':\n",
    "                aggregation = 'mean'\n",
    "\n",
    "            df.set_index('time', inplace=True)\n",
    "\n",
    "            x = f\"df.resample('D').{aggregation}()\"\n",
    "\n",
    "            df = eval(x)\n",
    "\n",
    "            df.reset_index(inplace=True)\n",
    "\n",
    "            df['kpi'] = kpi\n",
    "\n",
    "        df.loc[df[\"value\"] == True, \"value\"] = 1\n",
    "        df.loc[df[\"value\"] == False, \"value\"] = 0\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_substrings(df, in_list, col_name='alias'):\n",
    "        \"\"\" This function returns a list of aliases for a given reference in the\n",
    "            the config file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: dataframe\n",
    "        It's the source dataframe\n",
    "        in_list: list\n",
    "        It's the list of given aliases in the config file\n",
    "        col_name: string\n",
    "        It's the column name in the source dataframe\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_aliases: list\n",
    "        This list contains the aliases that are filtered out\n",
    "        \"\"\"\n",
    "        groups = in_list\n",
    "\n",
    "        substrings = []\n",
    "        filtered_aliases = []\n",
    "        # Creating list of substrings\n",
    "        for group_id in groups:\n",
    "            print(\"group_id ==> \", group_id)\n",
    "            print(\"group_id data type ==> \",type(group_id))\n",
    "            substrings.append(group_id.split('_'))\n",
    "\n",
    "        unique_aliases = df.columns\n",
    "        # print(unique_aliases)\n",
    "        # Removing None from the list of unique aliases\n",
    "        unique_aliases = list(filter(None, unique_aliases))\n",
    "        # Filter the aliases that are to be used for the KPI\n",
    "        for alias in unique_aliases:\n",
    "            for sub in substrings:\n",
    "                if all(a in alias for a in sub):\n",
    "                    filtered_aliases.append(alias)\n",
    "\n",
    "        return filtered_aliases\n",
    "\n",
    "    @staticmethod\n",
    "    def __batch_subset(batch, start_time, end_time):\n",
    "        \"\"\"Gets a dataframe and a specific duration, where it takes a subset\n",
    "        of dataframe that falls in the start and end times.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch: dataframe\n",
    "        The source dataframe\n",
    "        start_time: timestamp\n",
    "        This will be the start or minimum time of the dataframe\n",
    "        end_time: timestamp\n",
    "        This be the end or maximum time of the dataframe\n",
    "        Returns\n",
    "        ------\n",
    "        batch: dataframe\n",
    "        This is resultant dataframe between the start and end times\n",
    "        batch_min_time: timestamp\n",
    "        The minimum time of the dataframe\n",
    "        batch_max_time: timestamp\n",
    "        The maximum time of the dataframe\n",
    "        \"\"\"\n",
    "        # in case start time is given\n",
    "        if start_time is not None:\n",
    "            # start_time = str(start_time)\n",
    "            # split the time(string) in hour,min and secs\n",
    "            hour = start_time.split(':')[0]\n",
    "            minute = start_time.split(':')[1]\n",
    "            second = start_time.split(':')[2]\n",
    "            print(f'start_time {time(int(hour), int(minute), int(second))}')\n",
    "            # filter dataframe having time  greater or equal  to start time\n",
    "            batch = batch.loc[batch['time'].dt.time >= time(int(hour), int(minute), int(second))]\n",
    "        # in case end time is given\n",
    "        if end_time is not None:\n",
    "            # end_time = str(end_time)\n",
    "            # split the time(string) in hour,min and secs\n",
    "            hour = end_time.split(':')[0]\n",
    "            minute = end_time.split(':')[1]\n",
    "            second = end_time.split(':')[2]\n",
    "            print(f\"end_time {time(int(hour), int(minute), int(second))}\")\n",
    "            # filter dataframe having time  smaller than the  end time\n",
    "            batch = batch.loc[batch['time'].dt.time < time(int(hour), int(minute), int(second))]\n",
    "        # get miminium and maximum time of the dataframe\n",
    "        batch_min_time = batch.time.min()\n",
    "        batch_max_time = batch.time.max()\n",
    "        return batch, batch_min_time, batch_max_time\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_pairs_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                              daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        res = [index for index, val in enumerate(aliases)]\n",
    "        alias_cols = aliases.copy()\n",
    "        alias_cols.append('time')\n",
    "        cols = [val for index, val in enumerate(self.__res) if index in res]\n",
    "        cols.append('time')\n",
    "        df = data_rd[alias_cols].copy()\n",
    "        df.columns = cols\n",
    "        if df.shape[0] > 0:\n",
    "            df_master = df\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def alias_dataframe(self, data_rd=None, aliases=None, kpi=None, resample_grain=None,\n",
    "                        daily_resample=None, agg=None, non_negative=None):\n",
    "        df_master = pd.DataFrame()\n",
    "        df = data_rd[['time', aliases]].copy()\n",
    "        df.rename(columns={aliases: 'alias_a'}, inplace=True)\n",
    "        df_master = df\n",
    "\n",
    "        return df_master\n",
    "\n",
    "    @staticmethod\n",
    "    def __alias_pairs(self, data_rd, rule, aliases):\n",
    "        self.__index = [index for index, val in enumerate(aliases)]\n",
    "        col_names = self.__res\n",
    "        col_names = [val for index, val in enumerate(col_names) if index in self.__index]\n",
    "        null_list_index = [index for index, val in enumerate(aliases) if len(val) == 0]\n",
    "        non_null_dfs_list = []\n",
    "        null_dfs_list = []\n",
    "\n",
    "        for i in range(len(aliases)):\n",
    "\n",
    "            non_null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            null_alias_df = pd.DataFrame(columns=['alias', 'keys'])\n",
    "            if len(aliases[i]) > 0:\n",
    "                for alias in aliases[i]:\n",
    "                    keys = alias.split(\"_\")\n",
    "                    for ele in rule[col_names[i]].strip().split('_'):\n",
    "                        keys.remove(ele)\n",
    "\n",
    "                    if len(keys) == 0:\n",
    "\n",
    "                        df = pd.DataFrame([[alias, None]], columns=['alias', 'keys'])\n",
    "\n",
    "                        null_alias_df = pd.concat([null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        key = '_'.join(map(str, keys))\n",
    "                        df = pd.DataFrame([[alias, key]], columns=['alias', 'keys'])\n",
    "                        non_null_alias_df = pd.concat([non_null_alias_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "            if non_null_alias_df.shape[0] > 0:\n",
    "                non_null_dfs_list.append(non_null_alias_df)\n",
    "            if null_alias_df.shape[0] > 0:\n",
    "                null_dfs_list.append(null_alias_df)\n",
    "\n",
    "        if len(null_dfs_list) == len(col_names):\n",
    "            nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "        else:\n",
    "\n",
    "            if len(non_null_dfs_list) > 1:\n",
    "                non_nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, on=[\"keys\"], how=\"inner\"),\n",
    "                                            non_null_dfs_list)\n",
    "            elif len(non_null_dfs_list) == 1:\n",
    "                non_nulls_df_merge = non_null_dfs_list[0]\n",
    "            else:\n",
    "                non_nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "            if len(null_dfs_list) > 1:\n",
    "                nulls_df_merge = reduce(lambda left, right: pd.merge(left, right, how=\"cross\"), null_dfs_list)\n",
    "            elif len(null_dfs_list) == 1:\n",
    "                nulls_df_merge = null_dfs_list[0]\n",
    "            else:\n",
    "                nulls_df_merge = pd.DataFrame(columns=['alias', 'keys'])\n",
    "\n",
    "        if nulls_df_merge.shape[0] > 0 and non_nulls_df_merge.shape[0] > 0:\n",
    "            alias_pairs_df = pd.merge(nulls_df_merge, non_nulls_df_merge, how='cross')\n",
    "        elif nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = non_nulls_df_merge\n",
    "        elif non_nulls_df_merge.shape[0] == 0:\n",
    "            alias_pairs_df = nulls_df_merge\n",
    "\n",
    "        subs = 'keys'\n",
    "        keys_cols = [col for col in alias_pairs_df if subs in col]\n",
    "        alias_pairs_df = alias_pairs_df.drop(keys_cols, axis=1)\n",
    "\n",
    "        if alias_pairs_df.shape[1] == len(col_names):\n",
    "            alias_pairs = alias_pairs_df.values.tolist()\n",
    "        else:\n",
    "            alias_pairs = []\n",
    "\n",
    "        return alias_pairs\n",
    "\n",
    "    def return_log(self):\n",
    "        return self.__df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa839351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpi_function(kwargs):\n",
    "    \"\"\" This function actually reads data from the source table and perform operations on it and then\n",
    "        sends back the resultant logs to the destination table in Bigquery.\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_time: timestamp\n",
    "        It the time from which data will start\n",
    "        end_time: timestamp\n",
    "        It the end of the data\n",
    "        project: GCP project id\n",
    "        dataset_src: Bigquery source dataset\n",
    "        location: Project location\n",
    "        dataset_dest: Bigquery destination dataset\n",
    "        table_src: Bigquery source table\n",
    "        table_dest: Bigquery destination table\n",
    "        Returns\n",
    "        -------\n",
    "        This function doesn't return anything\n",
    "        -------\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = kwargs['start_time']\n",
    "    end_time = kwargs['end_time']\n",
    "    project = kwargs['project']\n",
    "    client_id = kwargs['client_id']\n",
    "    device_id = kwargs['device_id']\n",
    "    dataset_src = kwargs['dataset_src']\n",
    "    dataset_dest = kwargs['dataset_dest']\n",
    "    table_src = kwargs['table_src']\n",
    "    table_dest = kwargs['table_dest']\n",
    "    location = kwargs['location']\n",
    "    config = kwargs['config']\n",
    "\n",
    "    def station_data(station_id, start, end):\n",
    "        data = Daily(station_id, start, end)\n",
    "        data = data.fetch()\n",
    "        data.reset_index(inplace=True)\n",
    "        data = data[['time', 'tavg', 'tmin', 'tmax']]\n",
    "        #     print(data)\n",
    "        return data\n",
    "\n",
    "    current_time = arrow.utcnow().floor('day')\n",
    "    # if end_time <= current_time:\n",
    "    output = config\n",
    "\n",
    "    rules = pd.DataFrame(output['rules'])\n",
    "    rules.index = rules.index.astype(int)\n",
    "    rules.sort_index(inplace=True)\n",
    "    rules = rules[['kpi_name',\n",
    "                   'alias_a',\n",
    "                   'alias_b',\n",
    "                   'alias_c',\n",
    "                   'alias_d',\n",
    "                   'alias_e',\n",
    "                   'alias_f',\n",
    "                   'equation_1',\n",
    "                   'equation_2',\n",
    "                   'aggregation',\n",
    "                   'start_time',\n",
    "                   'end_time',\n",
    "                   'mute_logs']].copy()\n",
    "    rules = rules.where(pd.notnull(rules), None)\n",
    "    settings = output['settings']['mute_rules']\n",
    "    try:\n",
    "        station_id = output['settings']['station_id']\n",
    "    except:\n",
    "        print(f'Station ID is not present for {client_id} ')\n",
    "    df = rules\n",
    "\n",
    "    # create an empty dataframe\n",
    "    logs = pd.DataFrame()\n",
    "    # get access to bigqyery\n",
    "#     credentials, project_id = google.auth.default()\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    if settings is False:\n",
    "\n",
    "        dataframe = pd.read_csv('main_data.csv')\n",
    "\n",
    "        data_rd = dataframe\n",
    "        data_rd['value'] = data_rd['value'].astype('float')\n",
    "        data_rd['time'] = data_rd['time_local']\n",
    "        data_rd['time'] = pd.to_datetime(data_rd['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        dff2 = data_rd[['time', 'alias','value']].copy()\n",
    "        dff2 = dff2[dff2.alias == 'outside_air_temp']\n",
    "        dff2.set_index('time',inplace=True)\n",
    "        dff2 = dff2.resample('1h').first()\n",
    "\n",
    "        dff2['alias'] = 'air_temperature_1h'\n",
    "        dff2.reset_index(inplace=True)\n",
    "\n",
    "        dff2['time_local'] = dff2['time']\n",
    "\n",
    "        frames = [data_rd, dff2[['time_local', 'time', 'alias', 'value']]]\n",
    "        data_rd = pd.concat(frames, axis=0).reset_index(drop=True)\n",
    "        if data_rd.shape[0] > 0:\n",
    "            left_table = data_rd.drop(['alias','value','time_local'],axis=1)\n",
    "            left_table = left_table.groupby(\"time\").min().reset_index()\n",
    "            sub_df = data_rd[['time','alias','value']].copy()\n",
    "            sub_df['value'] = pd.to_numeric(sub_df['value'])\n",
    "            right_table = pd.pivot_table(sub_df,index='time',columns='alias',values='value').reset_index()\n",
    "            data_rd = pd.merge(left_table,right_table,on='time', how='outer')\n",
    "\n",
    "        if data_rd.shape[0] > 0:\n",
    "            min_time = data_rd.time.min()\n",
    "            max_time = data_rd.time.max()\n",
    "            try:\n",
    "                if station_id is not None:\n",
    "                    meteo_data = station_data(station_id, min_time, max_time)\n",
    "                    data_rd = data_rd.merge(meteo_data, on='time', how='left')\n",
    "                    data_rd[['tavg', 'tmax', 'tmin']] = data_rd[['tavg', 'tmax', 'tmin']].astype('float')\n",
    "                else:\n",
    "                    print('Station is None')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "            for i in range(df.shape[0]):\n",
    "                obj = kpi(df.iloc[i], data_rd)\n",
    "                dff = obj.return_log()\n",
    "                logs = pd.concat([logs, dff], axis=0)\n",
    "\n",
    "                if df.iloc[i]['mute_logs']:\n",
    "                    aliases_list = df.iloc[i]['mute_logs'].split(',')\n",
    "                    for alias in aliases_list:\n",
    "                        logs = logs[~logs.aliases.str.contains(alias.strip())]\n",
    "        else:\n",
    "            print(f\"Data is not available from {start_time} to {end_time}\")\n",
    "\n",
    "\n",
    "        if logs.shape[0]>0:\n",
    "\n",
    "            logs['time'] = logs['time'].dt.date\n",
    "\n",
    "            logs.rename(columns={\"time\":\"date\"},inplace=True)\n",
    "            logs['client_id'] = client_id\n",
    "            logs['device_id'] = device_id\n",
    "            logs['date'] = logs['date'].astype(str)\n",
    "            # load the logs dataframe to destination table in the bigquery\n",
    "            table = client.get_table(f\"{project}.{dataset_dest}.{table_dest}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('no logs saved')\n",
    "    else:\n",
    "        print(\"All rules are muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cec7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the config file here \n",
    "# ======================================\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b86de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 16\u001b[0m\n\u001b[0;32m      3\u001b[0m end_time  \u001b[38;5;241m=\u001b[39m arrow\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-12-31 00:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# You should not edit kwargs, except your config variable\u001b[39;00m\n\u001b[0;32m      6\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m      : start_time,\n\u001b[0;32m      8\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m'\u001b[39m        : end_time,\n\u001b[0;32m      9\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m'\u001b[39m         : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthermosphr-prod\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_id\u001b[39m\u001b[38;5;124m'\u001b[39m       : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgera-arcaden-germany\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_src\u001b[39m\u001b[38;5;124m'\u001b[39m     : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m        : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEurope/Berlin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_dest\u001b[39m\u001b[38;5;124m'\u001b[39m    : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_src\u001b[39m\u001b[38;5;124m'\u001b[39m       : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatamart_v2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_dest\u001b[39m\u001b[38;5;124m'\u001b[39m      : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbi\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 16\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m          : \u001b[43mconfig\u001b[49m,                      \u001b[38;5;66;03m# your config should be given here.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_id\u001b[39m\u001b[38;5;124m'\u001b[39m       : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mayQAWQgc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     20\u001b[0m kpi_function(kwargs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# data for not more than 5 days\n",
    "start_time = arrow.get('2022-10-01 00:00:00')\n",
    "end_time  = arrow.get('2022-12-31 00:00:00')\n",
    "\n",
    "# You should not edit kwargs, except your config variable\n",
    "kwargs = {\n",
    "      'start_time'      : start_time,\n",
    "      'end_time'        : end_time,\n",
    "      'project'         : 'thermosphr-prod',\n",
    "      'client_id'       : 'gera-arcaden-germany',\n",
    "      'dataset_src'     : 'bi',\n",
    "      'location'        : 'Europe/Berlin',\n",
    "      'dataset_dest'    : 'bi',\n",
    "      'table_src'       : 'datamart_v2',\n",
    "      'table_dest'      : 'bi',\n",
    "      'config'          : config,                      # your config should be given here.\n",
    "      'device_id'       : 'ayQAWQgc'\n",
    "}\n",
    "\n",
    "kpi_function(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb9256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A digit was found at index ['1']\n"
     ]
    }
   ],
   "source": [
    "string = \"1abcdef.\"\n",
    "match = re.findall(r'\\d', string)\n",
    "if match:\n",
    "    print(\"A digit was found at index\", match)\n",
    "else:\n",
    "    print(\"No digit was found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfcee4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The digits found are ['1', '2']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word = \"word12\"\n",
    "digits = re.findall(r'\\d', word)\n",
    "if digits:\n",
    "    print(\"The digits found are\", digits)\n",
    "else:\n",
    "    print(\"No digit was found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f87f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a173d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
